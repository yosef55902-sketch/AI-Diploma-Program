{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 Q Learning\n",
        "\n",
        "## ğŸ“š Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
        "\n",
        "This notebook demonstrates key concepts through hands-on examples.\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Understand the core concepts\n",
        "- See practical implementations\n",
        "- Be ready for exercises\n",
        "\n",
        "## ğŸ”— Prerequisites | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "\n",
        "- âœ… Python 3.8+ installed\n",
        "- âœ… Required libraries (see `requirements.txt`)\n",
        "\n",
        "- âœ… **numpy** library: `pip install numpy`\n",
        "- âœ… Basic Python knowledge\n",
        "\n",
        "---\n",
        "\n",
        "## Code Example | Ù…Ø«Ø§Ù„ Ø§Ù„ÙƒÙˆØ¯\n",
        "\n",
        "Run the code below to see the demonstration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Visualization: Q-Table Heatmap\n",
        "# ØªØµÙˆØ±: Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    \n",
        "    # Example Q-table (from training above)\n",
        "    q_table_example = np.array([[0.5, 0.8], [0.3, 0.9], [0.2, 0.7], [0.1, 0.6], [0.0, 0.0]])\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(q_table_example, cmap='YlOrRd', aspect='auto')\n",
        "    plt.colorbar(label='Q-Value | Ù‚ÙŠÙ…Ø© Q')\n",
        "    plt.xlabel('Actions | Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª (Left, Right)', fontsize=12)\n",
        "    plt.ylabel('States | Ø§Ù„Ø­Ø§Ù„Ø§Øª', fontsize=12)\n",
        "    plt.title('Q-Table Visualization | ØªØµÙˆØ± Ø¬Ø¯ÙˆÙ„ Q', fontsize=14, pad=20)\n",
        "    plt.xticks([0, 1], ['Left | ÙŠØ³Ø§Ø±', 'Right | ÙŠÙ…ÙŠÙ†'])\n",
        "    plt.yticks(range(5), [f'State {i}' for i in range(5)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"âœ… Q-table heatmap displayed\")\n",
        "except ImportError:\n",
        "    print(\"Note: Install matplotlib and numpy for Q-table visualization\")\n",
        "    print(\"Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª matplotlib Ùˆ numpy Ù„ØªØµÙˆØ± Ø¬Ø¯ÙˆÙ„ Q\")\n",
        "\n",
        "\"\"\"\n",
        "Unit 2 - Example 1: Q-Learning Algorithm\n",
        "Ø§Ù„ÙˆØ­Ø¯Ø© 2 - Ù…Ø«Ø§Ù„ 1: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning\n",
        "\n",
        "This example demonstrates:\n",
        "1. Q-learning algorithm\n",
        "2. Q-table initialization\n",
        "3. Q-value updates\n",
        "4. Policy extraction\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Example 1: Q-Learning Algorithm\")\n",
        "print(\"Ù…Ø«Ø§Ù„ 1: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simple Grid World Environment\n",
        "# Ø¨ÙŠØ¦Ø© Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø©\n",
        "# States: 0, 1, 2, 3, 4 (4 is goal)\n",
        "# Actions: 0=left, 1=right\n",
        "\n",
        "class SimpleGridWorld:\n",
        "    \"\"\"\n",
        "    Simple grid world for Q-learning demonstration.\n",
        "    Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© Ù„Ø¹Ø±Ø¶ Q-learning.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.states = 5\n",
        "        self.actions = 2  # left, right\n",
        "        self.goal_state = 4\n",
        "        \n",
        "    def get_reward(self, state, action, next_state):\n",
        "        \"\"\"Get reward for transition.\"\"\"\n",
        "        if next_state == self.goal_state:\n",
        "            return 10.0  # Goal reward\n",
        "        return -0.1  # Small negative reward for each step\n",
        "    \n",
        "    def get_next_state(self, state, action):\n",
        "        \"\"\"Get next state after action.\"\"\"\n",
        "        if action == 0:  # left\n",
        "            return max(0, state - 1)\n",
        "        else:  # right\n",
        "            return min(self.states - 1, state + 1)\n",
        "\n",
        "# Initialize Q-table\n",
        "# ØªÙ‡ÙŠØ¦Ø© Ø¬Ø¯ÙˆÙ„ Q\n",
        "env = SimpleGridWorld()\n",
        "Q = np.zeros((env.states, env.actions))\n",
        "\n",
        "print(\"\\nInitial Q-table:\")\n",
        "print(\"Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ø£ÙˆÙ„ÙŠ:\")\n",
        "print(Q)\n",
        "\n",
        "# Visualization: Q-Table Heatmap (After Training)\n",
        "# ØªØµÙˆØ±: Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q (Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    im = plt.imshow(Q, cmap='YlOrRd', aspect='auto', interpolation='nearest')\n",
        "    plt.colorbar(im, label='Q-Value | Ù‚ÙŠÙ…Ø© Q')\n",
        "    plt.xlabel('Actions | Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('States | Ø§Ù„Ø­Ø§Ù„Ø§Øª', fontsize=12, fontweight='bold')\n",
        "    plt.title('Trained Q-Table Heatmap | Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ù…Ø¯Ø±Ø¨', \n",
        "             fontsize=14, pad=20, fontweight='bold')\n",
        "    plt.xticks([0, 1], ['Left | ÙŠØ³Ø§Ø±', 'Right | ÙŠÙ…ÙŠÙ†'])\n",
        "    plt.yticks(range(env.states), [f'State {i} | Ø§Ù„Ø­Ø§Ù„Ø© {i}' for i in range(env.states)])\n",
        "    \n",
        "    # Add text annotations with Q-values\n",
        "    for i in range(env.states):\n",
        "        for j in range(env.actions):\n",
        "            text = plt.text(j, i, f'{Q[i, j]:.2f}', \n",
        "                           ha='center', va='center', \n",
        "                           color='white' if Q[i, j] > 0.5 else 'black',\n",
        "                           fontweight='bold', fontsize=11)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nâœ… Q-table heatmap visualization displayed\")\n",
        "    print(\"ØªÙ… Ø¹Ø±Ø¶ Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q Ø¨Ù†Ø¬Ø§Ø­\")\n",
        "except ImportError:\n",
        "    print(\"\\nâš ï¸  Install matplotlib for Q-table visualization:\")\n",
        "    print(\"   pip install matplotlib\")\n",
        "    print(\"Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª matplotlib Ù„ØªØµÙˆØ± Ø¬Ø¯ÙˆÙ„ Q\")\n",
        "\n",
        "# Q-Learning parameters\n",
        "# Ù…Ø¹Ø§Ù…Ù„Ø§Øª Q-Learning\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Q-Learning Training\")\n",
        "print(\"ØªØ¯Ø±ÙŠØ¨ Q-Learning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Training loop\n",
        "# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "num_episodes = 100\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = 0  # Start state\n",
        "    \n",
        "    while state != env.goal_state:\n",
        "        # Epsilon-greedy action selection\n",
        "        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… epsilon-greedy\n",
        "        if np.random.random() < epsilon:\n",
        "            action = np.random.randint(env.actions)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(Q[state])  # Exploit\n",
        "        \n",
        "        # Take action\n",
        "        next_state = env.get_next_state(state, action)\n",
        "        reward = env.get_reward(state, action, next_state)\n",
        "        \n",
        "        # Q-learning update\n",
        "        # ØªØ­Ø¯ÙŠØ« Q-learning\n",
        "        Q[state, action] = Q[state, action] + alpha * (\n",
        "            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
        "        )\n",
        "        \n",
        "        state = next_state\n",
        "\n",
        "print(\"\\nTrained Q-table:\")\n",
        "print(\"Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ù…Ø¯Ø±Ø¨:\")\n",
        "print(Q)\n",
        "\n",
        "# Extract policy\n",
        "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³ÙŠØ§Ø³Ø©\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Extracted Policy\")\n",
        "print(\"Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "policy = {}\n",
        "for state in range(env.states):\n",
        "    best_action = np.argmax(Q[state])\n",
        "    action_name = \"left\" if best_action == 0 else \"right\"\n",
        "    policy[state] = action_name\n",
        "    print(f\"State {state}: {action_name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Example completed successfully!\")\n",
        "print(\"ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Visualize Q-table as heatmap (after training)\n",
        "# ØªØµÙˆØ± Ø¬Ø¯ÙˆÙ„ Q ÙƒØ®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© (Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(10, 7))\n",
        "    im = plt.imshow(Q, cmap='YlOrRd', aspect='auto', interpolation='nearest')\n",
        "    plt.colorbar(im, label='Q-Value | Ù‚ÙŠÙ…Ø© Q', pad=0.02)\n",
        "    plt.xlabel('Actions | Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª (0=Left/ÙŠØ³Ø§Ø±, 1=Right/ÙŠÙ…ÙŠÙ†)', \n",
        "               fontsize=13, fontweight='bold')\n",
        "    plt.ylabel('States | Ø§Ù„Ø­Ø§Ù„Ø§Øª', fontsize=13, fontweight='bold')\n",
        "    plt.title('Trained Q-Table Heatmap | Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ù…Ø¯Ø±Ø¨', \n",
        "             fontsize=15, pad=20, fontweight='bold')\n",
        "    plt.xticks([0, 1], ['Left | ÙŠØ³Ø§Ø±', 'Right | ÙŠÙ…ÙŠÙ†'], fontsize=11)\n",
        "    plt.yticks(range(env.states), [f'State {i} | Ø§Ù„Ø­Ø§Ù„Ø© {i}' for i in range(env.states)], fontsize=11)\n",
        "    \n",
        "    # Add Q-value annotations\n",
        "    for i in range(env.states):\n",
        "        for j in range(env.actions):\n",
        "            text = plt.text(j, i, f'{Q[i, j]:.2f}', \n",
        "                           ha='center', va='center', \n",
        "                           color='white' if Q[i, j] > np.max(Q) * 0.5 else 'black',\n",
        "                           fontweight='bold', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nâœ… Q-table heatmap visualization displayed!\")\n",
        "    print(\"ØªÙ… Ø¹Ø±Ø¶ Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q Ø¨Ù†Ø¬Ø§Ø­!\")\n",
        "except ImportError:\n",
        "    print(\"\\nğŸ“¦ To see Q-table visualization, install:\")\n",
        "    print(\"   pip install matplotlib\")\n",
        "    print(\"\\nÙ…Ù„Ø§Ø­Ø¸Ø©: Ù„Ø±Ø¤ÙŠØ© Ø§Ù„ØªØµÙˆØ±ØŒ Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª:\")\n",
        "    print(\"   pip install matplotlib\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
        "\n",
        "Great job completing this example!\n",
        "\n",
        "**What you learned:**\n",
        "- Core concepts demonstrated in the code\n",
        "- Practical implementation details\n",
        "\n",
        "**Next steps:**\n",
        "- Complete the exercises in `exercises/` folder\n",
        "- Review the quiz materials\n",
        "- Proceed to the next example\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ’¡ Tip:** If you see errors, make sure:\n",
        "- All libraries are installed: `pip install -r requirements.txt`\n",
        "- You're using Python 3.8+\n",
        "- Cells are executed in order\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
