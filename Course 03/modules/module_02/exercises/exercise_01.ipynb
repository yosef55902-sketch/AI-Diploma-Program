{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 01\n",
    "\n",
    "**Module 02**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 01: Derivatives and Gradients",
    "",
    "This exercise helps you understand how derivatives and gradients work,",
    "which are fundamental for training machine learning models.",
    "",
    "Instructions:",
    "1. Complete the functions below",
    "2. Understand how derivatives relate to optimization",
    "3. Test your solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u26a0\ufe0f Important: Implement Functions First!\n",
    "\n",
    "**Before running tests**, complete all function implementations:\n",
    "\n",
    "1. \u2705 Find all functions with `pass` statements\n",
    "2. \u2705 Replace `pass` with your implementation\n",
    "3. \u2705 Make sure functions return values (not None)\n",
    "\n",
    "**The test code will give helpful hints if you forget!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcdd Your Implementation\n",
    "\n",
    "Complete the functions below. Replace `pass` with your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np",
    "from scipy.misc import derivative",
    "",
    "",
    "def compute_derivative(func, x, h=1e-6):",
    "    ",
    "    # TODO: Implement numerical derivative",
    "    # Formula: (f(x+h) - f(x)) / h",
    "    pass",
    "",
    "",
    "def compute_gradient(func, point):",
    "    ",
    "    # TODO: Compute partial derivatives",
    "    # Use compute_derivative for each variable",
    "    h = 1e-6",
    "    pass",
    "",
    "",
    "def gradient_descent_step(func, x, learning_rate=0.1):",
    "    ",
    "    # TODO: ",
    "    # 1. Compute gradient at current point",
    "    # 2. Move in opposite direction: x_new = x - lr * gradient",
    "    pass",
    "",
    "",
    "# Test your solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83e\uddea Test Your Solution\n",
    "\n",
    "**Run this after implementing all functions above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":",
    "    print(\"Testing Exercise 01: Derivatives and Gradients\")",
    "    print(\"=\" * 60)",
    "    ",
    "    # Test 1: Derivative",
    "    print(\"\\n1. Testing compute_derivative:\")",
    "    def f(x):",
    "        return x**2 + 3*x + 2",
    "    ",
    "    x0 = 2.0",
    "    deriv = compute_derivative(f, x0)",
    "    expected = 2*x0 + 3  # d/dx(x\u00b2 + 3x + 2) = 2x + 3",
    "    print(f\"   Function: f(x) = x\u00b2 + 3x + 2\")",
    "    print(f\"   At x = {x0}:\")",
    "    print(f\"   Computed derivative: {deriv:.4f}\")",
    "    print(f\"   Expected: {expected:.4f}\")",
    "    assert abs(deriv - expected) < 0.01, \"Derivative incorrect\"",
    "    print(\"   \u2705 Passed!\")",
    "    ",
    "    # Test 2: Gradient",
    "    print(\"\\n2. Testing compute_gradient:\")",
    "    def multivariable_func(point):",
    "        x, y = point",
    "        return x**2 + y**2 + x*y",
    "    ",
    "    point = np.array([1.0, 2.0])",
    "    grad = compute_gradient(multivariable_func, point)",
    "    expected = np.array([2*point[0] + point[1], 2*point[1] + point[0]])",
    "    print(f\"   Function: f(x, y) = x\u00b2 + y\u00b2 + xy\")",
    "    print(f\"   At point ({point[0]}, {point[1]}):\")",
    "    print(f\"   Computed gradient: {grad}\")",
    "    print(f\"   Expected: {expected}\")",
    "    assert np.allclose(grad, expected, atol=0.1), \"Gradient incorrect\"",
    "    print(\"   \u2705 Passed!\")",
    "    ",
    "    # Test 3: Gradient descent",
    "    print(\"\\n3. Testing gradient_descent_step:\")",
    "    def loss_func(x):",
    "        return (x - 3)**2  # Minimum at x = 3",
    "    ",
    "    x = 5.0",
    "    x_new = gradient_descent_step(loss_func, x, learning_rate=0.1)",
    "    print(f\"   Loss function: f(x) = (x - 3)\u00b2\")",
    "    print(f\"   Starting at x = {x}\")",
    "    print(f\"   After one step: x = {x_new:.4f}\")",
    "    print(f\"   Expected: x should move closer to 3\")",
    "    assert abs(x_new - 3) < abs(x - 3), \"Should move toward minimum\"",
    "    print(\"   \u2705 Passed!\")",
    "    ",
    "    print(\"\\n\" + \"=\" * 60)",
    "    print(\"\ud83c\udf89 All tests passed!\")",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u2705 Check Solution\n",
    "\n",
    "Compare with: `exercises/solutions/solution_01.ipynb`\n",
    "\n",
    "**\ud83d\udca1 Try solving it yourself first!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}