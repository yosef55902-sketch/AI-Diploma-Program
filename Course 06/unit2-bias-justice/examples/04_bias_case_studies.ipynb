{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Bias Case Studies | Ø¯Ø±Ø§Ø³Ø§Øª Ø­Ø§Ù„Ø© Ø§Ù„ØªØ­ÙŠØ²\n",
    "\n",
    "## ðŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Bias Detection** - Understanding how to detect bias\n",
    "- âœ… **Example 2: Bias Mitigation** - Understanding mitigation techniques\n",
    "- âœ… **Example 3: Fair Representation** - Understanding fair representation\n",
    "- âœ… **Basic Python knowledge**: Functions, data manipulation, ML concepts\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding real-world bias scenarios\n",
    "- Analyzing bias in different contexts\n",
    "- Applying detection and mitigation techniques to real cases\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FOURTH example in Unit 2** - it teaches you how to analyze real-world bias cases!\n",
    "\n",
    "**Why this example FOURTH?**\n",
    "- **Before** you can analyze real cases, you need to understand detection (Example 1)\n",
    "- **Before** you can understand solutions, you need to know mitigation (Example 2)\n",
    "- **Before** you can analyze cases deeply, you need fair representation (Example 3)\n",
    "\n",
    "**Builds on**: \n",
    "- ðŸ““ Example 1: Bias Detection (we know how to detect bias)\n",
    "- ðŸ““ Example 2: Bias Mitigation (we know how to fix bias)\n",
    "- ðŸ““ Example 3: Fair Representation (we know how to ensure fair features)\n",
    "\n",
    "**Leads to**: \n",
    "- ðŸ““ Example 5: Fair AI Development (building fair AI systems from the start!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Case studies provide **real-world context** (needed after learning techniques)\n",
    "2. Case studies teach **analysis skills** (critical for practitioners)\n",
    "3. Case studies show **different domains** (facial recognition, hiring, policing, credit)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Learning from Real Mistakes | Ø§Ù„Ù‚ØµØ©: Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©\n",
    "\n",
    "Imagine you're a medical student. **Before** you can treat patients, you need to study real cases - see what worked, what didn't, learn from mistakes. **After** studying cases, you understand patterns and can apply lessons to new situations!\n",
    "\n",
    "Same with AI bias: **Before** we learned techniques, now we study real cases - facial recognition bias, hiring discrimination, predictive policing issues. **After** case studies, we understand how bias manifests in practice!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Case Studies Matter | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… Ø¯Ø±Ø§Ø³Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø©ØŸ\n",
    "\n",
    "Case studies are essential for ethical AI:\n",
    "- **Real-World Context**: See how bias manifests in practice\n",
    "- **Learn from Mistakes**: Understand what went wrong and why\n",
    "- **Pattern Recognition**: Identify common bias patterns across domains\n",
    "- **Practical Application**: Apply detection and mitigation to real scenarios\n",
    "- **Prevention**: Learn how to avoid similar issues in your own work\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Analyze bias in facial recognition systems\n",
    "2. Analyze gender bias in hiring algorithms\n",
    "3. Analyze bias in predictive policing systems\n",
    "4. Analyze discrimination in credit scoring\n",
    "5. Identify common patterns across different bias cases\n",
    "6. Apply detection and mitigation techniques to real scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us analyze real-world bias case studies\n",
    "\n",
    "import numpy as np  # For numerical operations: Arrays, calculations, random number generation\n",
    "import pandas as pd  # For data manipulation: DataFrames, data analysis\n",
    "import matplotlib.pyplot as plt  # For creating visualizations: Charts, graphs, comparisons\n",
    "import seaborn as sns  # For statistical visualizations: Heatmaps, advanced plots\n",
    "from sklearn.ensemble import RandomForestClassifier  # For ML model: Classification algorithm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix  # For model evaluation: Performance metrics\n",
    "import warnings  # For suppressing warnings: Clean output\n",
    "import os  # For file operations: Saving images\n",
    "\n",
    "# Suppress warnings: Clean output\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings: Suppress non-critical warnings\n",
    "\n",
    "# Configure plotting: Set default styles for better visualizations\n",
    "plt.rcParams['font.size'] = 10  # Font size: Make text readable (10pt is good for most displays)\n",
    "plt.rcParams['figure.figsize'] = (14, 8)  # Figure size: 14 inches wide, 8 inches tall (good for detailed charts)\n",
    "sns.set_style(\"whitegrid\")  # Style: White background with grid for clean look\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nðŸ“š What each library does:\")\n",
    "print(\"   - numpy/pandas: Data manipulation and numerical operations\")\n",
    "print(\"   - matplotlib/seaborn: Create visualizations (charts, heatmaps)\")\n",
    "print(\"   - sklearn: Machine learning (models, metrics)\")\n",
    "print(\"   - os: File operations (saving images)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Analyze bias in facial recognition systems\n",
    "# This case study examines racial bias in facial recognition technology\n",
    "\n",
    "# BEFORE: We know facial recognition exists but don't know about bias\n",
    "# AFTER: We'll see how accuracy varies across demographic groups\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š CASE STUDY 1: Facial Recognition and Racial Bias\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFacial recognition systems have shown significant bias:\")\n",
    "print(\"  - Lower accuracy for people of color\")\n",
    "print(\"  - Higher false positive rates for minority groups\")\n",
    "print(\"  - Real-world consequences (wrongful arrests, surveillance)\\n\")\n",
    "\n",
    "def case_study_facial_recognition():\n",
    "    pass\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We learned techniques for detecting and mitigating bias, but haven't seen real-world examples.\n",
    "\n",
    "**AFTER**: We'll analyze real-world bias cases to understand how bias manifests in practice!\n",
    "\n",
    "**Why this matters**: Case studies help us learn from real mistakes and apply our knowledge to practical scenarios!\n",
    "## Part 2: Case Study 1 - Facial Recognition Bias | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø¯Ø±Ø§Ø³Ø© Ø­Ø§Ù„Ø© 1 - ØªØ­ÙŠØ² Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬ÙˆÙ‡\n",
    "\n",
    "### ðŸ“š Prerequisites (What You Need First)\n",
    "- âœ… **Library imports** (from Part 1) - Understanding visualization and analysis tools\n",
    "- âœ… **Bias detection knowledge** (from Example 1) - Understanding fairness metrics\n",
    "\n",
    "### ðŸ”— Relationship: What This Builds On\n",
    "This applies our bias detection skills to a real-world case!\n",
    "- Builds on: Bias detection skills, understanding of fairness metrics\n",
    "- Shows: How bias manifests in facial recognition systems\n",
    "\n",
    "### ðŸ“– The Story\n",
    "**Before case study**: We know bias exists but haven't seen real examples.\n",
    "**After case study**: We understand how facial recognition systems can be biased against certain groups!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Analyze Facial Recognition Bias | Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ­Ù„ÙŠÙ„ ØªØ­ÙŠØ² Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬ÙˆÙ‡\n",
    "\n",
    "**BEFORE**: We know facial recognition systems exist but don't know about their bias.\n",
    "\n",
    "**AFTER**: We'll analyze accuracy disparities across demographic groups to identify bias!\n",
    "\n",
    "**Why analyze this?** Facial recognition is widely used but has shown significant bias:\n",
    "- Lower accuracy for people of color\n",
    "- Higher false positive rates for minority groups\n",
    "- Real-world consequences (wrongful arrests, surveillance issues)\n",
    "    \"\"\"\n",
    "    Analyze bias in facial recognition systems.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Defines demographic groups (White, Black, Asian, Hispanic)\n",
    "    2. Sets accuracy rates for each group (based on real-world studies)\n",
    "    3. Sets false positive rates for each group\n",
    "    4. Calculates disparity between groups\n",
    "    5. Identifies bias patterns\n",
    "    \n",
    "    â° WHEN to use: To analyze bias in facial recognition systems\n",
    "    ðŸ’¡ WHY use: Shows how accuracy varies across demographic groups, revealing bias\n",
    "    \"\"\"\n",
    "    # Define demographic groups: Groups analyzed in the study\n",
    "    groups = ['White', 'Black', 'Asian', 'Hispanic']  # Groups: Demographic categories\n",
    "    \n",
    "    # Set accuracy rates: Based on real-world studies (e.g., NIST study)\n",
    "    # Why these numbers? Reflect actual performance disparities found in research\n",
    "    accuracy_rates = {\n",
    "        'White': 0.99,  # White: 99% accuracy (highest)\n",
    "        'Black': 0.65,  # Black: 65% accuracy (lowest - significant bias)\n",
    "        'Asian': 0.88,  # Asian: 88% accuracy\n",
    "        'Hispanic': 0.75  # Hispanic: 75% accuracy\n",
    "    }\n",
    "    \n",
    "    # Set false positive rates: Rate of incorrect positive identifications\n",
    "    # Why false positives matter? Higher rates mean more wrongful identifications\n",
    "    false_positive_rates = {\n",
    "        'White': 0.01,  # White: 1% false positive rate (lowest)\n",
    "        'Black': 0.35,  # Black: 35% false positive rate (highest - dangerous!)\n",
    "        'Asian': 0.12,  # Asian: 12% false positive rate\n",
    "        'Hispanic': 0.25  # Hispanic: 25% false positive rate\n",
    "    }\n",
    "    \n",
    "    # Display accuracy rates: Show performance by group\n",
    "    print(\"\\nAccuracy Rates by Demographic Group:\")\n",
    "    for group in groups:  # Loop through groups: Process each demographic group\n",
    "        print(f\"  {group}: {accuracy_rates[group]:.2%}\")  # Print: Group name and accuracy rate\n",
    "    \n",
    "    # Display false positive rates: Show error rates by group\n",
    "    print(\"\\nFalse Positive Rates by Demographic Group:\")\n",
    "    for group in groups:  # Loop through groups: Process each demographic group\n",
    "        print(f\"  {group}: {false_positive_rates[group]:.2%}\")  # Print: Group name and false positive rate\n",
    "    \n",
    "    # Calculate disparity: Measure difference between best and worst performing groups\n",
    "    # Why calculate disparity? Quantifies the extent of bias\n",
    "    max_acc = max(accuracy_rates.values())  # Max accuracy: Highest accuracy rate\n",
    "    min_acc = min(accuracy_rates.values())  # Min accuracy: Lowest accuracy rate\n",
    "    disparity = max_acc - min_acc  # Disparity: Difference between max and min (0.99 - 0.65 = 0.34)\n",
    "    \n",
    "    # Display results: Show disparity and interpretation\n",
    "    print(f\"\\nAccuracy Disparity: {disparity:.2%}\")  # Print: Disparity as percentage\n",
    "    print(\"This represents significant bias against minority groups.\")  # Interpretation: Explain what disparity means\n",
    "    \n",
    "    return groups, accuracy_rates, false_positive_rates  # Return: Groups and metrics for visualization\n",
    "\n",
    "# Run the case study\n",
    "print(\"Running facial recognition bias analysis...\")\n",
    "groups, accuracy_rates, false_positive_rates = case_study_facial_recognition()\n",
    "print(\"\\nâœ… Case study analysis complete!\")\n",
    "# ============================================================================\n",
    "# CASE STUDY 2: GENDER BIAS IN HIRING ALGORITHMS\n",
    "# ============================================================================\n",
    "def case_study_hiring_bias():\n",
    "    \"\"\"\n",
    "    Analyze gender bias in hiring algorithms\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CASE STUDY 2: Gender Bias in Hiring Algorithms\")\n",
    "    print(\"=\"*80)\n",
    "    # Simulated hiring data\n",
    "    np.random.seed(42)\n",
    "    n_candidates = 1000\n",
    "    # Generate candidate data\n",
    "    data = {\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_candidates, p=[0.5, 0.5]),\n",
    "        'experience_years': np.random.randint(0, 20, n_candidates),\n",
    "        'education_level': np.random.choice([1, 2, 3, 4], n_candidates),\n",
    "        'skill_score': np.random.normal(70, 15, n_candidates)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    # Introduce bias: Female candidates have lower hiring probability\n",
    "    base_prob = (df['skill_score'] / 100 + \n",
    "                 df['experience_years'] / 20 + \n",
    "                 df['education_level'] / 4) / 3\n",
    "    # Bias factor: reduce probability for female candidates\n",
    "    bias_factor = np.where(df['gender'] == 'Female', -0.15, 0)\n",
    "    hire_prob = base_prob + bias_factor + np.random.normal(0, 0.05, n_candidates)\n",
    "    hire_prob = np.clip(hire_prob, 0, 1)\n",
    "    df['hired'] = (hire_prob > 0.5).astype(int)\n",
    "    # Analyze by gender\n",
    "    print(\"\\nHiring Rates by Gender:\")\n",
    "    for gender in ['Male', 'Female']:\n",
    "        gender_data = df[df['gender'] == gender]\n",
    "        hire_rate = gender_data['hired'].mean()\n",
    "        avg_skill = gender_data['skill_score'].mean()\n",
    "        print(f\"  {gender}:\")\n",
    "        print(f\"    Hiring Rate: {hire_rate:.2%}\")\n",
    "        print(f\"    Average Skill Score: {avg_skill:.2f}\")\n",
    "    # Calculate disparity\n",
    "    male_rate = df[df['gender'] == 'Male']['hired'].mean()\n",
    "    female_rate = df[df['gender'] == 'Female']['hired'].mean()\n",
    "    disparity = male_rate - female_rate\n",
    "    print(f\"\\nHiring Rate Disparity: {disparity:.2%}\")\n",
    "    print(\"This indicates gender bias in the hiring algorithm.\")\n",
    "    return df\n",
    "# ============================================================================\n",
    "# CASE STUDY 3: PREDICTIVE POLICING AND CRIMINAL JUSTICE AI\n",
    "# ============================================================================\n",
    "def case_study_predictive_policing():\n",
    "    \"\"\"\n",
    "    Analyze bias in predictive policing systems\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CASE STUDY 3: Predictive Policing and Criminal Justice AI\")\n",
    "    print(\"=\"*80)\n",
    "    # Simulated recidivism prediction data\n",
    "    np.random.seed(42)\n",
    "    n_cases = 2000\n",
    "    # Generate data with bias\n",
    "    data = {\n",
    "        'race': np.random.choice(['White', 'Black', 'Hispanic'], n_cases, \n",
    "                                p=[0.6, 0.25, 0.15]),\n",
    "        'age': np.random.randint(18, 65, n_cases),\n",
    "        'prior_convictions': np.random.poisson(2, n_cases),\n",
    "        'employment_status': np.random.choice([0, 1], n_cases, p=[0.4, 0.6])\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    # True recidivism probability (should be race-neutral)\n",
    "    true_prob = (df['prior_convictions'] / 10 + \n",
    "                 (1 - df['employment_status']) * 0.2 + \n",
    "                 np.random.normal(0, 0.1, n_cases))\n",
    "    # Introduce bias: higher false positive rate for minority groups\n",
    "    bias_factor = np.where(df['race'] == 'Black', 0.15, \n",
    "                          np.where(df['race'] == 'Hispanic', 0.10, 0))\n",
    "    predicted_prob = true_prob + bias_factor\n",
    "    predicted_prob = np.clip(predicted_prob, 0, 1)\n",
    "    df['true_recidivism'] = (true_prob > 0.5).astype(int)\n",
    "    df['predicted_high_risk'] = (predicted_prob > 0.5).astype(int)\n",
    "    # Analyze by race\n",
    "    print(\"\\nPrediction Rates by Race:\")\n",
    "    for race in ['White', 'Black', 'Hispanic']:\n",
    "        race_data = df[df['race'] == race]\n",
    "        high_risk_rate = race_data['predicted_high_risk'].mean()\n",
    "        true_rate = race_data['true_recidivism'].mean()\n",
    "        print(f\"  {race}:\")\n",
    "        print(f\"    Predicted High Risk Rate: {high_risk_rate:.2%}\")\n",
    "        print(f\"    True Recidivism Rate: {true_rate:.2%}\")\n",
    "        print(f\"    Disparity: {high_risk_rate - true_rate:.2%}\")\n",
    "    return df\n",
    "# ============================================================================\n",
    "# CASE STUDY 4: CREDIT SCORING AND LENDING AI\n",
    "# ============================================================================\n",
    "def case_study_credit_scoring():\n",
    "    \"\"\"\n",
    "    Analyze discrimination in credit scoring and lending AI\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CASE STUDY 4: Discrimination in Credit Scoring and Lending AI\")\n",
    "    print(\"=\"*80)\n",
    "    # Simulated credit application data\n",
    "    np.random.seed(42)\n",
    "    n_applications = 1500\n",
    "    data = {\n",
    "        'race': np.random.choice(['White', 'Black', 'Hispanic', 'Asian'], \n",
    "                                n_applications, p=[0.5, 0.2, 0.2, 0.1]),\n",
    "        'income': np.random.normal(50000, 20000, n_applications),\n",
    "        'credit_score': np.random.normal(650, 100, n_applications),\n",
    "        'debt_to_income': np.random.uniform(0.1, 0.5, n_applications),\n",
    "        'employment_years': np.random.uniform(0, 20, n_applications)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    # True creditworthiness (should be race-neutral)\n",
    "    true_score = (df['credit_score'] / 850 * 0.4 +\n",
    "                  (1 - df['debt_to_income']) * 0.3 +\n",
    "                  df['employment_years'] / 20 * 0.2 +\n",
    "                  df['income'] / 100000 * 0.1 +\n",
    "                  np.random.normal(0, 0.05, n_applications))\n",
    "    # Introduce bias: lower approval rates for certain groups\n",
    "    bias_penalty = np.where(df['race'] == 'Black', -0.12,\n",
    "                           np.where(df['race'] == 'Hispanic', -0.08, 0))\n",
    "    approval_score = true_score + bias_penalty\n",
    "    approval_score = np.clip(approval_score, 0, 1)\n",
    "    df['approved'] = (approval_score > 0.5).astype(int)\n",
    "    df['true_creditworthy'] = (true_score > 0.5).astype(int)\n",
    "    # Analyze by race\n",
    "    print(\"\\nLoan Approval Rates by Race:\")\n",
    "    for race in ['White', 'Black', 'Hispanic', 'Asian']:\n",
    "        race_data = df[df['race'] == race]\n",
    "        approval_rate = race_data['approved'].mean()\n",
    "        true_rate = race_data['true_creditworthy'].mean()\n",
    "        avg_credit_score = race_data['credit_score'].mean()\n",
    "        print(f\"  {race}:\")\n",
    "        print(f\"    Approval Rate: {approval_rate:.2%}\")\n",
    "        print(f\"    True Creditworthy Rate: {true_rate:.2%}\")\n",
    "        print(f\"    Average Credit Score: {avg_credit_score:.0f}\")\n",
    "        print(f\"    Disparity: {approval_rate - true_rate:.2%}\")\n",
    "    return df\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "def plot_facial_recognition_bias(groups, accuracy_rates, false_positive_rates):\n",
    "    \"\"\"\n",
    "    Plot facial recognition bias analysis\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    acc_values = [accuracy_rates[g] for g in groups]\n",
    "    fpr_values = [false_positive_rates[g] for g in groups]\n",
    "    axes[0].bar(groups, acc_values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "    axes[0].set_title('Facial Recognition Accuracy by Demographic Group', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Accuracy Rate')\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[1].bar(groups, fpr_values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "    axes[1].set_title('False Positive Rate by Demographic Group', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('False Positive Rate')\n",
    "    axes[1].set_ylim([0, 0.4])\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    # Get the directory where this script is located\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    output_path = os.path.join(script_dir, 'case_study_facial_recognition.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nâœ… Saved: case_study_facial_recognition.png\")\n",
    "    plt.close()\n",
    "def plot_hiring_bias(df):\n",
    "    \"\"\"\n",
    "    Plot hiring bias analysis\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    # Hiring rates by gender\n",
    "    gender_rates = df.groupby('gender')['hired'].mean()\n",
    "    axes[0].bar(gender_rates.index, gender_rates.values, \n",
    "               color=['#3498db', '#e74c3c'])\n",
    "    axes[0].set_title('Hiring Rates by Gender', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Hiring Rate')\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    # Skill score distribution by gender and hiring status\n",
    "    for gender in ['Male', 'Female']:\n",
    "        for hired in [0, 1]:\n",
    "            subset = df[(df['gender'] == gender) & (df['hired'] == hired)]\n",
    "            axes[1].hist(subset['skill_score'], alpha=0.5, \n",
    "                       label=f'{gender} - {'Hired\" if hired else \"Not Hired\"}\",\n",
    "                       bins=20)\n",
    "    axes[1].set_title('Skill Score Distribution by Gender and Hiring Status', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Skill Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    # Get the directory where this script is located\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    output_path = os.path.join(script_dir, 'case_study_hiring_bias.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: case_study_hiring_bias.png\")\n",
    "    plt.close()\n",
    "def plot_predictive_policing_bias(df):\n",
    "    \"\"\"\n",
    "    Plot predictive policing bias analysis\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    # Prediction rates by race\n",
    "    race_pred = df.groupby('race')['predicted_high_risk'].mean()\n",
    "    race_true = df.groupby('race')['true_recidivism'].mean()\n",
    "    x = np.arange(len(race_pred.index))\n",
    "    width = 0.35\n",
    "    axes[0].bar(x - width/2, race_pred.values, width, label='Predicted High Risk', \n",
    "               alpha=0.8, color='#e74c3c')\n",
    "    axes[0].bar(x + width/2, race_true.values, width, label='True Recidivism', \n",
    "               alpha=0.8, color='#3498db')\n",
    "    axes[0].set_title('Recidivism Prediction vs Reality by Race', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Rate')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(race_pred.index)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    # Disparity calculation\n",
    "    disparity = race_pred - race_true\n",
    "    axes[1].bar(disparity.index, disparity.values, color='#e74c3c')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[1].set_title('Prediction Disparity by Race', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Disparity (Predicted - True)')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    # Get the directory where this script is located\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    output_path = os.path.join(script_dir, 'case_study_predictive_policing.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: case_study_predictive_policing.png\")\n",
    "    plt.close()\n",
    "def plot_credit_scoring_bias(df):\n",
    "    \"\"\"\n",
    "    Plot credit scoring bias analysis\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    # Approval rates by race\n",
    "    race_approved = df.groupby('race')['approved'].mean()\n",
    "    race_true = df.groupby('race')['true_creditworthy'].mean()\n",
    "    x = np.arange(len(race_approved.index))\n",
    "    width = 0.35\n",
    "    axes[0].bar(x - width/2, race_approved.values, width, label='Approval Rate', \n",
    "               alpha=0.8, color='#2ecc71')\n",
    "    axes[0].bar(x + width/2, race_true.values, width, label='True Creditworthy Rate', \n",
    "               alpha=0.8, color='#3498db')\n",
    "    axes[0].set_title('Loan Approval vs Creditworthiness by Race', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Rate')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(race_approved.index, rotation=15)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    # Average credit score by race\n",
    "    race_credit = df.groupby('race')['credit_score'].mean()\n",
    "    axes[1].bar(race_credit.index, race_credit.values, color='#f39c12')\n",
    "    axes[1].set_title('Average Credit Score by Race', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Credit Score')\n",
    "    axes[1].set_xticklabels(race_credit.index, rotation=15)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    # Get the directory where this script is located\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    output_path = os.path.join(script_dir, 'case_study_credit_scoring.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: case_study_credit_scoring.png\")\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"Unit 2 - Example 4: Bias Case Studies\")\n",
    "    print(\"=\"*80)\n",
    "    # Case Study 1: Facial Recognition\n",
    "    groups, accuracy_rates, false_positive_rates = case_study_facial_recognition()\n",
    "    # Case Study 2: Hiring Bias\n",
    "    df_hiring = case_study_hiring_bias()\n",
    "    # Case Study 3: Predictive Policing\n",
    "    df_policing = case_study_predictive_policing()\n",
    "    # Case Study 4: Credit Scoring\n",
    "    df_credit = case_study_credit_scoring()\n",
    "    # Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating Visualizations...\")\n",
    "    print(\"=\"*80)\n",
    "    plot_facial_recognition_bias(groups, accuracy_rates, false_positive_rates)\n",
    "    plot_hiring_bias(df_hiring)\n",
    "    plot_predictive_policing_bias(df_policing)\n",
    "    plot_credit_scoring_bias(df_credit)\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Takeaways:\")\n",
    "    print(\"1. Facial recognition systems show significant accuracy disparities across racial groups\")\n",
    "    print(\"2. Hiring algorithms can perpetuate gender bias\")\n",
    "    print(\"3. Predictive policing systems may over-predict risk for minority groups\")\n",
    "    print(\"4. Credit scoring algorithms can discriminate against certain demographic groups\")\n",
    "    print(\"5. All these cases highlight the importance of bias detection and mitigation\")\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}