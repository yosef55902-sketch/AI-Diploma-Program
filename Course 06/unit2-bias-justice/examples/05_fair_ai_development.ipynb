{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Fair AI Development | ØªØ·ÙˆÙŠØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø¹Ø§Ø¯Ù„\n",
    "\n",
    "## ðŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Bias Detection** - Understanding how to detect bias\n",
    "- âœ… **Example 2: Bias Mitigation** - Understanding mitigation techniques\n",
    "- âœ… **Example 3: Fair Representation** - Understanding fair representation\n",
    "- âœ… **Example 4: Bias Case Studies** - Understanding real-world bias cases\n",
    "- âœ… **Basic Python knowledge**: Functions, data manipulation, ML concepts\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding how to build fair AI systems from the start\n",
    "- Knowing which practices to follow for fair development\n",
    "- Understanding human-in-the-loop approaches\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FIFTH and FINAL example in Unit 2** - it teaches you how to build fair AI systems from the start!\n",
    "\n",
    "**Why this example LAST?**\n",
    "- **Before** you can build fair systems, you need to detect bias (Example 1)\n",
    "- **Before** you can build fair systems, you need to know mitigation (Example 2)\n",
    "- **Before** you can build fair systems, you need fair representation (Example 3)\n",
    "- **Before** you can build fair systems, you need to learn from cases (Example 4)\n",
    "\n",
    "**Builds on**: \n",
    "- ðŸ““ Example 1: Bias Detection (we know how to detect bias)\n",
    "- ðŸ““ Example 2: Bias Mitigation (we know how to fix bias)\n",
    "- ðŸ““ Example 3: Fair Representation (we know how to ensure fair features)\n",
    "- ðŸ““ Example 4: Bias Case Studies (we learned from real mistakes)\n",
    "\n",
    "**Leads to**: \n",
    "- ðŸ““ Unit 3: Privacy and Security (next unit in the course!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Fair development provides **best practices** (needed after learning all techniques)\n",
    "2. Fair development teaches **prevention** (better than fixing after the fact)\n",
    "3. Fair development shows **complete workflow** (data collection to deployment)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Building Right from the Start | Ø§Ù„Ù‚ØµØ©: Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©\n",
    "\n",
    "Imagine you're building a house. **Before** you start, you need to plan - choose the right location, design for safety, use quality materials. **After** planning, you build a house that's safe, durable, and meets all requirements!\n",
    "\n",
    "Same with AI: **Before** we learned to detect and fix bias, now we learn to build fair systems from the start - inclusive data collection, bias-aware algorithms, human-in-the-loop. **After** fair development, we have systems that are fair by design!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Fair AI Development Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… ØªØ·ÙˆÙŠØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø¹Ø§Ø¯Ù„ØŸ\n",
    "\n",
    "Fair AI development is essential for ethical AI:\n",
    "- **Prevention**: Build fair systems from the start (better than fixing later)\n",
    "- **Efficiency**: Avoid costly fixes and rework\n",
    "- **Trust**: Demonstrate commitment to fairness\n",
    "- **Compliance**: Meet regulatory requirements\n",
    "- **Better Outcomes**: Fair systems lead to better decisions\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Understand inclusive data collection strategies\n",
    "2. Learn bias-aware algorithm design\n",
    "3. Understand human-in-the-loop approaches for fairness\n",
    "4. Learn best practices for fair AI development\n",
    "5. Build a complete fair AI development workflow\n",
    "6. Understand how to prevent bias from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ðŸ“š What each library does:\n",
      "   - numpy/pandas: Data manipulation and numerical operations\n",
      "   - matplotlib/seaborn: Create visualizations (charts, heatmaps)\n",
      "   - sklearn: Machine learning (models, metrics, preprocessing)\n",
      "   - fairlearn: Fairness tools (metrics, evaluation)\n",
      "   - os: File operations (saving images)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us build fair AI systems from the start\n",
    "\n",
    "import numpy as np  # For numerical operations: Arrays, calculations, random number generation\n",
    "import pandas as pd  # For data manipulation: DataFrames, data analysis\n",
    "import matplotlib.pyplot as plt  # For creating visualizations: Charts, graphs, comparisons\n",
    "import seaborn as sns  # For statistical visualizations: Heatmaps, advanced plots\n",
    "from sklearn.model_selection import train_test_split  # For splitting data: Separate training and testing sets\n",
    "from sklearn.ensemble import RandomForestClassifier  # For ML model: Classification algorithm\n",
    "from sklearn.preprocessing import StandardScaler  # For data preprocessing: Feature scaling\n",
    "from sklearn.metrics import accuracy_score, classification_report  # For model evaluation: Performance metrics\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference  # For fairness metrics: Fairlearn library\n",
    "import warnings  # For suppressing warnings: Clean output\n",
    "import os  # For file operations: Saving images\n",
    "\n",
    "# Suppress warnings: Clean output (fairlearn may show warnings)\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings: Suppress non-critical warnings\n",
    "\n",
    "# Configure plotting: Set default styles for better visualizations\n",
    "plt.rcParams['font.size'] = 10  # Font size: Make text readable (10pt is good for most displays)\n",
    "plt.rcParams['figure.figsize'] = (14, 8)  # Figure size: 14 inches wide, 8 inches tall (good for detailed charts)\n",
    "sns.set_style(\"whitegrid\")  # Style: White background with grid for clean look\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nðŸ“š What each library does:\")\n",
    "print(\"   - numpy/pandas: Data manipulation and numerical operations\")\n",
    "print(\"   - matplotlib/seaborn: Create visualizations (charts, heatmaps)\")\n",
    "print(\"   - sklearn: Machine learning (models, metrics, preprocessing)\")\n",
    "print(\"   - fairlearn: Fairness tools (metrics, evaluation)\")\n",
    "print(\"   - os: File operations (saving images)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“Š INCLUSIVE DATA COLLECTION STRATEGIES\n",
      "================================================================================\n",
      "\n",
      "Different data collection strategies:\n",
      "  - Unbalanced: Majority group over-represented\n",
      "  - Balanced: Equal representation across groups\n",
      "  - Oversampled Minority: Minority groups over-represented (for fairness)\n",
      "\n",
      "Analyzing data collection strategies...\n",
      "\n",
      "Data Collection Strategies:\n",
      "\n",
      "Unbalanced:\n",
      "  Group_A: 800 samples (80.0%)\n",
      "  Group_B: 200 samples (20.0%)\n",
      "  Group_C: 0 samples (0.0%)\n",
      "\n",
      "Balanced:\n",
      "  Group_A: 400 samples (40.0%)\n",
      "  Group_B: 400 samples (40.0%)\n",
      "  Group_C: 200 samples (20.0%)\n",
      "\n",
      "Oversampled_Minority:\n",
      "  Group_A: 400 samples (33.3%)\n",
      "  Group_B: 400 samples (33.3%)\n",
      "  Group_C: 400 samples (33.3%)\n",
      "\n",
      "âœ… Data collection analysis complete!\n",
      "\n",
      "Key Insight: Inclusive data collection is the foundation of fair AI!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Demonstrate inclusive data collection strategies\n",
    "# This shows how different data collection approaches affect representation\n",
    "\n",
    "# BEFORE: We collect data without considering representation\n",
    "# AFTER: We'll see how different strategies affect group representation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š INCLUSIVE DATA COLLECTION STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDifferent data collection strategies:\")\n",
    "print(\"  - Unbalanced: Majority group over-represented\")\n",
    "print(\"  - Balanced: Equal representation across groups\")\n",
    "print(\"  - Oversampled Minority: Minority groups over-represented (for fairness)\\n\")\n",
    "\n",
    "def inclusive_data_collection():\n",
    "    \"\"\"\n",
    "    Demonstrate inclusive data collection strategies.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Defines three data collection scenarios (Unbalanced, Balanced, Oversampled)\n",
    "    2. Shows sample counts for each group in each scenario\n",
    "    3. Calculates percentages to show representation\n",
    "    4. Compares strategies to highlight importance of inclusive collection\n",
    "    \n",
    "    â° WHEN to use: To understand how data collection affects representation\n",
    "    ðŸ’¡ WHY use: Shows that data collection strategy matters for fairness\n",
    "    \"\"\"\n",
    "    # Define scenarios: Three different data collection approaches\n",
    "    # Why three scenarios? Shows progression from biased to fair collection\n",
    "    scenarios = {\n",
    "        'Unbalanced': {  # Unbalanced: Majority group over-represented (common problem)\n",
    "            'Group_A': 800,  # Group A: 80% of data (majority)\n",
    "            'Group_B': 200,  # Group B: 20% of data (minority)\n",
    "            'Group_C': 0  # Group C: 0% of data (completely missing!)\n",
    "        },\n",
    "        'Balanced': {  # Balanced: More equal representation\n",
    "            'Group_A': 400,  # Group A: 40% of data\n",
    "            'Group_B': 400,  # Group B: 40% of data\n",
    "            'Group_C': 200  # Group C: 20% of data (still underrepresented but present)\n",
    "        },\n",
    "        'Oversampled_Minority': {  # Oversampled: Minority groups over-represented (for fairness)\n",
    "            'Group_A': 400,  # Group A: 33.3% of data\n",
    "            'Group_B': 400,  # Group B: 33.3% of data\n",
    "            'Group_C': 400  # Group C: 33.3% of data (equal representation)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Display scenarios: Show each strategy and its group representation\n",
    "    print(\"\\nData Collection Strategies:\")\n",
    "    for scenario, counts in scenarios.items():  # Loop through scenarios: Process each strategy\n",
    "        total = sum(counts.values())  # Total: Sum of all samples\n",
    "        print(f\"\\n{scenario}:\")  # Print: Scenario name\n",
    "        for group, count in counts.items():  # Loop through groups: Process each group\n",
    "            percentage = count / total * 100 if total > 0 else 0  # Percentage: Calculate representation percentage\n",
    "            print(f\"  {group}: {count} samples ({percentage:.1f}%)\")  # Print: Group name, count, percentage\n",
    "    \n",
    "    return scenarios  # Return: Dictionary of scenarios for analysis\n",
    "\n",
    "# Run the data collection demonstration\n",
    "print(\"Analyzing data collection strategies...\")\n",
    "scenarios = inclusive_data_collection()\n",
    "print(\"\\nâœ… Data collection analysis complete!\")\n",
    "print(\"\\nKey Insight: Inclusive data collection is the foundation of fair AI!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bias-Aware Algorithms and Human-in-the-Loop | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ÙˆØ§Ø¹ÙŠØ© Ø¨Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„ØªØ¯Ø®Ù„ Ø§Ù„Ø¨Ø´Ø±ÙŠ\n",
    "\n",
    "### ðŸ“š Prerequisites (What You Need First)\n",
    "- âœ… **Library imports** (from Part 1) - Understanding data manipulation tools\n",
    "- âœ… **Understanding of bias** (from Examples 1-4) - Knowing how bias manifests\n",
    "- âœ… **Inclusive data collection** (from Step 2) - Understanding data collection strategies\n",
    "\n",
    "### ðŸ”— Relationship: What This Builds On\n",
    "This builds on inclusive data collection to show how to design algorithms and workflows that are fair by design!\n",
    "- Builds on: Inclusive data collection, bias detection, and mitigation techniques\n",
    "- Shows: How to incorporate fairness into algorithm design and evaluation workflows\n",
    "\n",
    "### ðŸ“– The Story\n",
    "**Before**: We collect inclusive data, but algorithms might still be biased.\n",
    "**After**: We design bias-aware algorithms and use human-in-the-loop evaluation to ensure fairness!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Bias-Aware Algorithms and Human-in-the-Loop\n",
    "# This demonstrates how to build fair AI systems with bias-aware algorithms and human oversight\n",
    "\n",
    "# ============================================================================\n",
    "# BIAS-AWARE ALGORITHMS\n",
    "# ============================================================================\n",
    "def generate_fair_dataset(n_samples=2000):\n",
    "    \"\"\"\n",
    "    Generate dataset for fair AI development\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    # Multiple sensitive attributes\n",
    "    sensitive_1 = np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
    "    sensitive_2 = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "    # Features\n",
    "    X1 = np.random.normal(0, 1, n_samples)\n",
    "    X2 = np.random.normal(0, 1, n_samples)\n",
    "    X3 = np.random.normal(0, 1, n_samples)\n",
    "    # Target (should be independent of sensitive attributes)\n",
    "    y = (0.4 * X1 + 0.3 * X2 + 0.3 * X3 + np.random.normal(0, 0.1, n_samples) > 0).astype(int)\n",
    "    df = pd.DataFrame({\n",
    "        'feature1': X1,\n",
    "        'feature2': X2,\n",
    "        'feature3': X3,\n",
    "        'sensitive_1': sensitive_1,\n",
    "        'sensitive_2': sensitive_2,\n",
    "        'target': y\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def bias_aware_training(X_train, y_train, sensitive_train, method='fairness_constraint'):\n",
    "    \"\"\"\n",
    "    Train model with bias-aware techniques\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    if method == 'fairness_constraint':\n",
    "        # Use sample weights to balance groups\n",
    "        group_counts = pd.Series(sensitive_train).value_counts()\n",
    "        total = len(sensitive_train)\n",
    "        weights = np.ones(len(sensitive_train))\n",
    "        for group in group_counts.index:\n",
    "            weights[sensitive_train == group] = total / (len(group_counts) * group_counts[group])\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_scaled, y_train, sample_weight=weights)\n",
    "    else:\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    return model, scaler\n",
    "\n",
    "# ============================================================================\n",
    "# HUMAN-IN-THE-LOOP FAIRNESS EVALUATION\n",
    "# ============================================================================\n",
    "def human_in_the_loop_evaluation(model, X_test, y_test, sensitive_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Simulate human-in-the-loop fairness evaluation\n",
    "    \"\"\"\n",
    "    # Get model predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > threshold).astype(int)\n",
    "    # Identify uncertain cases (close to threshold)\n",
    "    uncertainty = np.abs(y_pred_proba - threshold)\n",
    "    uncertain_mask = uncertainty < 0.1  # Cases within 10% of threshold\n",
    "    # Human review for uncertain cases\n",
    "    human_reviewed = y_pred.copy()\n",
    "    human_correct = 0\n",
    "    human_reviewed_count = uncertain_mask.sum()\n",
    "    if human_reviewed_count > 0:\n",
    "        # Simulate human review (assume 80% accuracy for human)\n",
    "        human_predictions = y_test[uncertain_mask].copy()\n",
    "        # Add some human error\n",
    "        human_error = np.random.random(human_reviewed_count) < 0.2\n",
    "        human_predictions[human_error] = 1 - human_predictions[human_error]\n",
    "        human_reviewed[uncertain_mask] = human_predictions\n",
    "        human_correct = (human_predictions == y_test[uncertain_mask]).sum()\n",
    "    return {\n",
    "        'automated': y_pred,\n",
    "        'human_reviewed': human_reviewed,\n",
    "        'uncertain_count': human_reviewed_count,\n",
    "        'human_correct': human_correct,\n",
    "        'uncertain_mask': uncertain_mask\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# COMPREHENSIVE FAIR AI DEVELOPMENT PIPELINE\n",
    "# ============================================================================\n",
    "def fair_ai_development_pipeline(df):\n",
    "    \"\"\"\n",
    "    Complete pipeline for fair AI development\n",
    "    \"\"\"\n",
    "    X = df[['feature1', 'feature2', 'feature3']].values\n",
    "    y = df['target'].values\n",
    "    sensitive = df['sensitive_1'].values\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "        X, y, sensitive, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    results = {}\n",
    "    # 1. Standard training (baseline)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. BIAS-AWARE ALGORITHMS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nBaseline Model (No Fairness Constraints):\")\n",
    "    baseline_model, baseline_scaler = bias_aware_training(\n",
    "        X_train, y_train, sensitive_train, method='standard'\n",
    "    )\n",
    "    X_test_scaled = baseline_scaler.transform(X_test)\n",
    "    y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "    results['Baseline'] = {\n",
    "        'model': baseline_model,\n",
    "        'scaler': baseline_scaler,\n",
    "        'predictions': y_pred_baseline,\n",
    "        'accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "        'dp_diff': demographic_parity_difference(y_test, y_pred_baseline, sensitive_features=sensitive_test),\n",
    "        'eo_diff': equalized_odds_difference(y_test, y_pred_baseline, sensitive_features=sensitive_test)\n",
    "    }\n",
    "    print(f\"  Accuracy: {results['Baseline']['accuracy']:.4f}\")\n",
    "    print(f\"  Demographic Parity Difference: {results['Baseline']['dp_diff']:.4f}\")\n",
    "    print(f\"  Equalized Odds Difference: {results['Baseline']['eo_diff']:.4f}\")\n",
    "    # 2. Fairness-constrained training\n",
    "    print(\"\\nFairness-Constrained Model:\")\n",
    "    fair_model, fair_scaler = bias_aware_training(\n",
    "        X_train, y_train, sensitive_train, method='fairness_constraint'\n",
    "    )\n",
    "    X_test_fair = fair_scaler.transform(X_test)\n",
    "    y_pred_fair = fair_model.predict(X_test_fair)\n",
    "    results['Fair'] = {\n",
    "        'model': fair_model,\n",
    "        'scaler': fair_scaler,\n",
    "        'predictions': y_pred_fair,\n",
    "        'accuracy': accuracy_score(y_test, y_pred_fair),\n",
    "        'dp_diff': demographic_parity_difference(y_test, y_pred_fair, sensitive_features=sensitive_test),\n",
    "        'eo_diff': equalized_odds_difference(y_test, y_pred_fair, sensitive_features=sensitive_test)\n",
    "    }\n",
    "    print(f\"  Accuracy: {results['Fair']['accuracy']:.4f}\")\n",
    "    print(f\"  Demographic Parity Difference: {results['Fair']['dp_diff']:.4f}\")\n",
    "    print(f\"  Equalized Odds Difference: {results['Fair']['eo_diff']:.4f}\")\n",
    "    # 3. Human-in-the-loop evaluation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. HUMAN-IN-THE-LOOP FAIRNESS EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    hitl_results = human_in_the_loop_evaluation(\n",
    "        fair_model, X_test_fair, y_test, sensitive_test\n",
    "    )\n",
    "    results['HITL'] = {\n",
    "        'automated': hitl_results['automated'],\n",
    "        'human_reviewed': hitl_results['human_reviewed'],\n",
    "        'uncertain_count': hitl_results['uncertain_count'],\n",
    "        'human_correct': hitl_results['human_correct']\n",
    "    }\n",
    "    print(f\"\\nAutomated Predictions: {len(hitl_results['automated'])}\")\n",
    "    print(f\"Uncertain Cases (Human Review): {hitl_results['uncertain_count']}\")\n",
    "    if hitl_results['uncertain_count'] > 0:\n",
    "        print(f\"Human Review Accuracy: {hitl_results['human_correct'] / hitl_results['uncertain_count']:.2%}\")\n",
    "    # Evaluate HITL performance\n",
    "    hitl_accuracy = accuracy_score(y_test, hitl_results['human_reviewed'])\n",
    "    hitl_dp_diff = demographic_parity_difference(\n",
    "        y_test, hitl_results['human_reviewed'], sensitive_features=sensitive_test\n",
    "    )\n",
    "    hitl_eo_diff = equalized_odds_difference(\n",
    "        y_test, hitl_results['human_reviewed'], sensitive_features=sensitive_test\n",
    "    )\n",
    "    results['HITL']['accuracy'] = hitl_accuracy\n",
    "    results['HITL']['dp_diff'] = hitl_dp_diff\n",
    "    results['HITL']['eo_diff'] = hitl_eo_diff\n",
    "    print(f\"\\nHITL Model Performance:\")\n",
    "    print(f\"  Accuracy: {hitl_accuracy:.4f}\")\n",
    "    print(f\"  Demographic Parity Difference: {hitl_dp_diff:.4f}\")\n",
    "    print(f\"  Equalized Odds Difference: {hitl_eo_diff:.4f}\")\n",
    "    return results, sensitive_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualization Functions\n",
    "# These functions create visualizations to compare different approaches\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "def plot_data_collection_strategies(scenarios):\n",
    "    \"\"\"\n",
    "    Plot data collection strategy comparison\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    groups = ['Group_A', 'Group_B', 'Group_C']\n",
    "    x = np.arange(len(groups))\n",
    "    width = 0.25\n",
    "    for idx, (scenario, counts) in enumerate(scenarios.items()):\n",
    "        values = [counts.get(g, 0) for g in groups]\n",
    "        ax.bar(x + idx * width, values, width, label=scenario.replace('_', ' '), alpha=0.8)\n",
    "    ax.set_title('Data Collection Strategies Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    ax.set_xlabel('Demographic Group')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    # Save to current directory (examples folder)\n",
    "    output_path = 'fair_ai_data_collection.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ… Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_fairness_comparison(results, sensitive_test):\n",
    "    \"\"\"\n",
    "    Plot fairness metrics comparison\n",
    "    \"\"\"\n",
    "    methods = ['Baseline', 'Fair', 'HITL']\n",
    "    dp_diffs = [abs(results[m]['dp_diff']) for m in methods]\n",
    "    eo_diffs = [abs(results[m]['eo_diff']) for m in methods]\n",
    "    accuracies = [results[m]['accuracy'] for m in methods]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axes[0].bar(methods, dp_diffs, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "    axes[0].set_title('Demographic Parity Difference (Lower is Better)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Absolute Difference')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[1].bar(methods, eo_diffs, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "    axes[1].set_title('Equalized Odds Difference (Lower is Better)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Absolute Difference')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[2].bar(methods, accuracies, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "    axes[2].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].set_ylim([0, 1])\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    # Save to current directory (examples folder)\n",
    "    output_path = 'fair_ai_comparison.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… Saved: {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_hitl_workflow(results):\n",
    "    \"\"\"\n",
    "    Plot human-in-the-loop workflow visualization\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # Create workflow diagram\n",
    "    stages = ['Automated\\nPrediction', 'Uncertainty\\nDetection', 'Human\\nReview', 'Final\\nDecision']\n",
    "    counts = [\n",
    "        len(results['HITL']['automated']),\n",
    "        results['HITL']['uncertain_count'],\n",
    "        results['HITL']['uncertain_count'],\n",
    "        len(results['HITL']['human_reviewed'])\n",
    "    ]\n",
    "    colors = ['#3498db', '#f39c12', '#2ecc71', '#9b59b6']\n",
    "    bars = ax.bar(stages, counts, color=colors, alpha=0.8)\n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{count}',\n",
    "               ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Human-in-the-Loop Fairness Evaluation Workflow', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Cases')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    # Save to current directory (examples folder)\n",
    "    output_path = 'fair_ai_hitl_workflow.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… Saved: {output_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Unit 2 - Example 5: Practical Approaches to Fair AI Development\n",
      "================================================================================\n",
      "\n",
      "Generating dataset for fair AI development...\n",
      "Dataset shape: (2000, 6)\n",
      "\n",
      "================================================================================\n",
      "2. BIAS-AWARE ALGORITHMS\n",
      "================================================================================\n",
      "\n",
      "Baseline Model (No Fairness Constraints):\n",
      "  Accuracy: 0.9383\n",
      "  Demographic Parity Difference: 0.0477\n",
      "  Equalized Odds Difference: 0.0229\n",
      "\n",
      "Fairness-Constrained Model:\n",
      "  Accuracy: 0.9400\n",
      "  Demographic Parity Difference: 0.0614\n",
      "  Equalized Odds Difference: 0.0394\n",
      "\n",
      "================================================================================\n",
      "3. HUMAN-IN-THE-LOOP FAIRNESS EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Automated Predictions: 600\n",
      "Uncertain Cases (Human Review): 40\n",
      "Human Review Accuracy: 77.50%\n",
      "\n",
      "HITL Model Performance:\n",
      "  Accuracy: 0.9533\n",
      "  Demographic Parity Difference: 0.0489\n",
      "  Equalized Odds Difference: 0.0111\n",
      "\n",
      "================================================================================\n",
      "Creating Visualizations...\n",
      "================================================================================\n",
      "\n",
      "âœ… Saved: fair_ai_data_collection.png\n",
      "âœ… Saved: fair_ai_comparison.png\n",
      "âœ… Saved: fair_ai_hitl_workflow.png\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. Inclusive data collection ensures representation of all groups\n",
      "2. Bias-aware algorithms incorporate fairness constraints during training\n",
      "3. Human-in-the-loop approaches improve fairness for uncertain cases\n",
      "4. Combining these approaches leads to more fair and trustworthy AI systems\n",
      "5. Continuous monitoring and evaluation are essential for maintaining fairness\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Run the Complete Fair AI Development Pipeline\n",
    "# This executes the entire workflow from data generation to visualization\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"Unit 2 - Example 5: Practical Approaches to Fair AI Development\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Generate dataset and run pipeline\n",
    "print(\"\\nGenerating dataset for fair AI development...\")\n",
    "df = generate_fair_dataset(n_samples=2000)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Run comprehensive pipeline\n",
    "results, sensitive_test = fair_ai_development_pipeline(df)\n",
    "\n",
    "# Create visualizations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Visualizations...\")\n",
    "print(\"=\"*80)\n",
    "plot_data_collection_strategies(scenarios)\n",
    "plot_fairness_comparison(results, sensitive_test)\n",
    "plot_hitl_workflow(results)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Inclusive data collection ensures representation of all groups\")\n",
    "print(\"2. Bias-aware algorithms incorporate fairness constraints during training\")\n",
    "print(\"3. Human-in-the-loop approaches improve fairness for uncertain cases\")\n",
    "print(\"4. Combining these approaches leads to more fair and trustworthy AI systems\")\n",
    "print(\"5. Continuous monitoring and evaluation are essential for maintaining fairness\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary: What We Learned | Ø§Ù„Ù…Ù„Ø®Øµ: Ù…Ø§ ØªØ¹Ù„Ù…Ù†Ø§Ù‡\n",
    "\n",
    "**BEFORE this notebook**: We learned to detect and fix bias, but hadn't learned how to build fair systems from the start.\n",
    "\n",
    "**AFTER this notebook**: We can:\n",
    "- âœ… Understand inclusive data collection strategies\n",
    "- âœ… Design bias-aware algorithms\n",
    "- âœ… Implement human-in-the-loop approaches for fairness\n",
    "- âœ… Apply best practices for fair AI development\n",
    "- âœ… Build complete fair AI development workflows\n",
    "- âœ… Prevent bias from the start (better than fixing later!)\n",
    "\n",
    "### Key Takeaways | Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Inclusive Data Collection**: Foundation of fair AI - ensure all groups are represented\n",
    "2. **Bias-Aware Algorithms**: Incorporate fairness constraints during training\n",
    "3. **Human-in-the-Loop**: Use human judgment for uncertain cases to improve fairness\n",
    "4. **Prevention**: Building fair systems from the start is more efficient than fixing later\n",
    "5. **Complete Workflow**: Fair development requires attention at every stage (data â†’ model â†’ deployment)\n",
    "\n",
    "### Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "- ðŸ““ **Unit 3: Privacy and Security** (next unit in the course!)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** ðŸŽ‰ You've completed Unit 2 and learned how to build fair AI systems from the ground up!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
