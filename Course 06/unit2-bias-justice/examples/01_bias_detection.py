"""
Unit 2: Bias, Justice, and Discrimination in AI
Example 1: Bias Detection in Machine Learning Models
Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙˆØ§Ù„ØªÙ…ÙŠÙŠØ² ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - Ù…Ø«Ø§Ù„ 1: Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªØ­ÙŠØ² ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©

This example demonstrates how to detect bias in ML models using fairness metrics.
Note: This example uses synthetic data. In practice, use fairlearn or aif360 libraries.
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

plt.rcParams['font.size'] = 10
plt.rcParams['figure.figsize'] = (14, 8)

# ============================================================================
# GENERATE SYNTHETIC DATA WITH BIAS
# Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù…Ø¹ ØªØ­ÙŠØ²
# ============================================================================

def generate_biased_data(n_samples=2000):
    """
    Generate synthetic hiring data with inherent bias
    Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªÙˆØ¸ÙŠÙ Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù…Ø¹ ØªØ­ÙŠØ² Ù…ØªØ£ØµÙ„
    """
    np.random.seed(42)
    
    # Create synthetic dataset
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©
    data = {
        'age': np.random.randint(22, 65, n_samples),
        'experience_years': np.random.randint(0, 20, n_samples),
        'education_level': np.random.choice([1, 2, 3, 4], n_samples, 
                                           p=[0.2, 0.3, 0.3, 0.2]),  # 1-4 scale
        'skill_score': np.random.normal(70, 15, n_samples),
        'group': np.random.choice(['Group_A', 'Group_B'], n_samples, p=[0.5, 0.5])
    }
    
    df = pd.DataFrame(data)
    
    # Introduce bias: Group_B has lower success rates even with similar qualifications
    # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØªØ­ÙŠØ²: Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ Ù„Ø¯ÙŠÙ‡Ø§ Ù…Ø¹Ø¯Ù„Ø§Øª Ù†Ø¬Ø§Ø­ Ø£Ù‚Ù„ Ø­ØªÙ‰ Ù…Ø¹ Ù…Ø¤Ù‡Ù„Ø§Øª Ù…Ù…Ø§Ø«Ù„Ø©
    bias_factor = np.where(df['group'] == 'Group_B', -0.15, 0)
    
    # Calculate hiring probability (biased)
    # Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ØªÙˆØ¸ÙŠÙ (Ù…ØªØ­ÙŠØ²Ø©)
    base_prob = (df['skill_score'] / 100 + 
                 df['experience_years'] / 20 + 
                 df['education_level'] / 4) / 3 + bias_factor
    
    # Add some noise
    base_prob += np.random.normal(0, 0.1, n_samples)
    base_prob = np.clip(base_prob, 0, 1)
    
    # Create binary outcome (hired = 1, not hired = 0)
    df['hired'] = (base_prob > 0.5).astype(int)
    
    return df

# ============================================================================
# BIAS DETECTION FUNCTIONS
# Ø¯ÙˆØ§Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªØ­ÙŠØ²
# ============================================================================

def calculate_demographic_parity(df, group_col='group', outcome_col='hired'):
    """
    Calculate demographic parity (statistical parity)
    Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ§ÙØ¤ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ (Ø§Ù„ØªÙƒØ§ÙØ¤ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ)
    
    Demographic Parity: P(Å¶=1|A=a) = P(Å¶=1|A=b) for all groups
    """
    groups = df[group_col].unique()
    parity_rates = {}
    
    for group in groups:
        group_data = df[df[group_col] == group]
        positive_rate = group_data[outcome_col].mean()
        parity_rates[group] = positive_rate
    
    # Calculate disparity
    rates = list(parity_rates.values())
    disparity = max(rates) - min(rates)
    
    return parity_rates, disparity

def calculate_equalized_odds(df, group_col='group', outcome_col='hired', 
                             prediction_col='predicted'):
    """
    Calculate equalized odds
    Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ§ÙØ¤ ÙÙŠ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
    
    Equalized Odds: P(Å¶=1|Y=y, A=a) = P(Å¶=1|Y=y, A=b) for all groups and outcomes
    """
    groups = df[group_col].unique()
    metrics = {}
    
    for group in groups:
        group_data = df[df[group_col] == group]
        
        # True Positive Rate (TPR) / Sensitivity
        tp = ((group_data[outcome_col] == 1) & 
              (group_data[prediction_col] == 1)).sum()
        fn = ((group_data[outcome_col] == 1) & 
              (group_data[prediction_col] == 0)).sum()
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # False Positive Rate (FPR)
        fp = ((group_data[outcome_col] == 0) & 
              (group_data[prediction_col] == 1)).sum()
        tn = ((group_data[outcome_col] == 0) & 
              (group_data[prediction_col] == 0)).sum()
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        metrics[group] = {'TPR': tpr, 'FPR': fpr}
    
    # Calculate disparities
    tprs = [m['TPR'] for m in metrics.values()]
    fprs = [m['FPR'] for m in metrics.values()]
    tpr_disparity = max(tprs) - min(tprs)
    fpr_disparity = max(fprs) - min(fprs)
    
    return metrics, tpr_disparity, fpr_disparity

# ============================================================================
# TRAIN MODEL AND DETECT BIAS
# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§ÙƒØªØ´Ø§Ù Ø§Ù„ØªØ­ÙŠØ²
# ============================================================================

def train_and_analyze_bias(df):
    """Train a model and analyze for bias"""
    # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ²
    
    # Prepare features
    X = df[['age', 'experience_years', 'education_level', 'skill_score']]
    y = df['hired']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # Train model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Add predictions to test data
    test_df = X_test.copy()
    test_df['hired'] = y_test.values
    test_df['predicted'] = y_pred
    test_df['group'] = df.loc[X_test.index, 'group'].values
    
    # Calculate bias metrics
    parity_rates, parity_disparity = calculate_demographic_parity(test_df)
    equalized_odds, tpr_disparity, fpr_disparity = calculate_equalized_odds(test_df)
    
    return test_df, model, parity_rates, parity_disparity, equalized_odds, tpr_disparity, fpr_disparity

# ============================================================================
# VISUALIZATIONS
# Ø§Ù„ØªØµÙˆØ±Ø§Øª
# ============================================================================

def visualize_demographic_parity(parity_rates, disparity):
    """Visualize demographic parity"""
    # ØªØµÙˆØ± Ø§Ù„ØªÙƒØ§ÙØ¤ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ
    
    groups = list(parity_rates.keys())
    rates = list(parity_rates.values())
    colors = ['#3498db', '#e74c3c']
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.bar(groups, rates, color=colors, alpha=0.8, edgecolor='black', linewidth=2)
    
    # Add value labels
    for bar, rate in zip(bars, rates):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{rate:.3f}\n({rate*100:.1f}%)',
               ha='center', va='bottom', fontweight='bold', fontsize=11)
    
    # Add disparity line
    ax.axhline(y=max(rates), color='red', linestyle='--', alpha=0.5, label='Max')
    ax.axhline(y=min(rates), color='blue', linestyle='--', alpha=0.5, label='Min')
    
    ax.set_ylabel('Positive Prediction Rate / Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 
                  fontsize=12, fontweight='bold')
    ax.set_title(f'Demographic Parity Analysis\n'
                f'ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§ÙØ¤ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ\n'
                f'Disparity: {disparity:.3f} / Ø§Ù„ÙØ±Ù‚: {disparity:.3f}',
                fontsize=14, fontweight='bold', pad=20)
    ax.set_ylim(0, max(rates) * 1.2)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('demographic_parity.png',
                dpi=300, bbox_inches='tight')
    print("âœ… Saved: demographic_parity.png")
    plt.close()

def visualize_equalized_odds(equalized_odds, tpr_disparity, fpr_disparity):
    """Visualize equalized odds metrics"""
    # ØªØµÙˆØ± Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙƒØ§ÙØ¤ ÙÙŠ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
    
    groups = list(equalized_odds.keys())
    tprs = [equalized_odds[g]['TPR'] for g in groups]
    fprs = [equalized_odds[g]['FPR'] for g in groups]
    
    x = np.arange(len(groups))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars1 = ax.bar(x - width/2, tprs, width, label='True Positive Rate (TPR) / Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ',
                   color='#2ecc71', alpha=0.8, edgecolor='black')
    bars2 = ax.bar(x + width/2, fprs, width, label='False Positive Rate (FPR) / Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ø§Ù„Ø®Ø§Ø·Ø¦',
                   color='#e74c3c', alpha=0.8, edgecolor='black')
    
    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}',
                   ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax.set_xlabel('Group / Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©', fontsize=12, fontweight='bold')
    ax.set_ylabel('Rate / Ø§Ù„Ù…Ø¹Ø¯Ù„', fontsize=12, fontweight='bold')
    ax.set_title(f'Equalized Odds Analysis\n'
                f'ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§ÙØ¤ ÙÙŠ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª\n'
                f'TPR Disparity: {tpr_disparity:.3f} | FPR Disparity: {fpr_disparity:.3f}',
                fontsize=14, fontweight='bold', pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels(groups)
    ax.legend(fontsize=10)
    ax.set_ylim(0, max(max(tprs), max(fprs)) * 1.2)
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('equalized_odds.png',
                dpi=300, bbox_inches='tight')
    print("âœ… Saved: equalized_odds.png")
    plt.close()

def visualize_confusion_matrices(test_df):
    """Visualize confusion matrices by group"""
    # ØªØµÙˆØ± Ù…ØµÙÙˆÙØ§Øª Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
    
    groups = test_df['group'].unique()
    fig, axes = plt.subplots(1, len(groups), figsize=(14, 5))
    
    for idx, group in enumerate(groups):
        group_data = test_df[test_df['group'] == group]
        cm = confusion_matrix(group_data['hired'], group_data['predicted'])
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                   cbar_kws={'label': 'Count / Ø§Ù„Ø¹Ø¯Ø¯'})
        axes[idx].set_title(f'{group}\nConfusion Matrix / Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ',
                           fontsize=12, fontweight='bold')
        axes[idx].set_xlabel('Predicted / Ø§Ù„Ù…ØªÙˆÙ‚Ø¹', fontsize=10)
        axes[idx].set_ylabel('Actual / Ø§Ù„ÙØ¹Ù„ÙŠ', fontsize=10)
        axes[idx].set_xticklabels(['Not Hired', 'Hired'])
        axes[idx].set_yticklabels(['Not Hired', 'Hired'])
    
    plt.tight_layout()
    plt.savefig('confusion_matrices_by_group.png',
                dpi=300, bbox_inches='tight')
    print("âœ… Saved: confusion_matrices_by_group.png")
    plt.close()

# ============================================================================
# MAIN EXECUTION
# Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("Unit 2 - Example 1: Bias Detection in ML Models")
    print("Ø§Ù„ÙˆØ­Ø¯Ø© 2 - Ù…Ø«Ø§Ù„ 1: Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªØ­ÙŠØ² ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©")
    print("="*80)
    
    # Generate data
    print("\nğŸ“Š Generating synthetic data with bias...")
    print("Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù…Ø¹ ØªØ­ÙŠØ²...")
    df = generate_biased_data(n_samples=2000)
    
    # Show data summary
    print("\nğŸ“‹ Data Summary / Ù…Ù„Ø®Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
    print("-" * 60)
    print(f"Total samples: {len(df)}")
    print(f"Groups: {df['group'].value_counts().to_dict()}")
    print(f"\nHiring rates by group:")
    for group in df['group'].unique():
        rate = df[df['group'] == group]['hired'].mean()
        print(f"  {group}: {rate:.3f} ({rate*100:.1f}%)")
    
    # Train model and analyze
    print("\nğŸ” Training model and analyzing bias...")
    print("ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ²...")
    test_df, model, parity_rates, parity_disparity, equalized_odds, tpr_disparity, fpr_disparity = train_and_analyze_bias(df)
    
    # Print results
    print("\nğŸ“Š BIAS DETECTION RESULTS / Ù†ØªØ§Ø¦Ø¬ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªØ­ÙŠØ²:")
    print("="*80)
    
    print("\n1. Demographic Parity / Ø§Ù„ØªÙƒØ§ÙØ¤ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ:")
    print("-" * 60)
    for group, rate in parity_rates.items():
        print(f"  {group}: {rate:.3f} ({rate*100:.1f}%)")
    print(f"\n  Disparity / Ø§Ù„ÙØ±Ù‚: {parity_disparity:.3f}")
    if parity_disparity > 0.1:
        print("  âš ï¸  HIGH DISPARITY - Potential bias detected!")
        print("     ØªØ­ÙŠØ² Ù…Ø­ØªÙ…Ù„ - ÙØ±Ù‚ Ø¹Ø§Ù„ÙŠ!")
    else:
        print("  âœ… Low disparity - Fair from demographic parity perspective")
        print("     ÙØ±Ù‚ Ù…Ù†Ø®ÙØ¶ - Ø¹Ø§Ø¯Ù„ Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ø§Ù„ØªÙƒØ§ÙØ¤ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ")
    
    print("\n2. Equalized Odds / Ø§Ù„ØªÙƒØ§ÙØ¤ ÙÙŠ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:")
    print("-" * 60)
    for group, metrics in equalized_odds.items():
        print(f"  {group}:")
        print(f"    TPR: {metrics['TPR']:.3f}")
        print(f"    FPR: {metrics['FPR']:.3f}")
    print(f"\n  TPR Disparity / ÙØ±Ù‚ TPR: {tpr_disparity:.3f}")
    print(f"  FPR Disparity / ÙØ±Ù‚ FPR: {fpr_disparity:.3f}")
    
    if tpr_disparity > 0.1 or fpr_disparity > 0.1:
        print("  âš ï¸  HIGH DISPARITY - Bias in equalized odds!")
        print("     ØªØ­ÙŠØ² ÙÙŠ Ø§Ù„ØªÙƒØ§ÙØ¤ ÙÙŠ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª - ÙØ±Ù‚ Ø¹Ø§Ù„ÙŠ!")
    else:
        print("  âœ… Low disparity - Fair from equalized odds perspective")
        print("     ÙØ±Ù‚ Ù…Ù†Ø®ÙØ¶ - Ø¹Ø§Ø¯Ù„ Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ø§Ù„ØªÙƒØ§ÙØ¤ ÙÙŠ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª")
    
    # Create visualizations
    print("\n" + "="*80)
    print("Creating Visualizations / Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØµÙˆØ±Ø§Øª...")
    print("="*80)
    
    visualize_demographic_parity(parity_rates, parity_disparity)
    visualize_equalized_odds(equalized_odds, tpr_disparity, fpr_disparity)
    visualize_confusion_matrices(test_df)
    
    print("\n" + "="*80)
    print("âœ… Example completed successfully!")
    print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­!")
    print("="*80)
    print("\nKey Takeaways / Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:")
    print("1. Multiple fairness metrics can reveal different types of bias")
    print("   (Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ù†ØµØ§Ù Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒØ´Ù Ø£Ù†ÙˆØ§Ø¹Ø§Ù‹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„ØªØ­ÙŠØ²)")
    print("2. Demographic parity and equalized odds measure different aspects")
    print("   (Ø§Ù„ØªÙƒØ§ÙØ¤ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ ÙˆØ§Ù„ØªÙƒØ§ÙØ¤ ÙÙŠ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ÙŠÙ‚ÙŠØ³Ø§Ù† Ø¬ÙˆØ§Ù†Ø¨ Ù…Ø®ØªÙ„ÙØ©)")
    print("3. It's important to test for bias before and after model deployment")
    print("   (Ù…Ù† Ø§Ù„Ù…Ù‡Ù… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ­ÙŠØ² Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯ Ù†Ø´Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)")
    print("="*80 + "\n")

