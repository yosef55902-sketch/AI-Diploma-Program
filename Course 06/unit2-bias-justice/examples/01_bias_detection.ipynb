{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Bias Detection in Machine Learning Models | ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ŸÅŸä ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä\n",
    "\n",
    "## üìö Prerequisites (What You Need First) | ÿßŸÑŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- ‚úÖ **Unit 1: Foundations of AI Ethics** - You need to understand ethical frameworks and case studies!\n",
    "- ‚úÖ **Basic Python knowledge**: Functions, dictionaries, data manipulation\n",
    "- ‚úÖ **Basic ML knowledge**: Classification, train/test split, confusion matrices\n",
    "- ‚úÖ **Understanding of bias**: What is algorithmic bias? (from Unit 1 case studies)\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why bias detection matters\n",
    "- Knowing which metrics to use for bias detection\n",
    "- Interpreting bias detection results\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where This Notebook Fits | ŸÖŸÉÿßŸÜ Ÿáÿ∞ÿß ÿßŸÑÿØŸÅÿ™ÿ±\n",
    "\n",
    "**This is the FIRST example in Unit 2** - it teaches you how to detect bias in AI systems!\n",
    "\n",
    "**Why this example FIRST?**\n",
    "- **Before** you can mitigate bias, you need to detect it\n",
    "- **Before** you can ensure fairness, you need to measure it\n",
    "- **Before** you can fix problems, you need to identify them\n",
    "\n",
    "**Builds on**: \n",
    "- üìì Unit 1: Foundations (ethical frameworks, case studies like COMPAS showed us bias exists!)\n",
    "\n",
    "**Leads to**: \n",
    "- üìì Example 2: Bias Mitigation (once we detect bias, we learn to fix it!)\n",
    "- üìì Example 3: Fair Representation (ensuring fair representation in data)\n",
    "- üìì Example 4: Bias Case Studies (analyzing real bias cases)\n",
    "- üìì Example 5: Fair AI Development (building fair AI systems)\n",
    "\n",
    "**Why this order?**\n",
    "1. Detection provides **measurement tools** (needed before mitigation)\n",
    "2. Detection teaches **what bias looks like** (critical for understanding)\n",
    "3. Detection shows **how to quantify fairness** (needed for all fairness work)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Finding the Problem Before Fixing It | ÿßŸÑŸÇÿµÿ©: ÿ•Ÿäÿ¨ÿßÿØ ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ŸÇÿ®ŸÑ ÿ•ÿµŸÑÿßÿ≠Ÿáÿß\n",
    "\n",
    "Imagine you're a doctor diagnosing a patient. **Before** you can treat an illness, you need to diagnose it - run tests, check symptoms, identify the problem. **After** diagnosis, you can prescribe the right treatment!\n",
    "\n",
    "Same with AI bias: **Before** we can fix bias, we need to detect it - measure fairness metrics, check for disparities, identify where bias exists. **After** detection, we can apply the right mitigation strategies!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Bias Detection Matters | ŸÑŸÖÿßÿ∞ÿß ŸäŸáŸÖ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ÿü\n",
    "\n",
    "Bias detection is essential for ethical AI:\n",
    "- **Identify Problems**: Find where bias exists in your models\n",
    "- **Measure Fairness**: Quantify how fair (or unfair) your system is\n",
    "- **Track Progress**: Monitor if bias mitigation efforts are working\n",
    "- **Ensure Compliance**: Meet fairness requirements and regulations\n",
    "- **Build Trust**: Demonstrate commitment to fairness\n",
    "\n",
    "## Learning Objectives | ÿ£ŸáÿØÿßŸÅ ÿßŸÑÿ™ÿπŸÑŸÖ\n",
    "1. Understand different types of bias in ML models\n",
    "2. Learn fairness metrics (demographic parity, equalized odds)\n",
    "3. Detect bias using statistical measures\n",
    "4. Visualize bias in model predictions\n",
    "5. Interpret bias detection results\n",
    "6. Understand when to use different fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us detect bias in machine learning models\n",
    "\n",
    "import matplotlib.pyplot as plt  # For creating visualizations: Charts, graphs, bias visualizations\n",
    "import numpy as np  # For numerical operations: Arrays, calculations, random number generation\n",
    "import pandas as pd  # For data manipulation: DataFrames, data analysis\n",
    "from sklearn.model_selection import train_test_split  # For splitting data: Separate training and testing sets\n",
    "from sklearn.ensemble import RandomForestClassifier  # For ML model: Classification algorithm\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # For model evaluation: Performance metrics\n",
    "import seaborn as sns  # For statistical visualizations: Heatmaps, advanced plots\n",
    "import os  # For file operations: Saving images\n",
    "\n",
    "# Configure matplotlib settings: Set default figure size and font size for better visualizations\n",
    "plt.rcParams['font.size'] = 10  # Font size: Make text readable (10pt is good for most displays)\n",
    "plt.rcParams['figure.figsize'] = (14, 8)  # Figure size: 14 inches wide, 8 inches tall (good for detailed charts)\n",
    "\n",
    "print(\" Libraries imported successfully!\")\n",
    "print(\"\\nüìö What each library does:\")\n",
    "print(\"   - matplotlib/seaborn: Create visualizations (bias charts, heatmaps)\")\n",
    "print(\"   - numpy: Numerical operations (arrays, calculations)\")\n",
    "print(\"   - pandas: Data manipulation (DataFrames, analysis)\")\n",
    "print(\"   - sklearn: Machine learning (models, metrics, data splitting)\")\n",
    "print(\"   - os: File operations (saving images)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate synthetic data with intentional bias\n",
    "# This creates a dataset where we know bias exists, so we can practice detecting it\n",
    "\n",
    "# BEFORE: No data with known bias to practice on\n",
    "# AFTER: We'll have a synthetic hiring dataset with intentional bias\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä GENERATING SYNTHETIC DATA WITH BIAS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWe'll create a hiring dataset where:\")\n",
    "print(\"  - Group_B has lower hiring rates even with similar qualifications\")\n",
    "print(\"  - This simulates real-world bias we need to detect\")\n",
    "print(\"  - We'll use this to practice bias detection methods\\n\")\n",
    "\n",
    "def generate_biased_data(n_samples=2000):\n",
    "    \"\"\"\n",
    "    Generate synthetic hiring data with inherent bias.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Creates synthetic features (age, experience, education, skills)\n",
    "    2. Introduces intentional bias: Group_B has lower hiring rates\n",
    "    3. Calculates hiring probability with bias factor\n",
    "    4. Creates binary hiring outcome\n",
    "    \n",
    "    ‚è∞ WHEN to use: To create test data with known bias for practice\n",
    "    üí° WHY use: Allows us to practice bias detection on data where we know bias exists\n",
    "    \"\"\"\n",
    "    # Set random seed: Ensure reproducible results\n",
    "    np.random.seed(42)  # Seed value: Makes random numbers predictable for consistency\n",
    "    \n",
    "    # Create synthetic dataset: Generate features for hiring simulation\n",
    "    # Why synthetic? Allows us to control bias and practice detection safely\n",
    "    data = {\n",
    "        'age': np.random.randint(22, 65, n_samples),  # Age: Random ages between 22-65\n",
    "        'experience_years': np.random.randint(0, 20, n_samples),  # Experience: Years of work experience\n",
    "        'education_level': np.random.choice([1, 2, 3, 4], n_samples, \n",
    "                                           p=[0.2, 0.3, 0.3, 0.2]),  # Education: 1-4 scale with probabilities\n",
    "        'skill_score': np.random.normal(70, 15, n_samples),  # Skills: Normal distribution, mean=70, std=15\n",
    "        'group': np.random.choice(['Group_A', 'Group_B'], n_samples, p=[0.5, 0.5])  # Group: Two groups, equal probability\n",
    "    }\n",
    "    df = pd.DataFrame(data)  # Create DataFrame: Convert dictionary to pandas DataFrame\n",
    "    \n",
    "    # Introduce bias: Group_B has lower success rates even with similar qualifications\n",
    "    # Why introduce bias? To simulate real-world discrimination we need to detect\n",
    "    bias_factor = np.where(df['group'] == 'Group_B', -0.15, 0)  # Bias: -0.15 penalty for Group_B\n",
    "    \n",
    "    # Calculate hiring probability: Base probability with bias factor\n",
    "    # Why this formula? Combines qualifications (skills, experience, education) with bias\n",
    "    base_prob = (df['skill_score'] / 100 +  # Skills component: Normalize to 0-1\n",
    "                 df['experience_years'] / 20 +  # Experience component: Normalize to 0-1\n",
    "                 df['education_level'] / 4) / 3 + bias_factor  # Education component: Average all three, add bias\n",
    "    \n",
    "    # Add noise: Real-world randomness\n",
    "    base_prob += np.random.normal(0, 0.1, n_samples)  # Noise: Small random variation\n",
    "    base_prob = np.clip(base_prob, 0, 1)  # Clip: Ensure probability stays between 0 and 1\n",
    "    \n",
    "    # Create binary outcome: Hired (1) or not hired (0)\n",
    "    df['hired'] = (base_prob > 0.5).astype(int)  # Binary: 1 if probability > 0.5, else 0\n",
    "    \n",
    "    return df  # Return: DataFrame with features and biased hiring outcome\n",
    "\n",
    "# Generate the biased dataset\n",
    "print(\"Generating synthetic hiring data with bias...\")\n",
    "df = generate_biased_data(n_samples=2000)\n",
    "print(f\" Generated dataset with {len(df)} samples\")\n",
    "print(f\"   Group_A hiring rate: {df[df['group']=='Group_A']['hired'].mean():.2%}\")\n",
    "print(f\"   Group_B hiring rate: {df[df['group']=='Group_B']['hired'].mean():.2%}\")\n",
    "print(\"   (Notice the difference - this is the bias we'll detect!)\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 3: Detecting Bias with Fairness Metrics | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ´ÿßŸÑÿ´: ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÖŸÇÿßŸäŸäÿ≥ ÿßŸÑÿπÿØÿßŸÑÿ©\n",
    "\n",
    "### üìö Prerequisites (What You Need First)\n",
    "-  **Biased data generated** (from Part 2) - Understanding the dataset with known bias\n",
    "-  **Understanding of fairness** - Knowing what fairness means\n",
    "\n",
    "### üîó Relationship: What This Builds On\n",
    "This is where we actually detect the bias we created!\n",
    "- Builds on: Biased dataset, understanding of fairness metrics\n",
    "- Shows: How to measure and detect bias\n",
    "\n",
    "### üìñ The Story\n",
    "**Before detection**: We have biased data but don't know how to measure it.\n",
    "**After detection**: We can quantify bias using fairness metrics and see exactly where it exists!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Calculate Fairness Metrics | ÿßŸÑÿÆÿ∑Ÿàÿ© 3: ÿ≠ÿ≥ÿßÿ® ŸÖŸÇÿßŸäŸäÿ≥ ÿßŸÑÿπÿØÿßŸÑÿ©\n",
    "\n",
    "**BEFORE**: We have data but don't know how to measure bias.\n",
    "\n",
    "**AFTER**: We'll calculate fairness metrics (demographic parity, equalized odds) to detect bias!\n",
    "\n",
    "**Why fairness metrics?** They provide:\n",
    "- Quantitative measures of bias\n",
    "- Standard ways to compare groups\n",
    "- Clear thresholds for what's \"fair\"\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing Bias | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿÆÿßŸÖÿ≥: ÿ™ÿµŸàÿ± ÿßŸÑÿ™ÿ≠Ÿäÿ≤\n",
    "\n",
    "### üìö Prerequisites (What You Need First)\n",
    "- ‚úÖ **Bias detection results** (from Part 4) - Having fairness metrics calculated\n",
    "- ‚úÖ **Visualization libraries** (from Part 1) - Understanding matplotlib/seaborn\n",
    "\n",
    "### üîó Relationship: What This Builds On\n",
    "This visualizes the bias we detected!\n",
    "- Builds on: Bias detection results, visualization skills\n",
    "- Shows: Visual representation of bias metrics\n",
    "\n",
    "### üìñ The Story\n",
    "**Before visualization**: We have numbers but can't easily see the bias.\n",
    "**After visualization**: We can see bias clearly in charts and graphs!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Visualize Bias Detection Results | ÿßŸÑÿÆÿ∑Ÿàÿ© 5: ÿ™ÿµŸàÿ± ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤\n",
    "\n",
    "**BEFORE**: We have bias metrics but no visual representation.\n",
    "\n",
    "**AFTER**: We'll create charts showing demographic parity, equalized odds, and confusion matrices!\n",
    "\n",
    "**Why visualize?** Visual representation helps us:\n",
    "- See bias patterns clearly\n",
    "- Compare groups easily\n",
    "- Communicate findings to others\n",
    "- Identify which groups are most affected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create visualizations of bias detection results\n",
    "# This helps us see bias patterns clearly\n",
    "\n",
    "# BEFORE: We have numbers but no visual representation\n",
    "# AFTER: We'll have clear charts showing bias metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CREATING BIAS VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWe'll create three visualizations:\")\n",
    "print(\"  1. Demographic Parity Chart: Shows prediction rates by group\")\n",
    "print(\"  2. Equalized Odds Chart: Shows TPR and FPR by group\")\n",
    "print(\"  3. Confusion Matrices: Shows prediction accuracy by group\\n\")\n",
    "\n",
    "def visualize_demographic_parity(parity_rates, disparity):\n",
    "    \"\"\"\n",
    "    Visualize demographic parity analysis.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Create bar chart showing positive prediction rates for each group\n",
    "    2. Add value labels on bars\n",
    "    3. Add reference lines showing max/min rates\n",
    "    4. Display disparity value\n",
    "    5. Save as high-resolution image\n",
    "    \n",
    "    ‚è∞ WHEN to use: After calculating demographic parity - see visual comparison\n",
    "    üí° WHY use: Bar chart makes it easy to see differences between groups\n",
    "    \"\"\"\n",
    "    # Extract data: Get groups and their rates\n",
    "    groups = list(parity_rates.keys())  # Groups: Extract group names for x-axis\n",
    "    rates = list(parity_rates.values())  # Rates: Extract positive rates for bars\n",
    "    colors = ['#3498db', '#e74c3c']  # Colors: Blue and red for visual distinction\n",
    "    \n",
    "    # Create chart: Initialize the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))  # Create plot: 10x6 inches for readable chart\n",
    "    \n",
    "    # Create bars: Draw bars for each group\n",
    "    bars = ax.bar(groups, rates, color=colors, alpha=0.8, edgecolor='black', linewidth=2)  # Bars: Group names on x-axis, rates on y-axis\n",
    "    \n",
    "    # Add value labels: Show exact rates on bars\n",
    "    for bar, rate in zip(bars, rates):  # Loop through bars: Process each group\n",
    "        height = bar.get_height()  # Get height: Extract bar height (rate value)\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,  # Position text: Center on top of bar\n",
    "               f'{rate:.3f}\\n({rate*100:.1f}%)',  # Text content: Show rate as decimal and percentage\n",
    "               ha='center', va='bottom', fontweight='bold', fontsize=11)  # Text style: Centered, bold, readable\n",
    "    \n",
    "    # Add reference lines: Show max and min rates for comparison\n",
    "    ax.axhline(y=max(rates), color='red', linestyle='--', alpha=0.5, label='Max')  # Max line: Red dashed line at highest rate\n",
    "    ax.axhline(y=min(rates), color='blue', linestyle='--', alpha=0.5, label='Min')  # Min line: Blue dashed line at lowest rate\n",
    "    \n",
    "    # Add labels: Make chart readable\n",
    "    ax.set_ylabel('Positive Prediction Rate', fontsize=12, fontweight='bold')  # Y-axis label: Describe what y-axis shows\n",
    "    ax.set_title(f'Demographic Parity Analysis\\nDisparity: {disparity:.3f}', \n",
    "                fontsize=14, fontweight='bold', pad=20)  # Title: Main heading with disparity value\n",
    "    \n",
    "    # Set axis limits: Ensure consistent scale\n",
    "    ax.set_ylim(0, max(rates) * 1.2)  # Y-axis range: 0 to 20% above max rate (room for labels)\n",
    "    \n",
    "    # Add legend and grid: Improve readability\n",
    "    ax.legend()  # Legend: Explain reference lines\n",
    "    ax.grid(axis='y', alpha=0.3)  # Grid: Light gray lines help read values\n",
    "    \n",
    "    # Save visualization: Export as high-resolution image\n",
    "    plt.tight_layout()  # Adjust layout: Prevent label cutoff\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))  # Get directory: Find notebook location\n",
    "    output_path = os.path.join(script_dir, 'demographic_parity.png')  # File path: Save in notebook directory\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')  # Save image: High resolution (300 dpi), tight bounds\n",
    "    print(\" Saved: demographic_parity.png\")  # Success message: Confirm save\n",
    "    plt.close()  # Close figure: Free memory\n",
    "\n",
    "def visualize_equalized_odds(equalized_odds, tpr_disparity, fpr_disparity):\n",
    "    \"\"\"\n",
    "    Visualize equalized odds metrics.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Create grouped bar chart showing TPR and FPR for each group\n",
    "    2. Add value labels on bars\n",
    "    3. Display TPR and FPR disparity values\n",
    "    4. Save as high-resolution image\n",
    "    \n",
    "    ‚è∞ WHEN to use: After calculating equalized odds - see visual comparison\n",
    "    üí° WHY use: Grouped bars make it easy to compare TPR and FPR across groups\n",
    "    \"\"\"\n",
    "    # Extract data: Get groups and their metrics\n",
    "    groups = list(equalized_odds.keys())  # Groups: Extract group names\n",
    "    tprs = [equalized_odds[g]['TPR'] for g in groups]  # TPRs: Extract True Positive Rates\n",
    "    fprs = [equalized_odds[g]['FPR'] for g in groups]  # FPRs: Extract False Positive Rates\n",
    "    \n",
    "    # Set up bar positions: Create positions for grouped bars\n",
    "    x = np.arange(len(groups))  # X positions: Array of positions for each group\n",
    "    width = 0.35  # Bar width: Width of each bar (leaves room for two bars per group)\n",
    "    \n",
    "    # Create chart: Initialize the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))  # Create plot: 10x6 inches for readable chart\n",
    "    \n",
    "    # Create bars: Draw grouped bars for TPR and FPR\n",
    "    bars1 = ax.bar(x - width/2, tprs, width, label='True Positive Rate (TPR)',  # TPR bars: Green bars, offset left\n",
    "                   color='#2ecc71', alpha=0.8, edgecolor='black')  # Color: Green for positive metric\n",
    "    bars2 = ax.bar(x + width/2, fprs, width, label='False Positive Rate (FPR)',  # FPR bars: Red bars, offset right\n",
    "                   color='#e74c3c', alpha=0.8, edgecolor='black')  # Color: Red for negative metric\n",
    "    \n",
    "    # Add value labels: Show exact values on bars\n",
    "    for bars in [bars1, bars2]:  # Loop through bar groups: Process TPR and FPR bars\n",
    "        for bar in bars:  # Loop through individual bars: Process each bar\n",
    "            height = bar.get_height()  # Get height: Extract bar height (rate value)\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,  # Position text: Center on top of bar\n",
    "                   f'{height:.3f}',  # Text content: Show rate as decimal\n",
    "                   ha='center', va='bottom', fontsize=9, fontweight='bold')  # Text style: Centered, bold, readable\n",
    "    \n",
    "    # Add labels: Make chart readable\n",
    "    ax.set_xlabel('Group', fontsize=12, fontweight='bold')  # X-axis label: Describe what x-axis shows\n",
    "    ax.set_ylabel('Rate', fontsize=12, fontweight='bold')  # Y-axis label: Describe what y-axis shows\n",
    "    ax.set_title(f'Equalized Odds Analysis\\nTPR Disparity: {tpr_disparity:.3f} | FPR Disparity: {fpr_disparity:.3f}', \n",
    "                fontsize=14, fontweight='bold', pad=20)  # Title: Main heading with disparity values\n",
    "    \n",
    "    # Configure x-axis: Set group names as labels\n",
    "    ax.set_xticks(x)  # Set tick positions: Place ticks at each group position\n",
    "    ax.set_xticklabels(groups)  # Set tick labels: Use group names\n",
    "    \n",
    "    # Add legend and grid: Improve readability\n",
    "    ax.legend(fontsize=10)  # Legend: Explain what TPR and FPR mean\n",
    "    ax.set_ylim(0, max(max(tprs), max(fprs)) * 1.2)  # Y-axis range: 0 to 20% above max rate\n",
    "    ax.grid(axis='y', alpha=0.3)  # Grid: Light gray lines help read values\n",
    "    \n",
    "    # Save visualization: Export as high-resolution image\n",
    "    plt.tight_layout()  # Adjust layout: Prevent label cutoff\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))  # Get directory: Find notebook location\n",
    "    output_path = os.path.join(script_dir, 'equalized_odds.png')  # File path: Save in notebook directory\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')  # Save image: High resolution (300 dpi), tight bounds\n",
    "    print(\" Saved: equalized_odds.png\")  # Success message: Confirm save\n",
    "    plt.close()  # Close figure: Free memory\n",
    "\n",
    "def visualize_confusion_matrices(test_df):\n",
    "    \"\"\"\n",
    "    Visualize confusion matrices by group.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Create separate confusion matrix for each group\n",
    "    2. Use heatmap to show counts clearly\n",
    "    3. Display side-by-side for easy comparison\n",
    "    4. Save as high-resolution image\n",
    "    \n",
    "    ‚è∞ WHEN to use: After making predictions - see accuracy differences by group\n",
    "    üí° WHY use: Confusion matrices show exactly where predictions differ between groups\n",
    "    \"\"\"\n",
    "    # Get groups: Identify all groups in test data\n",
    "    groups = test_df['group'].unique()  # Groups: Extract unique group values\n",
    "    \n",
    "    # Create subplots: One confusion matrix per group\n",
    "    fig, axes = plt.subplots(1, len(groups), figsize=(14, 5))  # Subplots: Side-by-side, 14x5 inches\n",
    "    \n",
    "    # Create confusion matrix for each group: Process each group\n",
    "    for idx, group in enumerate(groups):  # Loop through groups: Process each group\n",
    "        group_data = test_df[test_df['group'] == group]  # Filter: Get data for this group\n",
    "        cm = confusion_matrix(group_data['hired'], group_data['predicted'])  # Calculate: Confusion matrix for this group\n",
    "        \n",
    "        # Create heatmap: Visualize confusion matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],  # Heatmap: Blue color scheme, show counts\n",
    "                   cbar_kws={'label': 'Count'})  # Colorbar: Explain what colors mean\n",
    "        \n",
    "        # Add labels: Make heatmap readable\n",
    "        axes[idx].set_title(f'{group}\\nConfusion Matrix', fontsize=12, fontweight='bold')  # Title: Group name and chart type\n",
    "        axes[idx].set_xlabel('Predicted', fontsize=10)  # X-axis label: Describe what x-axis shows\n",
    "        axes[idx].set_ylabel('Actual', fontsize=10)  # Y-axis label: Describe what y-axis shows\n",
    "        axes[idx].set_xticklabels(['Not Hired', 'Hired'])  # X labels: Meaning of columns\n",
    "        axes[idx].set_yticklabels(['Not Hired', 'Hired'])  # Y labels: Meaning of rows\n",
    "    \n",
    "    # Save visualization: Export as high-resolution image\n",
    "    plt.tight_layout()  # Adjust layout: Prevent label cutoff\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))  # Get directory: Find notebook location\n",
    "    output_path = os.path.join(script_dir, 'confusion_matrices_by_group.png')  # File path: Save in notebook directory\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')  # Save image: High resolution (300 dpi), tight bounds\n",
    "    print(\" Saved: confusion_matrices_by_group.png\")  # Success message: Confirm save\n",
    "    plt.close()  # Close figure: Free memory\n",
    "\n",
    "# Create all visualizations\n",
    "print(\"Creating bias visualizations...\")\n",
    "visualize_demographic_parity(parity_rates, parity_disparity)\n",
    "visualize_equalized_odds(equalized_odds, tpr_disparity, fpr_disparity)\n",
    "visualize_confusion_matrices(test_df)\n",
    "print(\"\\n All visualizations created!\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary: What We Learned | ÿßŸÑŸÖŸÑÿÆÿµ: ŸÖÿß ÿ™ÿπŸÑŸÖŸÜÿßŸá\n",
    "\n",
    "**BEFORE this notebook**: We knew bias exists but didn't know how to detect it in our models.\n",
    "\n",
    "**AFTER this notebook**: We can:\n",
    "- ‚úÖ Generate synthetic data with known bias for practice\n",
    "- ‚úÖ Calculate fairness metrics (demographic parity, equalized odds)\n",
    "- ‚úÖ Train ML models and detect bias in their predictions\n",
    "- ‚úÖ Visualize bias using charts and confusion matrices\n",
    "- ‚úÖ Interpret bias detection results\n",
    "- ‚úÖ Understand when to use different fairness metrics\n",
    "\n",
    "### Key Takeaways | ÿßŸÑÿßÿ≥ÿ™ŸÜÿ™ÿßÿ¨ÿßÿ™ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©\n",
    "\n",
    "1. **Multiple Metrics Matter**: Different fairness metrics reveal different types of bias\n",
    "2. **Demographic Parity vs. Equalized Odds**: They measure different aspects of fairness\n",
    "3. **Bias Detection is Essential**: Must test for bias before and after model deployment\n",
    "4. **Visualization Helps**: Charts make bias patterns easier to understand\n",
    "5. **Systematic Approach**: Following a structured process ensures comprehensive bias detection\n",
    "\n",
    "### Next Steps | ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ©\n",
    "\n",
    "- üìì **Example 2**: Bias Mitigation (learn how to fix the bias we detected!)\n",
    "- üìì **Example 3**: Fair Representation (ensure fair representation in data!)\n",
    "- üìì **Example 4**: Bias Case Studies (analyze real-world bias cases!)\n",
    "- üìì **Example 5**: Fair AI Development (build fair AI systems from the start!)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You've learned how to detect bias in machine learning models systematically!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2: Understanding Bias in Data | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ´ÿßŸÜŸä: ŸÅŸáŸÖ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ŸÅŸä ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "\n",
    "### üìö Prerequisites (What You Need First)\n",
    "-  **Library imports** (from Part 1) - Understanding data manipulation and ML tools\n",
    "-  **Understanding of bias** (from Unit 1) - Knowing what bias is\n",
    "\n",
    "### üîó Relationship: What This Builds On\n",
    "This creates data with intentional bias so we can practice detecting it!\n",
    "- Builds on: Data manipulation skills, understanding of bias\n",
    "- Shows: How bias manifests in data\n",
    "\n",
    "### üìñ The Story\n",
    "**Before biased data**: We need data with known bias to practice detection.\n",
    "**After biased data**: We have a dataset where we know bias exists, so we can test our detection methods!\n",
    "## Part 4: Training Model and Detecting Bias | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ±ÿßÿ®ÿπ: ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸàÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤\n",
    "\n",
    "### üìö Prerequisites (What You Need First)\n",
    "-  **Fairness metrics** (from Part 3) - Understanding how to calculate bias\n",
    "-  **Biased data** (from Part 2) - Having data to analyze\n",
    "\n",
    "### üîó Relationship: What This Builds On\n",
    "This trains a model and uses our fairness metrics to detect bias!\n",
    "- Builds on: Fairness metric functions, biased dataset\n",
    "- Shows: How to detect bias in a trained model\n",
    "\n",
    "### üìñ The Story\n",
    "**Before training**: We have data and metrics but no model to analyze.\n",
    "**After training**: We have a trained model and can detect bias in its predictions!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Train Model and Detect Bias | ÿßŸÑÿÆÿ∑Ÿàÿ© 4: ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸàÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤\n",
    "\n",
    "**BEFORE**: We have data and fairness metrics but haven't trained a model yet.\n",
    "\n",
    "**AFTER**: We'll train a model and use our fairness metrics to detect bias in its predictions!\n",
    "\n",
    "**Why train a model?** Real-world bias detection happens on trained models, not just data!\n",
    "\n",
    "---\n",
    "\n",
    "# Step 4: Train a machine learning model and detect bias in its predictions\n",
    "# This shows how bias manifests in model predictions\n",
    "\n",
    "# BEFORE: We have data and metrics but no model predictions\n",
    "# AFTER: We'll have a trained model and bias detection results\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ TRAINING MODEL AND DETECTING BIAS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWe'll:\")\n",
    "print(\"  1. Train a Random Forest classifier on our biased data\")\n",
    "print(\"  2. Make predictions on test data\")\n",
    "print(\"  3. Calculate fairness metrics on predictions\")\n",
    "print(\"  4. Detect bias in the model's behavior\\n\")\n",
    "\n",
    "def train_and_analyze_bias(df):\n",
    "    \"\"\"\n",
    "    Train a model and analyze for bias using fairness metrics.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Prepare features and target variable\n",
    "    2. Split data into training and testing sets\n",
    "    3. Train Random Forest classifier\n",
    "    4. Make predictions on test set\n",
    "    5. Calculate demographic parity and equalized odds\n",
    "    6. Return results for visualization\n",
    "    \n",
    "    ‚è∞ WHEN to use: After having data and fairness metric functions - detect bias in model\n",
    "    üí° WHY use: Shows how bias in data translates to bias in model predictions\n",
    "    \"\"\"\n",
    "    # Prepare features: Select columns to use for prediction\n",
    "    X = df[['age', 'experience_years', 'education_level', 'skill_score']]  # Features: Input variables for the model\n",
    "    y = df['hired']  # Target: What we want to predict (hired or not)\n",
    "    \n",
    "    # Split data: Separate into training and testing sets\n",
    "    # Why split? Training set teaches the model, test set evaluates it\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y  # Split: 70% train, 30% test, stratified to maintain class balance\n",
    "    )\n",
    "    \n",
    "    # Train model: Create and train Random Forest classifier\n",
    "    # Why Random Forest? Good for classification, handles non-linear relationships\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)  # Model: 100 trees, fixed random seed\n",
    "    model.fit(X_train, y_train)  # Train: Learn patterns from training data\n",
    "    print(\" Model trained successfully!\")  # Success message: Confirm training complete\n",
    "    \n",
    "    # Make predictions: Use model to predict on test data\n",
    "    y_pred = model.predict(X_test)  # Predictions: Model's predictions for test set\n",
    "    \n",
    "    # Add predictions to test data: Combine predictions with test data for analysis\n",
    "    test_df = X_test.copy()  # Copy: Create copy of test features\n",
    "    test_df['hired'] = y_test.values  # Actual: Add actual outcomes\n",
    "    test_df['predicted'] = y_pred  # Predicted: Add model predictions\n",
    "    test_df['group'] = df.loc[X_test.index, 'group'].values  # Group: Add group labels for fairness analysis\n",
    "    \n",
    "    # Calculate bias metrics: Use our fairness functions to detect bias\n",
    "    parity_rates, parity_disparity = calculate_demographic_parity(test_df)  # Demographic parity: Overall prediction balance\n",
    "    equalized_odds, tpr_disparity, fpr_disparity = calculate_equalized_odds(test_df)  # Equalized odds: Accuracy balance\n",
    "    \n",
    "    return test_df, model, parity_rates, parity_disparity, equalized_odds, tpr_disparity, fpr_disparity  # Return: All results for analysis\n",
    "\n",
    "# Train model and detect bias\n",
    "print(\"Training model and analyzing for bias...\")\n",
    "test_df, model, parity_rates, parity_disparity, equalized_odds, tpr_disparity, fpr_disparity = train_and_analyze_bias(df)\n",
    "\n",
    "# Print initial results\n",
    "print(\"\\nüìä BIAS DETECTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. Demographic Parity\")\n",
    "print(\"-\" * 60)\n",
    "for group, rate in parity_rates.items():\n",
    "    print(f\"  {group}: {rate:.3f} ({rate*100:.1f}%)\")\n",
    "print(f\"\\n  Disparity: {parity_disparity:.3f}\")\n",
    "if parity_disparity > 0.1:\n",
    "    print(\"  ‚ö†Ô∏è  HIGH DISPARITY - Potential bias detected!\")\n",
    "else:\n",
    "    print(\"   Low disparity - Fair from demographic parity perspective\")\n",
    "\n",
    "print(\"\\n2. Equalized Odds\")\n",
    "print(\"-\" * 60)\n",
    "for group, metrics in equalized_odds.items():\n",
    "    print(f\"  {group}:\")\n",
    "    print(f\"    TPR: {metrics['TPR']:.3f}\")\n",
    "    print(f\"    FPR: {metrics['FPR']:.3f}\")\n",
    "print(f\"\\n  TPR Disparity: {tpr_disparity:.3f}\")\n",
    "print(f\"  FPR Disparity: {fpr_disparity:.3f}\")\n",
    "if tpr_disparity > 0.1 or fpr_disparity > 0.1:\n",
    "    print(\"  ‚ö†Ô∏è  HIGH DISPARITY - Bias in equalized odds!\")\n",
    "else:\n",
    "    print(\"   Low disparity - Fair from equalized odds perspective\")\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "def visualize_demographic_parity(parity_rates, disparity):\n",
    "    \"\"\"Visualize demographic parity\"\"\"\n",
    "    groups = list(parity_rates.keys())\n",
    "    rates = list(parity_rates.values())\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.bar(groups, rates, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, rates):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{rate:.3f}\\n({rate*100:.1f}%)',\n",
    "               ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    # Add disparity line\n",
    "    ax.axhline(y=max(rates), color='red', linestyle='--', alpha=0.5, label='Max')\n",
    "    ax.axhline(y=min(rates), color='blue', linestyle='--', alpha=0.5, label='Min')\n",
    "    ax.set_ylabel('Positive Prediction Rate', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Demographic Parity Analysis\\n'\n",
    "                f''\n",
    "                f'Disparity: {disparity:.3f}',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_ylim(0, max(rates) * 1.2)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('demographic_parity.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\" Saved: demographic_parity.png\")\n",
    "    plt.close()\n",
    "def visualize_equalized_odds(equalized_odds, tpr_disparity, fpr_disparity):\n",
    "    \"\"\"Visualize equalized odds metrics\"\"\"\n",
    "    groups = list(equalized_odds.keys())\n",
    "    tprs = [equalized_odds[g]['TPR'] for g in groups]\n",
    "    fprs = [equalized_odds[g]['FPR'] for g in groups]\n",
    "    x = np.arange(len(groups))\n",
    "    width = 0.35\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, tprs, width, label='True Positive Rate (TPR)',\n",
    "                   color='#2ecc71', alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, fprs, width, label='False Positive Rate (FPR)',\n",
    "                   color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    ax.set_xlabel('Group', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Rate', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Equalized Odds Analysis\\n'\n",
    "                f''\n",
    "                f'TPR Disparity: {tpr_disparity:.3f} | FPR Disparity: {fpr_disparity:.3f}',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim(0, max(max(tprs), max(fprs)) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('equalized_odds.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\" Saved: equalized_odds.png\")\n",
    "    plt.close()\n",
    "def visualize_confusion_matrices(test_df):\n",
    "    \"\"\"Visualize confusion matrices by group\"\"\"\n",
    "    groups = test_df['group'].unique()\n",
    "    fig, axes = plt.subplots(1, len(groups), figsize=(14, 5))\n",
    "    for idx, group in enumerate(groups):\n",
    "        group_data = test_df[test_df['group'] == group]\n",
    "        cm = confusion_matrix(group_data['hired'], group_data['predicted'])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        axes[idx].set_title(f'{group}\\nConfusion Matrix',\n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted', fontsize=10)\n",
    "        axes[idx].set_ylabel('Actual', fontsize=10)\n",
    "        axes[idx].set_xticklabels(['Not Hired', 'Hired'])\n",
    "        axes[idx].set_yticklabels(['Not Hired', 'Hired'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices_by_group.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\" Saved: confusion_matrices_by_group.png\")\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"Unit 2 - Example 1: Bias Detection in ML Models\")\n",
    "    print(\"\")\n",
    "    print(\"=\"*80)\n",
    "    # Generate data\n",
    "    print(\"\\nüìä Generating synthetic data with bias...\")\n",
    "    print(\"\")\n",
    "    df = generate_biased_data(n_samples=2000)\n",
    "    # Show data summary\n",
    "    print(\"\\nüìã Data Summary\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Groups: {df[\"group'].value_counts().to_dict()}')\n",
    "    print(f\"\\nHiring rates by group:\")\n",
    "    for group in df['group'].unique():\n",
    "        rate = df[df['group'] == group]['hired'].mean()\n",
    "        print(f\"  {group}: {rate:.3f} ({rate*100:.1f}%)\")\n",
    "    # Train model and analyze\n",
    "    print(\"\\nüîç Training model and analyzing bias...\")\n",
    "    print(\"\")\n",
    "    test_df, model, parity_rates, parity_disparity, equalized_odds, tpr_disparity, fpr_disparity = train_and_analyze_bias(df)\n",
    "    # Print results\n",
    "    print(\"\\nüìä BIAS DETECTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n1. Demographic Parity\")\n",
    "    print(\"-\" * 60)\n",
    "    for group, rate in parity_rates.items():\n",
    "        print(f\"  {group}: {rate:.3f} ({rate*100:.1f}%)\")\n",
    "    print(f\"\\n  Disparity\")\n",
    "    if parity_disparity > 0.1:\n",
    "        print(\"  ‚ö†Ô∏è  HIGH DISPARITY - Potential bias detected!\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"   Low disparity - Fair from demographic parity perspective\")\n",
    "        print(\"\")\n",
    "    print(\"\\n2. Equalized Odds\")\n",
    "    print(\"-\" * 60)\n",
    "    for group, metrics in equalized_odds.items():\n",
    "        print(f\"  {group}:\")\n",
    "        print(f\"    TPR: {metrics[\"TPR']:.3f}')\n",
    "        print(f\"    FPR: {metrics[\"FPR']:.3f}')\n",
    "    print(f\"\\n  TPR Disparity\")\n",
    "    print(f\"FPR Disparity\")\n",
    "    if tpr_disparity > 0.1 or fpr_disparity > 0.1:\n",
    "        print(\"  ‚ö†Ô∏è  HIGH DISPARITY - Bias in equalized odds!\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"   Low disparity - Fair from equalized odds perspective\")\n",
    "        print(\"\")\n",
    "    # Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating Visualizations\")\n",
    "    print(\"=\"*80)\n",
    "    visualize_demographic_parity(parity_rates, parity_disparity)\n",
    "    visualize_equalized_odds(equalized_odds, tpr_disparity, fpr_disparity)\n",
    "    visualize_confusion_matrices(test_df)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" Example completed successfully!\")\n",
    "    print(\"\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Takeaways\")\n",
    "    print(\"1. Multiple fairness metrics can reveal different types of bias\")\n",
    "    print(\"\")\n",
    "    print(\"2. Demographic parity and equalized odds measure different aspects\")\n",
    "    print(\"\")\n",
    "    print(\"3. It\"s important to test for bias before and after model deployment\")\n",
    "    print(\"\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}