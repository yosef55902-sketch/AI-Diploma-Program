{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Bias Mitigation Techniques | ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ®ÙÙŠÙ Ù…Ù† Ø§Ù„ØªØ­ÙŠØ²\n",
    "\n",
    "## ðŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Bias Detection** - You need to know how to detect bias first!\n",
    "- âœ… **Basic Python knowledge**: Functions, data manipulation, ML concepts\n",
    "- âœ… **Understanding of fairness metrics**: Demographic parity, equalized odds (from Example 1)\n",
    "- âœ… **Basic ML knowledge**: Model training, predictions, evaluation\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why bias mitigation is needed\n",
    "- Knowing which mitigation technique to use\n",
    "- Understanding when to apply different techniques\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the SECOND example in Unit 2** - it teaches you how to FIX the bias you detected!\n",
    "\n",
    "**Why this example SECOND?**\n",
    "- **Before** you can fix bias, you need to detect it (Example 1)\n",
    "- **Before** you can ensure fairness, you need to know how to mitigate bias\n",
    "- **Before** you can build fair systems, you need mitigation techniques\n",
    "\n",
    "**Builds on**: \n",
    "- ðŸ““ Example 1: Bias Detection (we detected bias, now we fix it!)\n",
    "\n",
    "**Leads to**: \n",
    "- ðŸ““ Example 3: Fair Representation (ensuring fair representation in data)\n",
    "- ðŸ““ Example 4: Bias Case Studies (analyzing real bias cases)\n",
    "- ðŸ““ Example 5: Fair AI Development (building fair AI systems)\n",
    "\n",
    "**Why this order?**\n",
    "1. Mitigation provides **solutions** (needed after detection)\n",
    "2. Mitigation teaches **how to fix problems** (critical skill)\n",
    "3. Mitigation shows **different approaches** (pre-processing, in-processing, post-processing)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Fixing What We Found | Ø§Ù„Ù‚ØµØ©: Ø¥ØµÙ„Ø§Ø­ Ù…Ø§ ÙˆØ¬Ø¯Ù†Ø§Ù‡\n",
    "\n",
    "Imagine you're a doctor who diagnosed an illness. **Before** you can help the patient, you need to treat it - prescribe medicine, perform surgery, provide therapy. **After** treatment, the patient recovers!\n",
    "\n",
    "Same with AI bias: **Before** we detected bias, now we need to fix it - use pre-processing, in-processing, or post-processing techniques. **After** mitigation, we have fairer models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Bias Mitigation Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ù„ØªØ®ÙÙŠÙ Ù…Ù† Ø§Ù„ØªØ­ÙŠØ²ØŸ\n",
    "\n",
    "Bias mitigation is essential for ethical AI:\n",
    "- **Fix Problems**: Address bias detected in models\n",
    "- **Improve Fairness**: Make models fairer across groups\n",
    "- **Meet Requirements**: Comply with fairness regulations\n",
    "- **Build Trust**: Demonstrate commitment to fairness\n",
    "- **Better Outcomes**: Fairer models lead to better decisions\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Understand three types of bias mitigation (pre-processing, in-processing, post-processing)\n",
    "2. Learn reweighing technique for pre-processing\n",
    "3. Learn correlation removal for fair representation\n",
    "4. Learn threshold optimization for post-processing\n",
    "5. Compare effectiveness of different mitigation techniques\n",
    "6. Understand when to use each technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us implement bias mitigation techniques\n",
    "\n",
    "import numpy as np  # For numerical operations: Arrays, calculations, random number generation\n",
    "import pandas as pd  # For data manipulation: DataFrames, data analysis\n",
    "import matplotlib.pyplot as plt  # For creating visualizations: Charts, graphs, comparisons\n",
    "import seaborn as sns  # For statistical visualizations: Heatmaps, advanced plots\n",
    "from sklearn.model_selection import train_test_split  # For splitting data: Separate training and testing sets\n",
    "from sklearn.ensemble import RandomForestClassifier  # For ML model: Classification algorithm\n",
    "from sklearn.preprocessing import StandardScaler  # For data preprocessing: Feature scaling\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # For model evaluation: Performance metrics\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference  # For fairness metrics: Fairlearn library\n",
    "from fairlearn.postprocessing import ThresholdOptimizer  # For post-processing: Threshold optimization\n",
    "from fairlearn.preprocessing import CorrelationRemover  # For pre-processing: Correlation removal\n",
    "import warnings  # For suppressing warnings: Clean output\n",
    "import os  # For file operations: Saving images\n",
    "\n",
    "# Suppress warnings: Clean output (fairlearn may show warnings)\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings: Suppress non-critical warnings\n",
    "\n",
    "# Configure plotting: Set default styles for better visualizations\n",
    "plt.rcParams['font.size'] = 10  # Font size: Make text readable (10pt is good for most displays)\n",
    "plt.rcParams['figure.figsize'] = (14, 8)  # Figure size: 14 inches wide, 8 inches tall (good for detailed charts)\n",
    "sns.set_style(\"whitegrid\")  # Style: White background with grid for clean look\n",
    "\n",
    "print(\" Libraries imported successfully!\")\n",
    "print(\"\\nðŸ“š What each library does:\")\n",
    "print(\"   - numpy/pandas: Data manipulation and numerical operations\")\n",
    "print(\"   - matplotlib/seaborn: Create visualizations (charts, heatmaps)\")\n",
    "print(\"   - sklearn: Machine learning (models, metrics, preprocessing)\")\n",
    "print(\"   - fairlearn: Fairness tools (metrics, mitigation techniques)\")\n",
    "print(\"   - os: File operations (saving images)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate synthetic dataset with bias\n",
    "# This creates data with known bias so we can practice mitigation techniques\n",
    "\n",
    "# BEFORE: No biased data to practice mitigation on\n",
    "# AFTER: We'll have a dataset with intentional bias\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š GENERATING BIASED DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWe'll create a dataset where:\")\n",
    "print(\"  - Group B has lower positive outcome rates even with similar features\")\n",
    "print(\"  - This simulates real-world bias we need to mitigate\")\n",
    "print(\"  - We'll use this to practice different mitigation techniques\\n\")\n",
    "\n",
    "def generate_biased_dataset(n_samples=2000, bias_strength=0.3):\n",
    "    pass\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2: Understanding Bias Mitigation | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: ÙÙ‡Ù… Ø§Ù„ØªØ®ÙÙŠÙ Ù…Ù† Ø§Ù„ØªØ­ÙŠØ²\n",
    "\n",
    "### ðŸ“š Prerequisites (What You Need First)\n",
    "-  **Biased dataset** (from Step 2) - Having data with known bias\n",
    "-  **Understanding of mitigation** - Knowing what mitigation means\n",
    "\n",
    "### ðŸ”— Relationship: What This Builds On\n",
    "This is where we actually fix the bias!\n",
    "- Builds on: Biased dataset, understanding of fairness\n",
    "- Shows: Three approaches to fixing bias\n",
    "\n",
    "### ðŸ“– The Story\n",
    "**Before mitigation**: We have biased data but don't know how to fix it.\n",
    "**After mitigation**: We have three techniques to reduce or eliminate bias!\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset with bias in the sensitive attribute.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Creates sensitive attribute (two groups)\n",
    "    2. Generates features (X1, X2)\n",
    "    3. Introduces bias: Group B has lower positive outcome rates\n",
    "    4. Creates target variable based on features and bias\n",
    "    \n",
    "    â° WHEN to use: To create test data with known bias for practice\n",
    "    ðŸ’¡ WHY use: Allows us to practice mitigation on data where we know bias exists\n",
    "    \"\"\"\n",
    "    # Set random seed: Ensure reproducible results\n",
    "    np.random.seed(42)  # Seed value: Makes random numbers predictable for consistency\n",
    "    \n",
    "    # Create sensitive attribute: Two groups (0 = Group A, 1 = Group B)\n",
    "    # Why sensitive attribute? This is what we'll protect against bias\n",
    "    sensitive = np.random.binomial(1, 0.5, n_samples)  # Binary: 50% chance of each group\n",
    "    \n",
    "    # Generate features: Create input variables for the model\n",
    "    X1 = np.random.normal(0, 1, n_samples)  # Feature 1: Normal distribution, mean=0, std=1\n",
    "    X2 = np.random.normal(0, 1, n_samples)  # Feature 2: Normal distribution, mean=0, std=1\n",
    "    \n",
    "    # Calculate true probability: Base probability based on features only\n",
    "    # Why this formula? Combines features to create realistic relationship\n",
    "    true_prob = 0.3 + 0.4 * X1 + 0.3 * X2  # Base probability: Linear combination of features\n",
    "    \n",
    "    # Introduce bias: Group B (sensitive=0) gets penalty\n",
    "    # Why introduce bias? To simulate real-world discrimination we need to fix\n",
    "    bias_penalty = bias_strength * (1 - sensitive)  # Bias: Penalty for Group B (0)\n",
    "    \n",
    "    # Calculate final probability: Add bias and noise\n",
    "    prob = true_prob - bias_penalty + np.random.normal(0, 0.1, n_samples)  # Final probability: Base - bias + noise\n",
    "    prob = np.clip(prob, 0, 1)  # Clip: Ensure probability stays between 0 and 1\n",
    "    \n",
    "    # Create target variable: Binary outcome based on probability\n",
    "    y = (prob > 0.5).astype(int)  # Binary: 1 if probability > 0.5, else 0\n",
    "    \n",
    "    # Create DataFrame: Organize data for analysis\n",
    "    df = pd.DataFrame({\n",
    "        'feature1': X1,  # Feature 1: First input variable\n",
    "        'feature2': X2,  # Feature 2: Second input variable\n",
    "        'sensitive': sensitive,  # Sensitive: Group membership (protected attribute)\n",
    "        'target': y  # Target: Outcome we want to predict\n",
    "    })\n",
    "    \n",
    "    return df  # Return: DataFrame with features, sensitive attribute, and biased target\n",
    "\n",
    "# Generate the biased dataset\n",
    "print(\"Generating synthetic biased dataset...\")\n",
    "df = generate_biased_dataset(n_samples=2000, bias_strength=0.3)\n",
    "print(f\" Generated dataset with {len(df)} samples\")\n",
    "print(f\"   Group A (sensitive=1) positive rate: {df[df['sensitive']==1]['target'].mean():.2%}\")\n",
    "print(f\"   Group B (sensitive=0) positive rate: {df[df['sensitive']==0]['target'].mean():.2%}\")\n",
    "print(\"   (Notice the difference - this is the bias we'll mitigate!)\")\n",
    "# ============================================================================\n",
    "# PRE-PROCESSING TECHNIQUES\n",
    "# ============================================================================\n",
    "def preprocess_reweighing(X_train, y_train, sensitive_train):\n",
    "    \"\"\"\n",
    "    Reweighing: Assign different weights to balance group representation\n",
    "    \"\"\"\n",
    "    # Calculate weights to balance groups\n",
    "    group_counts = pd.Series(sensitive_train).value_counts()\n",
    "    total = len(sensitive_train)\n",
    "    weights = np.ones(len(sensitive_train))\n",
    "    for group in group_counts.index:\n",
    "        group_size = group_counts[group]\n",
    "        # Weight inversely proportional to group size\n",
    "        weights[sensitive_train == group] = total / (2 * group_size)\n",
    "    return weights\n",
    "def preprocess_correlation_removal(X_train, X_test, sensitive_train):\n",
    "    \"\"\"\n",
    "    Fair Representation Learning: Remove correlation with sensitive attribute\n",
    "    \"\"\"\n",
    "    # Use CorrelationRemover from fairlearn\n",
    "    remover = CorrelationRemover(sensitive_feature_ids=[X_train.shape[1]])\n",
    "    # Add sensitive attribute as last column\n",
    "    X_train_extended = np.column_stack([X_train, sensitive_train])\n",
    "    X_test_extended = np.column_stack([X_test, np.zeros(len(X_test))])\n",
    "    # Fit and transform\n",
    "    X_train_fair = remover.fit_transform(X_train_extended)\n",
    "    X_test_fair = remover.transform(X_test_extended)\n",
    "    # Remove the last column (was sensitive attribute)\n",
    "    X_train_fair = X_train_fair[:, :-1]\n",
    "    X_test_fair = X_test_fair[:, :-1]\n",
    "    return X_train_fair, X_test_fair\n",
    "# ============================================================================\n",
    "# IN-PROCESSING TECHNIQUES\n",
    "# ============================================================================\n",
    "def train_fair_model(X_train, y_train, sensitive_train, method='reweighing'):\n",
    "    \"\"\"\n",
    "    Train model with fairness constraints during training\n",
    "    \"\"\"\n",
    "    if method == 'reweighing':\n",
    "        # Use sample weights from reweighing\n",
    "        weights = preprocess_reweighing(X_train, y_train, sensitive_train)\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train, sample_weight=weights)\n",
    "    else:\n",
    "        # Standard training\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "    return model\n",
    "# ============================================================================\n",
    "# POST-PROCESSING TECHNIQUES\n",
    "# ============================================================================\n",
    "def postprocess_equalized_odds(model, X_test, y_test, sensitive_test):\n",
    "    \"\"\"\n",
    "    Post-processing: Adjust predictions to achieve equalized odds\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    # Use ThresholdOptimizer from fairlearn\n",
    "    threshold_optimizer = ThresholdOptimizer(\n",
    "        estimator=model,\n",
    "        constraints=\"equalized_odds\",\n",
    "        prefit=True\n",
    "    )\n",
    "    threshold_optimizer.fit(X_test, y_test, sensitive_features=sensitive_test)\n",
    "    y_pred_fair = threshold_optimizer.predict(X_test, sensitive_features=sensitive_test)\n",
    "    return y_pred_fair\n",
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "def evaluate_fairness(y_true, y_pred, sensitive):\n",
    "    \"\"\"\n",
    "    Evaluate fairness metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'demographic_parity_diff': demographic_parity_difference(\n",
    "            y_true, y_pred, sensitive_features=sensitive\n",
    "        ),\n",
    "        'equalized_odds_diff': equalized_odds_difference(\n",
    "            y_true, y_pred, sensitive_features=sensitive\n",
    "        ),\n",
    "        'accuracy': accuracy_score(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "def compare_mitigation_techniques(df):\n",
    "    \"\"\"\n",
    "    Compare different bias mitigation techniques\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X = df[['feature1', 'feature2']].values\n",
    "    y = df['target'].values\n",
    "    sensitive = df['sensitive'].values\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "        X, y, sensitive, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    results = {}\n",
    "    # 1. Baseline (no mitigation)\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. BASELINE MODEL (No Mitigation)\")\n",
    "    print(\"=\"*80)\n",
    "    baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    baseline_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "    results['Baseline'] = evaluate_fairness(y_test, y_pred_baseline, sensitive_test)\n",
    "    print(f\"Demographic Parity Difference: {results[\"Baseline']['demographic_parity_diff']:.4f}')\n",
    "    print(f\"Equalized Odds Difference: {results[\"Baseline']['equalized_odds_diff']:.4f}')\n",
    "    print(f\"Accuracy: {results[\"Baseline']['accuracy']:.4f}')\n",
    "    # 2. Pre-processing: Reweighing\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. PRE-PROCESSING: REWEIGHING\")\n",
    "    print(\"=\"*80)\n",
    "    weights = preprocess_reweighing(X_train_scaled, y_train, sensitive_train)\n",
    "    reweigh_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    reweigh_model.fit(X_train_scaled, y_train, sample_weight=weights)\n",
    "    y_pred_reweigh = reweigh_model.predict(X_test_scaled)\n",
    "    results['Reweighing'] = evaluate_fairness(y_test, y_pred_reweigh, sensitive_test)\n",
    "    print(f\"Demographic Parity Difference: {results[\"Reweighing']['demographic_parity_diff']:.4f}')\n",
    "    print(f\"Equalized Odds Difference: {results[\"Reweighing']['equalized_odds_diff']:.4f}')\n",
    "    print(f\"Accuracy: {results[\"Reweighing']['accuracy']:.4f}')\n",
    "    # 3. Pre-processing: Correlation Removal\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. PRE-PROCESSING: CORRELATION REMOVAL\")\n",
    "    print(\"=\"*80)\n",
    "    X_train_fair, X_test_fair = preprocess_correlation_removal(\n",
    "        X_train_scaled, X_test_scaled, sensitive_train\n",
    "    )\n",
    "    corr_removal_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    corr_removal_model.fit(X_train_fair, y_train)\n",
    "    y_pred_corr = corr_removal_model.predict(X_test_fair)\n",
    "    results['Correlation Removal'] = evaluate_fairness(y_test, y_pred_corr, sensitive_test)\n",
    "    print(f\"Demographic Parity Difference: {results[\"Correlation Removal']['demographic_parity_diff']:.4f}')\n",
    "    print(f\"Equalized Odds Difference: {results[\"Correlation Removal']['equalized_odds_diff']:.4f}')\n",
    "    print(f\"Accuracy: {results[\"Correlation Removal']['accuracy']:.4f}')\n",
    "    # 4. Post-processing: Equalized Odds\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4. POST-PROCESSING: EQUALIZED ODDS\")\n",
    "    print(\"=\"*80)\n",
    "    y_pred_post = postprocess_equalized_odds(\n",
    "        baseline_model, X_test_scaled, y_test, sensitive_test\n",
    "    )\n",
    "    results['Post-processing'] = evaluate_fairness(y_test, y_pred_post, sensitive_test)\n",
    "    print(f\"Demographic Parity Difference: {results[\"Post-processing']['demographic_parity_diff']:.4f}')\n",
    "    print(f\"Equalized Odds Difference: {results[\"Post-processing']['equalized_odds_diff']:.4f}')\n",
    "    print(f\"Accuracy: {results[\"Post-processing']['accuracy']:.4f}')\n",
    "    return results, {\n",
    "        'baseline': (y_test, y_pred_baseline, sensitive_test),\n",
    "        'reweighing': (y_test, y_pred_reweigh, sensitive_test),\n",
    "        'correlation_removal': (y_test, y_pred_corr, sensitive_test),\n",
    "        'post_processing': (y_test, y_pred_post, sensitive_test)\n",
    "    }\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "def plot_mitigation_comparison(results):\n",
    "    \"\"\"\n",
    "    Plot comparison of different mitigation techniques\n",
    "    \"\"\"\n",
    "    methods = list(results.keys())\n",
    "    dp_diffs = [abs(results[m]['demographic_parity_diff']) for m in methods]\n",
    "    eo_diffs = [abs(results[m]['equalized_odds_diff']) for m in methods]\n",
    "    accuracies = [results[m]['accuracy'] for m in methods]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    # Demographic Parity Difference\n",
    "    axes[0].bar(methods, dp_diffs, color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12'])\n",
    "    axes[0].set_title('Demographic Parity Difference (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Absolute Difference')\n",
    "    axes[0].tick_params(axis='x', rotation=15)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    # Equalized Odds Difference\n",
    "    axes[1].bar(methods, eo_diffs, color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12'])\n",
    "    axes[1].set_title('Equalized Odds Difference (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Absolute Difference')\n",
    "    axes[1].tick_params(axis='x', rotation=15)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    # Accuracy\n",
    "    axes[2].bar(methods, accuracies, color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12'])\n",
    "    axes[2].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].tick_params(axis='x', rotation=15)\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    axes[2].set_ylim([0, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit2-bias-justice', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n Saved: bias_mitigation_comparison.png\")\n",
    "    plt.close()\n",
    "def plot_confusion_matrices(predictions_dict):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for each method\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.flatten()\n",
    "    titles = ['Baseline', 'Reweighing', 'Correlation Removal', 'Post-processing']\n",
    "    for idx, (method, (y_true, y_pred, sensitive)) in enumerate(predictions_dict.items()):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{titles[idx]}\\nAccuracy: {accuracy_score(y_true, y_pred):.3f}', \n",
    "                           fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit2-bias-justice', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\" Saved: bias_mitigation_confusion_matrices.png\")\n",
    "    plt.close()\n",
    "def plot_fairness_by_group(predictions_dict):\n",
    "    \"\"\"\n",
    "    Plot fairness metrics by sensitive group\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.flatten()\n",
    "    titles = ['Baseline', 'Reweighing', 'Correlation Removal', 'Post-processing']\n",
    "    for idx, (method, (y_true, y_pred, sensitive)) in enumerate(predictions_dict.items()):\n",
    "        # Calculate metrics by group\n",
    "        group_0 = sensitive == 0\n",
    "        group_1 = sensitive == 1\n",
    "        acc_group_0 = accuracy_score(y_true[group_0], y_pred[group_0])\n",
    "        acc_group_1 = accuracy_score(y_true[group_1], y_pred[group_1])\n",
    "        tpr_group_0 = np.sum((y_true[group_0] == 1) & (y_pred[group_0] == 1)) / max(np.sum(y_true[group_0] == 1), 1)\n",
    "        tpr_group_1 = np.sum((y_true[group_1] == 1) & (y_pred[group_1] == 1)) / max(np.sum(y_true[group_1] == 1), 1)\n",
    "        groups = ['Group A', 'Group B']\n",
    "        accuracies = [acc_group_0, acc_group_1]\n",
    "        tprs = [tpr_group_0, tpr_group_1]\n",
    "        x = np.arange(len(groups))\n",
    "        width = 0.35\n",
    "        axes[idx].bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "        axes[idx].bar(x + width/2, tprs, width, label='True Positive Rate', alpha=0.8)\n",
    "        axes[idx].set_title(titles[idx], fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_ylabel('Score')\n",
    "        axes[idx].set_xticks(x)\n",
    "        axes[idx].set_xticklabels(groups)\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        axes[idx].set_ylim([0, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit2-bias-justice', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\" Saved: bias_mitigation_by_group.png\")\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"Unit 2 - Example 2: Bias Mitigation Techniques\")\n",
    "    print(\"=\"*80)\n",
    "    # Generate biased dataset\n",
    "    print(\"\\nGenerating synthetic biased dataset...\")\n",
    "    df = generate_biased_dataset(n_samples=2000, bias_strength=0.3)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(df['target'].value_counts())\n",
    "    print(f\"\\nSensitive attribute distribution:\")\n",
    "    print(df['sensitive'].value_counts())\n",
    "    # Compare mitigation techniques\n",
    "    results, predictions = compare_mitigation_techniques(df)\n",
    "    # Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating Visualizations...\")\n",
    "    print(\"=\"*80)\n",
    "    plot_mitigation_comparison(results)\n",
    "    plot_confusion_matrices(predictions)\n",
    "    plot_fairness_by_group(predictions)\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Takeaways:\")\n",
    "    print(\"1. Pre-processing techniques modify data before training\")\n",
    "    print(\"2. In-processing techniques modify the learning algorithm\")\n",
    "    print(\"3. Post-processing techniques adjust predictions after training\")\n",
    "    print(\"4. Each technique has trade-offs between fairness and accuracy\")\n",
    "    print(\"5. The best technique depends on the specific use case and requirements\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary: What We Learned | Ø§Ù„Ù…Ù„Ø®Øµ: Ù…Ø§ ØªØ¹Ù„Ù…Ù†Ø§Ù‡\n",
    "\n",
    "**BEFORE this notebook**: We knew how to detect bias but didn't know how to fix it.\n",
    "\n",
    "**AFTER this notebook**: We can:\n",
    "- âœ… Understand three types of bias mitigation (pre-processing, in-processing, post-processing)\n",
    "- âœ… Implement reweighing for pre-processing\n",
    "- âœ… Implement correlation removal for fair representation\n",
    "- âœ… Implement threshold optimization for post-processing\n",
    "- âœ… Compare effectiveness of different mitigation techniques\n",
    "- âœ… Choose the right technique for different scenarios\n",
    "\n",
    "### Key Takeaways | Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Pre-processing**: Fix bias in data before training (reweighing, correlation removal)\n",
    "2. **In-processing**: Modify learning algorithm during training (fairness constraints)\n",
    "3. **Post-processing**: Adjust predictions after training (threshold optimization)\n",
    "4. **Trade-offs**: Each technique balances fairness and accuracy differently\n",
    "5. **Context Matters**: Best technique depends on use case and requirements\n",
    "\n",
    "### Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "- ðŸ““ **Example 3**: Fair Representation (ensuring fair representation in data!)\n",
    "- ðŸ““ **Example 4**: Bias Case Studies (analyze real-world bias cases!)\n",
    "- ðŸ““ **Example 5**: Fair AI Development (build fair AI systems from the start!)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** ðŸŽ‰ You've learned how to mitigate bias in machine learning models using multiple techniques!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement pre-processing bias mitigation techniques\n",
    "# These techniques fix bias in the data before training the model\n",
    "\n",
    "# BEFORE: We have biased data that will create biased models\n",
    "# AFTER: We'll have techniques to fix bias in the data\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”§ PRE-PROCESSING MITIGATION TECHNIQUES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWe'll implement two pre-processing techniques:\")\n",
    "print(\"  1. Reweighing: Balance groups by adjusting sample weights\")\n",
    "print(\"  2. Correlation Removal: Remove correlation with sensitive attribute\\n\")\n",
    "\n",
    "def preprocess_reweighing(X_train, y_train, sensitive_train):\n",
    "    pass\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}