# Course 01 - Course Summary
## ŸÖŸÑÿÆÿµ ÿßŸÑÿØŸàÿ±ÿ©

This document provides a comprehensive text summary of all course materials.
Ÿáÿ∞ÿß ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ ŸäŸàŸÅÿ± ŸÖŸÑÿÆÿµ ŸÜÿµŸä ÿ¥ÿßŸÖŸÑ ŸÑÿ¨ŸÖŸäÿπ ŸÖŸàÿßÿØ ÿßŸÑÿØŸàÿ±ÿ©.

---


## Pptx Files



### 01

--- Page 1 ---

AI Diploma

--- Page 2 ---

Semester One | Course OneIntroduction to Artificial
Intelligence and Applications

--- Page 3 ---

Unit 1  : Introduction and Applications to AI 01Course Content
02
03
04Unit 2 : AI Concepts , Terminology, and Application Domains - Part 1
Unit 3 : AI Concepts , Terminology, and Application Domains - Part 2
Unit 4 : Business and Career Transformation Through AI
05Unit 5 : Issues, Concerns, and Ethical Consideration

--- Page 4 ---

Before we start ...
https://discord.tuwaiqadmin.com/invite/cm4zmd25000032ibvz8z6vbns

--- Page 5 ---

Welcome to the course! Before we dive in, let‚Äôs take a moment to align on some shared commitments to ensure a
successful and enriching learning journey.Commitment to Learning and EngagementBefore we start ...
Our Commitments to You:
Engaging Content: We‚Äôll provide well-structured, interactive, and up-to-date lessons.
Supportive Environment: Our team is here to answer questions and guide you through challenges.
Real-World Applications: The course focuses on practical skills and knowledge you can use in your career.
Your Commitments as a Learner:
Active Participation: Engage fully in discussions, activities, and quizzes.
Open Mindset: Be ready to explore new ideas and challenge assumptions.
Accountability: Stay consistent with your learning schedule and complete assignments on time.
Collaboration: Support and learn from your peers in a respectful and constructive manner.
Ethics: Approach AI concepts with responsibility and an understanding of their ethical implications.

--- Page 6 ---

Course Overview: Exploring the World of AIBefore we start ...
Welcome to this exciting journey into Artificial Intelligence! Below is an outline of the key units you will explore
during this course, each designed to provide you with a comprehensive understanding of AI concepts, applications,
and ethical considerations.
 Unit 1  : Introduction and Applications to AI
Introduction to AI 1.
What is Intelligence? 2.
Weak AI and Strong AI 3.
Typical (Intelligent) System 4.
A BRIEF HISTORY OF AI 5.
Agents & Rationality 6.
PHILOSOPHY OF AI 7.
Strong AI 8.
Classical Search Algorithm CLASSICAL 9.
Greedy Search Algorithm for Simple Applications (Application) 10.
Adversarial Search 11.
Knowledge-based Agent 12.
Introduction to Logical Reasoning 13.
Forward & Backward Chainings 14.
To be continued ...

--- Page 7 ---

This content was designed using the following resources:Resources
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.
https://www.coursera.org/learn/introduction-to-ai/home/module/1
https://eecs.wsu.edu/~cook/ai/lectures/p.html
https://www.teach.cs.toronto.edu/~csc384h/summer/lectures.html
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 8 ---

Semester One | Course OneIntroduction to Artificial Intelligence and Applications 
Unit 1 : Introduction
and Applications to AI

--- Page 9 ---

WHAT IS INTELLIGENCE?
STRONG & WEAK AI

--- Page 10 ---

WHAT IS INTELLIGENCE?
‚ÄùIt is not my aim to surprise or shock you ‚Äì but the simplest way I can
summarize is to say that there are now in the world machines that can
think, that learn, and that create.
Moreover, their ability to do these things is going to increase rapidly until
‚Äî in a visible future ‚Äî the range of problems they can handle will be
coextensive with the range to which human mind has been applied.‚Äù
by Herbert A Simon (1957)
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 11 ---

STRONG AND WEAK AI
Weak AI ‚Äî acting intelligently
The belief that machines can be made to act as if they are intelligent
Strong AI ‚Äî being intelligent
The belief that those machines are actually thinking
Most AI researchers don‚Äôt care
‚Äúthe question of whether machines can think...
...is about as relevant as whether submarines can swim.‚Äù 
(Edsger W Dijkstra, 1984)
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 12 ---

WEAK AI 
Weak AI is a category that is flexible
as soon as we understand how an AI-program works, it appears less
‚Äúintelligent‚Äù.
And as soon as AI is successful, it becomes an own research area!
e.g., search algorithms, natural language processing, optimization,
theorem proving, machine learning etc.
And AI is left with the remaining hard-to-solve problems!
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 13 ---

WHAT IS AN AI SYSTEM?
Do we want a system that...
thinks like a human?
cognitive neuroscience / cognitive modelling
AGI = artificial general intelligence 
acts like a human?
the Turing test 
thinks rationally?
‚Äúlaws of thought‚Äù
from Aristotle‚Äôs syllogism to modern day theorem provers 
acts rationally?
‚Äúrational agents‚Äù
maximise goal achievement, given available information
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 14 ---

AI IS MULTIDISCIPLINARY
IntelligenceBehavior
Reasoning
Action
Human
PerformanceIdeal
PerformanceThinking
HumanlyThinking
Rationally
Acting
HumanlyActing
RationallyCognitive ModelingMathematical Logic
Turing TestEngineering :
 Goal-directed behavior
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 15 ---

Proposed by Alan Turing (1950) to operationally define intelligence.
A computer passes the test if a human interrogator cannot distinguish its responses from a human's.
Capabilities required to pass the standard Turing Test:
Natural Language Processing: For successful communication.
Knowledge Representation: To store and use information.
Automated Reasoning: To answer questions and draw conclusions.
Machine Learning: To adapt and detect patterns.
The test avoids physical interaction as simulating a person physically isn't essential for intelligence.
Total Turing Test:
Includes video signals for testing perceptual abilities and physical object interaction.
Additional requirements:
Computer Vision: To perceive objects.
Robotics: For object manipulation and movement.
ACTING HUMANLY
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 16 ---

The cognitive modeling approach o Trying to understand and model how the human mind works
 Cognitive Science focuses on modeling how people think.
We need to understand the actual workings of human minds. There are three ways to do this:
Introspection ‚Äì trying to catch our own thoughts as they go by.
Psychological experiments ‚Äì observing a person in action.
Brain imaging ‚Äì observing the brain in action.
The interdisciplinary field of cognitive science brings together computer models from AI and experimental
techniques from psychology to construct precise and testable theories of the human mind.THINKING HUMANLY
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 17 ---

The "laws of thought" approach,
Trying to understand how we actually think, and model how we should think.
Use ‚Äúsymbolic logic‚Äù to capture and manipulated the laws of rational thought as symbols for the
derivation for thoughts.
Limitations?
Not all intelligent behavior is mediated by logical deliberation.
Not easy to take informal knowledge and state it in the formal terms required by logical notation,
particularly when the knowledge is less than 100% certain.THINKING RATIONALLY
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 18 ---

The "rational agent" approach,
To achieve one‚Äôs goals, given one‚Äôs beliefs or understanding about the world.
An agent is a system that perceives an environment and acts within that environment.
Abstractly, an agent is a function that maps percept histories to actions: ùëì : ùí´  ‚Üí ùíú
An intelligent agent is one that acts rationally with respect to its goals.
For example, an agent that is designed to play a game should make moves that increase its chances
of winning the game!
A rational agent is one that acts rationally to achieve the best outcome, or the best-expected outcome
when there is uncertainty.
Achieving ‚Äúperfect rationality‚Äù, i.e., making the best decision theoretically possible, is not usually possible
due to limited resources in a real environment (e.g., time, memory, computational power, uncertainty, etc.).
‚Ä¢ The trick is to do the best with the available information and resources.ACTING RATIONALLY
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 19 ---

Intelligence involves sensing, reasoning, and acting.TYPICAL (INTELLIGENT) SYSTEM
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 20 ---

THE OBJECTVIE OF AI
The ultimate goal of artificial intelligence is to reach human-
level intelligence on a wide range of tasks.
Solve complex problems that human cannot perform,
understand, or comprehend.
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 21 ---

ELEMENTS OF INTELLIGENCE
Reasoning
Perception
Learning
Problem
SolvingElements
of 
Intelligence1
2
3
4
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 22 ---

A BRIEF HISTORY OF AI  
‚ÄúTHE THREE WAVES OF AI‚Äù

### 02

--- Page 1 ---

NOTABLE AI MOMENTS (1940‚Äì1970)
Year Event
1943 McCulloch & Pitts: Boolean circuit model of the brain
1950 ‚ÄùAlan Turing‚Äôs ‚ÄúComputing Machinery and Intelligence
1951 Marvin Minsky develops a neural network machine
1950sEarly AI programs: Samuel‚Äôs checkers program, Gelernter‚Äôs Geometry Engine, Newell & Simon‚Äôs Logic Theorist and
General Problem Solver
1956 Dartmouth meeting: ‚ÄúArtificial Intelligence‚Äù term adopted
1965 Robinson‚Äôs complete algorithm for logical reasoning
1966 Joseph Weizenbaum creates Eliza
1969 Minsky & Papert show limitations of the perceptron; Neural network research declines significantly
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 2 ---

NOTABLE AI MOMENTS (1970‚Äì2000)
Year Event
1971 Terry Winograd‚Äôs Shrdlu dialogue system
1972 Alain Colmerauer invents Prolog programming language
1976 MYCIN, an expert system for disease diagnosis
1980sEra of expert systems
1990sNeural networks, probability theory, AI agents
1993 RoboCup initiative to build soccer-playing robots
1997 IBM Deep Blue beats the World Chess Champion
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 3 ---

NOTABLE AI MOMENTS (2000‚Äì2018)
Year Event
2003 Very large datasets: genomic sequences
2007 Very large datasets: WAC (web as corpus)
2011 IBM Watson wins Jeopardy
2012 US state of Nevada permits driverless cars
2010sDeep learning takes over: recommendation systems, image analysis, board games, machine translation, pattern
recognition
2017 Google AlphaGo beats the world‚Äôs best Go player, Ke Jie
2017 AlphaZero learns board games by itself and beats the best programs
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 4 ---

What is the Forth Wave of AI ? (2018 - 2024) 
YearEvent
2018 Generative Adversarial Networks (GANs) gain traction, driving advances in realistic image and video generation.
2019Transformer architectures dominate natural language processing and generative tasks, influencing many models beyond
OpenAI‚Äôs developments.
2020Multimodal AI systems emerge, combining text, image, and audio generation capabilities, showcasing potential in
creative and scientific fields.
2021DALL-E-like models and image generation models see broader adoption, enabling detailed and creative image synthesis
from text prompts.
2022Generative AI for audio advances, with models capable of generating music and realistic speech, expanding AI‚Äôs
applications in entertainment.
2023Text-to-3D generation becomes viable, with models generating 3D objects and animations from textual descriptions,
aiding industries like gaming and design.
2024Autonomous AI agents capable of generating text, code, images, and simulations work in tandem, revolutionizing
productivity and creativity.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 5 ---

CLASSICAL AI VS MODERN AI
The goal of classical AI was to explicitly
represent human knowledge using facts
and rules.
Facts and rules had to be explicitly
specified by people which makes them
either limited or not well-defined!Classical AI (Before 1990s) Modern AI (Post-2000s):
Key Techniques include:
Expert systems, Rule-based systems,
Fuzzy systems, and Symbolic ReasoningModern AI has become more effective due
to advancements in data volume,
statistical models, and computing power. 
It can autonomously infer rules, patterns,
and irregularities from data.
Key Techniques include:
Machine learning, computational intelligence
(e.g., neural networks,).
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 6 ---

Agents & Rationality

--- Page 7 ---

An agent perceives its environment via sensors and acts upon it via actuators.
Examples of Agents:
Human Agent:
Sensors: Eyes, ears, other organs.
Actuators: Hands, legs, vocal tract.
Robotic Agent:
Sensors: Cameras, infrared range finders.
Actuators: Motors, other mechanical parts.
Key Design Elements: PAGE
Percepts: Inputs from the environment.
Actions: Outputs to affect the environment.
Goals: Objectives to achieve.
Environments: Contexts in which the agent operates.
What is an Agent? 
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 8 ---

Percepts: Location, contents (e.g., dirty/clean).
Actions: Left, Right, Clean, NoOp.
Agent Function:
If square is dirty ‚Üí Clean.
Else ‚Üí Move to another square.
Key Question:
What makes a good agent function? Who decides?
Agent Example: Vacuum-Cleaner Agent
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 9 ---

Performance Measure:
Points for squares cleaned per time.
Penalties for unnecessary moves.
Rational Agent:
Maximizes the performance measure based on
percept history and built-in knowledge.
Rational ‚â† Successful:
Rational agents lack omniscience and cannot
predict all outcomes.Rationality & Performance
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 10 ---

PEAS (Task Environment Design):
Performance Measure: Success criteria.
Environment: External world.
Actuators: Action mechanisms.
Sensors: Perception mechanisms.PEAS Framework
Example: Autonomous Car:
Performance: Safety, fuel efficiency, time optimization.
Environment: Roads, traffic, pedestrians.
Actuators: Steering, brakes, signals.
Sensors: Cameras, GPS, sonar.
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 11 ---

Environment Types
Dimensions of Complexity:
Observable: Full vs. Partial.
Deterministic: Deterministic vs. Stochastic.
Episodic: Episodic vs. Sequential.
Static: Static vs. Dynamic.
Discrete: Discrete vs. Continuous.
Agents: Single vs. Multiple.
Real World:
Partially Observable, Stochastic, Sequential, Dynamic,
Continuous, Multi-Agent.Environment Types
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 12 ---

Fully observable vs. partially observable:
Fully Observable Environment: The agent's sensors provide complete information about the environment's
state, allowing it to choose actions without uncertainty.
Partially Observable Environment: The agent lacks full information due to noisy or inaccurate sensors or
missing data, requiring it to make informed guesses about the environment.The Nature of Environments
Single-agent vs. multi-agent :
Single-Agent Environment: The agent operates independently without interactions with other agents.
Multi-Agent Environment: Multiple agents interact, which can be competitive (e.g., chess: one agent's gain is
another's loss) or cooperative (e.g., taxi-driving: avoiding collisions benefits all agents) or a mix of both.
Deterministic vs. stochastic:
Deterministic Environment: The next state is entirely determined by the current state and the agent's actions;
otherwise, it is stochastic (e.g., taxi driving with unpredictable events).
Uncertain Environment: Either not fully observable or not deterministic; stochastic environments involve
probabilistic outcomes, while non-deterministic ones lack defined probabilities for outcomes.
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 13 ---

Episodic vs. sequential :
Episodic Environment: Each decision is independent of past and future decisions, simplifying the agent's task
(e.g., spotting defective parts on an assembly line).
Sequential Environment: Current decisions affect future outcomes, requiring the agent to consider long-term
consequences (e.g., chess, taxi driving).The Nature of Environments
Static vs. dynamic:
Static Environment: Does not change while the agent is deliberating, simplifying decision-making as the agent
need not monitor changes (e.g., a solved puzzle).
Dynamic Environment: Continuously changes, requiring the agent to act quickly as inaction is treated as a
decision (e.g., real-time navigation). Semi-Dynamic: The environment stays static, but the agent's
performance score changes over time.
Discrete vs. continuous:
Static Environment: Remains unchanged during the agent's deliberation, simplifying decision-making as time and
environmental updates are not a concern.
Dynamic Environment: Changes while the agent deliberates, requiring quick decisions as inaction is treated as
doing nothing; Semi-Dynamic: The environment is static, but the agent's performance score changes over time.
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 14 ---

TaskObservabl
eDeterminis
ticStatic Episodic Discrete Agents
Chess (with
clock)Fully Deterministic Semi-dyn. Sequential Discrete Multiple (Comp.)
Poker Partially Stochastic Static Sequential Discrete Multiple (Comp.)
Driving Partially Stochastic Dynamic Sequential Continuous Multiple (Coop.)
Image
RecognitionFully Deterministic Static Episodic Disc./Cont. SingleEnvironment Examples
The real world is...
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 15 ---

Defining Solutions:
Solutions must handle unstated assumptions with common-
sense reasoning.
Types of Solutions:
Optimal: Best solution by quality measure.
Satisfying: Adequate solution.
Approx. Optimal: Close to the best possible.
Probable: Likely a valid solution.Solution Quality
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 16 ---

Simple Reflex Agent:
Acts based on current percept; ignores history.
Model-Based Reflex Agent:
Maintains internal state based on percept history.
Goal-Based Agent:
Actions aim to achieve defined goals.
Utility-Based Agent:
Measures performance through utility functions.
Learning Agent:
Adapts and improves through experience (online/offline).Types of Agents
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 17 ---

This type of agent is usually made of a general-purpose interpreter for condition-action rules to
create rule sets for specific task environments.
For example: the vacuum world agent is a simple reflex agent (because its decision is based
only on the current location and on whether that location contains dirt).
By ignoring the percept history, the agent program become very small compared to the corresponding
table (from 4T to 4 possibilities).
This type relies on condition-action rules (Also called situation- action rules, productions, or if-then
rules):
if car-in-front-is-braking then initiate-braking. 
Advantage:
Simple enough!  
Disadvantage:
Limited intelligence ~ Actions depend only on the current information provided by their sensors!
Works only if the environment is fully observable!
Can lead to Infinite loops.Simple reflex agents
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 18 ---

The most effective way to handle partial observability is to keep track of the part of the world that the
agent can‚Äôt see now.
The agent maintains an internal state (i.e., memory) that depends on the percept history.
Hence, has an access to information that is not currently available to their sensors.
The internal state can contain information about the state of the external environment.
This knowledge about ‚Äúhow the world works‚Äù is called a model of the world. An agent that uses such a
model is called a model-based agent.Model-based reflex agents
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 19 ---

Knowing something about the current state of the environment is not always enough to decide what
to do.
For example: at a road intersection, the taxi can turn left, turn right, or go straight on. The correct
decision depends on where the taxi is trying to go to.
The appropriate action for the agent will often depend on what its goals are, and so it must be provided
with some goal information.
If a long sequence of actions is required to reach the goal, then search and planning must be
implemented.
The goal-based agent is more flexible:
We can simply specify a new goal, rather than re-programming all the rules.Goal-based agents
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 20 ---

Goals alone are not enough to generate high-quality behavior in most environments.
This is because there are often many sequences of actions that can result in the same goal being
achieved.
For example: many action sequences will get the taxi to its destination (thereby achieving the goal)
but some are quicker, safer, more reliable, or cheaper than others.
This type of agent programs appear when we design decision making agents that must handle the
uncertainty inherent in stochastic or partially observable environments.
This type of agent makes its decisions based on the maximum utility of its choices. Its focus is not
only on achieving goals but to find the best alternative/way to reach that particular goal.
To do so, we utilize a utility function of choice, which maps a state or sequence of states to a value, to
rate each possible solution against the general performance measure.
A rational utility-based agent always chooses the action that maximizes the expected utility of the action
outcomes.Utility-based agents
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 21 ---

his type of agent operates in initially unknown environments and become more knowledgeable over time
so as to improve performance.
Consists of four main components:
Critic: Evaluates how well the agent is doing wrt the external performance standard.
Learning element:: Makes improvements
Performance element: Contains the knowledge about the environment and selects the actions.
Problem generator: Suggests actions that will lead to new and informative experiences.
For example, from Google Assistant to other predictive searches, all use learning agents to adapt and
learn about the user and make accurate suggestions, and recommendations, and deliver appropriate
ads.Learning agents
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 22 ---

Agent Logic & Types
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 23 ---

Each state of the world is indivisible (it has no internal
structure).
For example: the problem of finding a driving
route from one end of a country to the other via
some sequence of cities.
For the purposes of solving this problem, it may suffice to
reduce the state of world to just the name of the city we
are in‚Äî a single atom of knowledge.
The algorithms underlying search and game-playing (see
later this semester) all work mostly with atomic
representations.Agent Representations
Atomic representation
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 24 ---

A factored representation splits up each state into a
fixed set of variables or attributes, each of which can
have a value.
Two different factored states can share some
attributes such as being at some particular GPS
location (see back circles in right figure).Agent Representations
Factored representation
Structured representation describes the world using
variables while capturing knowledge and reasoning.Structured representation
Jackson, P. C. (2019). Introduction to artificial intelligence. Courier Dover Publications.

--- Page 25 ---

PHILOSOPHY OF AI 
IS AI POSSIBLE?
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 26 ---

There are different opinions...
...some are slightly positive:
‚Äúevery feature of intelligence can be so precisely described that a
machine can be made to simulate it‚Äù (McCarthy et al, 1955) ...
and some lean towards the negative:
‚ÄúAI stands not even a ghost of a chance of producing durable results‚Äù
(Sayre, 1993)
It‚Äôs all in the definitions:
what do we mean by ‚Äúthinking‚Äù and ‚Äúintelligence‚Äù?Is AI Possible ? 
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 27 ---

Paper: "Computing Machinery and Intelligence" ‚Äì Alan Turing (1950).
Key Contributions:
Introduced the "imitation game" (Turing Test) to define intelligence.
Discussed objections to AI, including nearly all objections raised since.
A must-read foundational work in AI and philosophy!Turing's Key Contribution
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 28 ---

TURING‚Äôs Key Objections
The Theological
Objection:
Claim: Thinking
requires an
immortal soul,
which only
humans have.01
The "Heads in
the Sand"
Objection:
Claim: The idea
of machines
thinking is too
dreadful; we hope
it‚Äôs not true.02
The Mathematical
Objection:
Claim: Based on
G√∂del‚Äôs
incompleteness
theorem, there are
limits to what machines
can do logically.03
The Argument from
Consciousness:
Claim: Machines
cannot feel
emotions like
pleasure, grief, or
anger.04
Arguments from Various
Disabilities:
Claim: Machines will never do
certain human things like:
Be kind, resourceful,
beautiful, friendly.
Have a sense of humor or
tell right from wrong.
Fall in love or enjoy
strawberries and cream.05
Lady Lovelace‚Äôs
Objection:
Claim: Machines
cannot originate
anything; they
only do what we
program them to
do.06
Argument from
Continuity in the
Nervous System:
Claim: The nervous
system‚Äôs continuous
behavior cannot be
mimicked by
discrete-state
machines.07
Argument from
Informality of
Behavior:
Claim: Human
behavior lacks strict
rules; machines with
fixed rules cannot
replicate this.08
The Argument from Extrasensory Perception (ESP):
Scenario: A man with telepathic abilities guesses the
suit of cards more accurately than a machine.
Claim: Machines cannot replicate such abilities.
Turing‚Äôs Comment: This was the strongest argument
in his view, given the statistical evidence for ESP at
the time.09
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 29 ---

Are Turing‚Äôs objections still
relevant, and can machines
truly "think" or "feel"?

--- Page 30 ---

Assignment: Reflection on Turing‚Äôs Objections.
‚ÄúTopic: Are Turing‚Äôs objections still relevant, and
can machines truly "think" or "feel"?‚Äù
Write a one-page research responding this question
giving your perspective and what you have
witnessed in today‚Äùs AI !

--- Page 31 ---

Strong AI

--- Page 32 ---

Done by Searle (1980) and Moravec (1988).. 
Suppose we gradually replace each neuron in your head with an
electronic copy...
...what will happen to your mind, your consciousness?
Searle argues that you will gradually feel dislocated from your body 
Moravec argues you won‚Äôt notice anythingTHE BRAIN REPLACEMENT EXPERIMENT
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 33 ---

Will AI lead to superintelligence?
‚Äú...ever accelerating progress of technology and changes in the mode of human life, which gives the
appearance of approaching some essential singularity in the history of the race beyond which human
affairs, as we know them, could not continue‚Äù (von Neumann, mid-1950s)
‚ÄúWe will successfully reverse-engineer the human brain by the mid-2020s. By the end of that decade,
computers will be capable of human-level intelligence.‚Äù (Kurzweil, 2011)
‚ÄúThere is not the slightest reason to believe in a coming singularity.‚Äù (Pinker, 2008)THE TECHNOLOGICAL SINGULARITY
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 34 ---

CLASSICAL SEARCH
ALGORITHMS

--- Page 35 ---

Often we are not given an algorithm to solve a problem, but only a specification of a solution
‚Äî we have to search for it.
A typical problem is when the agent is in one state, it has a set of deterministic actions it can
carry out, and wants to get to a goal state.
Many AI problems can be abstracted into the problem of finding a path in a directed graph.
Often there is more than one way to represent a problem as a graph.
State-Space Search:
The agent explores possible states and actions to find a solution.
The goal is to find a path from the start state to the goal state.WHAT IS SEARCH IN AI?
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 36 ---

Problems are often represented as graphs.
For example, consider cities as nodes and roads connecting them as edges. A path
represents a series of roads leading from one city to another.
Nodes (States): Represent the points or conditions in the problem.
Edges (Actions): Transitions or steps that connect nodes.
Path: A sequence of edges leading from the start node to the goal node. Problems are often
represented as graphs.GRAPHS AND STATES:
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 37 ---

We want to drive from Arad to Bucharest in AradEXAMPLE: TRAVEL IN ROMANIA
NodeEdges
Path
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 38 ---

Grid game: Rob needs to collect coins C1, C2, C3, C4 ,without running out of fuel, and end up at location (1,1):EXAMPLE: GRID GAME
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 39 ---

EXAMPLE: VACUUM-CLEANING AGENT
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 40 ---

EXAMPLE: THE 8-PUZZLE
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 41 ---

EXAMPLE: THE 8-QUEENS PROBLEM
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 42 ---

Uninformed Search:
These algorithms do not use additional information about the goal.
Depth-First Search (DFS):
Imagine exploring a maze by always turning left until you hit a dead end, then
backtracking to try a new path. This is how DFS operates. Explores one path
deeply before backtracking.
Breadth-First Search (BFS): 
Explores all nodes at a given depth before going deeper.
Uniform-Cost Search (UCS): 
Explores paths with the lowest cost first.TYPES OF SEARCH ALGORITHMS:
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 43 ---

Heuristic Search:
These algorithms use a heuristic (a guiding function) to estimate the closeness to the goal.
What is a Heuristic?
A heuristic is a function that estimates how close a node is to the goal.
Examples of Heuristics:
Straight-Line Distance (SLD): The shortest distance between two points in navigation.
Manhattan Distance: Used in grid-based problems, sums the horizontal and vertical steps to
the goal.
Misplaced Tiles: For the 8-puzzle, counts how many tiles are not in their goal positions.
Greedy Best-First Search: 
Chooses the node that appears closest to the goal.
A*Search:
Combines UCS and heuristic information for optimal pathfinding.TYPES OF SEARCH ALGORITHMS:
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 44 ---

A generic search algorithm:
Given a graph, start nodes, and a goal description, incrementally explore paths
from the start nodes.
Maintain a frontier of nodes that are to be explored.
As search proceeds, the frontier expands into the unexplored nodes
until a goal node is encountered.
The way in which the frontier is expanded defines the search strategy.
Tree Search vs. Graph Search:
Tree Search: Does not check if a node has been visited, which might lead to
revisiting states.
Graph Search: Keeps track of visited nodes to avoid duplicating work and loops.HOW DO WE SEARCH IN A GRAPH?
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 45 ---

TREE SEARCH VS. GRAPH SEARCH:
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 46 ---

GREEY BEST FIRST SEARCH | REAL WORLD EXAMPLE
Uninformed Search
No info about search space
Depth , Breadth First SearchHeuristic of Informed  Search
No heuristic function helps he search ‚Äú how far we are from the goal?‚Äù and
‚Äúhow to reach the goal?‚Äù 
Greedy Best First Search
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 47 ---

GREEY BEST FIRST SEARCH | REAL WORLD EXAMPLE
Faster and much efficient 
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 48 ---

GREEY BEST FIRST SEARCH | REAL WORLD EXAMPLE
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 49 ---

GREEY BEST FIRST SEARCH | DOES IT FAIL ?
Not Optimal
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 50 ---

Explore Search Algorithms:
Use the interactive demo tool to visualize different search strategies: Pathfinding.js Demo.
Experiment with the algorithms and heuristics on a grid-based search problem.
Observe how changes in heuristic affect the performance and result.
Live Demonstration:
Consider demonstrating the tool live during the lecture to show real-time performance of each
algorithm.
Prepared Examples:
Prepare a few pre-set scenarios to highlight key differences between algorithms, such as:
Comparing DFS, BFS, and A* on a maze with multiple solutions.
Illustrating the impact of a good vs. poor heuristic in A* search.
Activity:
Encourage students to test different scenarios to see how search algorithms behave.INTERACTIVE DEMO
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 51 ---

Search as the Foundation of AI:
Search is a fundamental concept in AI because it models decision-making and problem-solving.
Many real-world problems‚Äîfrom navigation systems to robotics‚Äîr ely on search algorithms to find
efficient solutions.
These methods serve as a bridge to more complex AI tasks such as learning, optimization, and
planning.
Applications of Search:
Robotics: Planning paths for autonomous robots.
Gaming: Designing AI opponents that plan moves intelligently.
Optimization: Finding optimal resource allocations in logistics.
Why Search Matters:
Search algorithms are the starting point for understanding broader AI techniques. By mastering them,
you build a strong foundation for more advanced topics like machine learning and neural networks.CONCLUSION
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 52 ---

Any Questions ?

### 03

--- Page 1 ---

Adversarial
Search

--- Page 2 ---

single agent controlling the environment
The agent looks to find a sequence of actions that leads to the goal | Goal-based search agent.
But what happens when other agents' interest conflict with you?
Game Playing | agent tries to anticipate the unpredictable opponent‚Äôs next move
Game-playing agents in AI are designed to be rational  and will choose the move that leads to the best or high-quality gain for them.
Adversarial search is a special type of search where multiple agents influence
the current problem state >> Competitive multi-agent environments.ADVERSARIAL SEARCH
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 3 ---

The solution in AI-based games is not a fixed sequence of actions but a strategy
(policy). 
We specify a move for every possible opponent reply.
Game playing in AI assumes competitive (unpredictable) multi-agent environments.ADVERSARIAL SEARCH
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 4 ---

Assumptions:
Environment is deterministic and fully observable (i.e., perfect information)
Competitive (unpredictable) multi-agent environments.
Two agents act alternately (taking turns).
Zero-sum game: One player in the game tries to maximize a single value, while the
other player tries to minimize it.
Game problem formulation:
The initial state.
Operators/Actions: legal moves a player can make.
Transition model: defines the result of a legal move.
Goal (terminal test): determines when the game is over.
Utility (payoff) function: measures the outcome of the game and its desirability.ADVERSARIAL SEARCH
Game Tree
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 5 ---

ADVERSARIAL SEARCH
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 6 ---

HOW ADVERSARIAL SEARCH WORKS?
Game Tree Representation:  The diagram represents all possible states of the game as a
tree. The root is the current state of the game, and the branches are the potential moves
available to each player.
Utility Values:  At the termina l nodes (leaves), utility values represent the outcome of the
game:
+1 if Max wins.
-1 if Min wins.
0 for a draw.
Backward Induction:  The Minimax algorithm  is applied to evaluate the best move:
Starting from the terminal nodes, the utility values propagate back up the tree.1.
Max selects the move with the highest value from the child nodes.2.
Min selects the move with the lowest value from the child nodes.3.
This alternation continues until the best move for Max is identified at the root.4.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 7 ---

MINIMAX ALGORITHM
Generate the Game Tree: Create a tree of all possible moves and outcomes from the current game state. 1.
Evaluate Terminal Nodes: Assign utility values to terminal nodes (end states). 2.
Backpropagate Utilities: 3.
If it‚Äôs Max‚Äôs turn, choose the child node with the maximum utility value.
If it‚Äôs Min‚Äôs turn, choose the child node with the minimum utility value.
Optimal Move Selection: At the root node (current game state), Max selects the move that leads to the best utility value for him
considering Min will respond optimally.4.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 8 ---

WHY IS IT ADVERSARIAL?
The search is adversarial because the two players have directly opposing objectives:
Max wants to maximize the utility for himself.
Min wants to minimize Max's utility, effectively working against Max‚Äôs strategy.
This setup mirrors many competitive scenarios (e.g., chess, checkers, tic-tac-toe), where
each player's decision depe nds not only on their strategy but also on predicting the
opponent's moves.
Adversarial search is central to AI in games, allowing algorithms to simulate and counteract
an opponent's moves effectively.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 9 ---

Complete?  Yes (if tree is finite)
Optimal?  Yes
Time Complexity:                      (m is max depth of the tree).
Space Complexity:
O(bm) when generates all actions at once
O(m) when generates actions one at a time.
The time complexity is a major problem in Minimax because it grows exponentially in the depth of the
tree, effectively with real games.
Possible remedy:  compu te the correct minimax decision without looking at every node in the game tree
√† Pruning!
We shall consider a modification of Minimax technique called ‚Äú Alpha-Beta Pruning‚Äù
EVALUATING MINIMAX SEARCH:
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 10 ---

A technique applied to the standard Minimax tree and it returns the same move as minimax  but
prunes away branches that cannot possibly influence the final decision.
Alpha-beta pruning  can be applied to trees of any depth, and it is often possible to prune entire
subtrees rather than just leaves.
Main elements: Each node in the tree include two values, ùú∂  and ùú∑ .
ùú∂ = the value of the best choice (i.e., highest-value) we have found so far at any choice point
along the path for MAX.
ùú∑ = the value of the best choice (i.e., lowest-value) we have found so far at any choice point along the
path for MIN.
Basic Principle:  If a move is determined worse than another move already examined, then there is no
need for further examination of the node, i.e., when ùú∂  ‚â• ùú∑ .ALPHA-BETA PRUNING
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

### 04

--- Page 1 ---

Knowledge-
based Agent

--- Page 2 ---

Knowledge  is information about a domain of interest that can be used  to solve
problems in that domain..
Solving a problem requires enough knowledge about it.
Framework of effective knowledge utilization:
1. Representing Knowledge (Knowledge Representation)
Creating a formal representation or encoding of the world.:
To make information computer-tractable (easy for a computer to process).
2. Using Knowledge (Reasoning, Inferencing, Planning)
Using the encoded knowledge to draw conclusions or make decisions.
Apply logical rules or algorithms.
Derive new facts or solve problems.
3. Obtaining New Knowledge (Learning)
Adding new information to the existing knowledge base.
Allows the system to adapt and improve over time.THE THEORY OF KNOWLEDGE
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 3 ---

KNOWLEDGE MODEL (BELLINGER 1980)
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 4 ---

BASIC TYPES OF KNOWLEDGE
1. Declarative Knowledge
Definition:
Knowing WHAT something is.
Key Features:
Explicit: Can be verbally or clearly articulated.
Expressed in declarative statements, such as:
Facts
Concepts
Principles
Examples:
"Paris is the capital of France."
"Water boils at 100¬∞C."2. Procedural Knowledge
Definition:
Knowing HOW to do something.
Key Features:
Implicit: Often difficult to articulate verbally.
Encoded as procedures or steps (e.g., code or
workflows).
Examples:
Riding a bike.
Writing a computer program to sort data.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 5 ---

KNOWLEDGE-BASED AGENT
Key Points About Agents:
Agents Don‚Äôt Need to Be Sophisticated to Be Intelligent
An agent can be intelligent and useful even without physical interaction (e.g., robots).
Limitations of Basic Agents
Agents from are not efficient or smart enough for complex environments.
What Makes a Good Agent?
Needs knowledge about its environment to make better decisions.
Crucial in:
Partially Observable Environments
Dynamic Environments
Situations requiring new tasks
Knowledge-Based Agents
Much of AI focuses on building knowledge-based systems and agents.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 6 ---

GENERIC ARCHITECTURE OF KNOWLEDGE-
BASED AGENTS
1. Knowledge Level
Definition:
Describes what the agent knows and its goals.
Example:
Automated taxi driver knows:
"Riyadh East Ring links South region and North region."
2. Logical Level
Definition:
Represents the agent's knowledge using logical sentences.
Example:
Knowledge Base contains:
Link(RUH_RING, S, N) (First-Order Logic representation).3. Implementation Level
Definition:
The implementation details, including functions and
data structures.
Example:
The logical sentence Link(RUH_RING, S, N) is
implemented as a C structure or a Prolog fact.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 7 ---

Key Formula:
Autonomous Agent = Knowledge-Based Agent + Learning Mechanism

--- Page 8 ---

WHAT IS KNOWLEDGE REPRESENTATION
Why is KR Important?
Problem-Solving Needs:
Large amounts of knowledge.
Mechanisms to manipulate that knowledge.
Definition:
KR is a field of AI focused on representing the world in a computer-tractable form.
Enables AI agents to perform intelligent and rational tasks.
Knowledge vs. Representation
Knowledge:
Describes the world.
Determines a system‚Äôs competence by what it knows.
Representation:
Encodes the knowledge.
Affects the system‚Äôs performance in solving problems.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 9 ---

WHAT IS KNOWLEDGE REPRESENTATION
Goals of KR for Intelligent Agents:
Formal Representation:
Use a knowledge representation language to describe the world.
Reasoning:
Make inferences using the representation.
Action Selection:
Decide and evaluate actions based on the reasoning process.
Requirements of Effective Knowledge Representation
Representational Adequacy:
Ability to represent all necessary knowledge for the domain.
Inferential Adequacy:
Ability to manipulate knowledge to produce new insights.
Inferential Efficiency:
Ability to efficiently update knowledge structures.
Acquisitional Efficiency:
Ability to easily acquire new knowledge (e.g., automatic methods)
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 10 ---

Logic-Based Systems:
Propositional Logic
First-order Predicate Calculus
Probabilistic Models:
Bayesian Belief Networks
Graph-Based Models:
Semantic Networks
Other Methods:
Natural Language Processing
(NLP)
Note: No single KR system works for all
scenarios. Complex AI systems often
combine multiple representations.EXAMPLES OF KR LANGUAGES
LOGIC-BASED SYSTEMS BAYESIAN BELIEF NETWORKS
SEMANTIC NETWORK | GRAPH BASED NLP
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 11 ---

KEY ASPECTS OF A KR LANGUAGE
Syntax:
Defines valid configurations of language components.
Example: 2 + 3 ‚â° 3 + 2 (Different syntax, same meaning).
Semantics:
Defines the facts or meanings referred to by sentences.
Example: Division in Python 2 (3/2) vs. Python 3 (3/2) (Same syntax, different semantics).
Historical Context
Logic:
One of the oldest KR languages in AI.
Foundation for systems like:
Rule-based expert systems.
Prolog programming language.
Limitations of Search Algorithms
Generate and evaluate successors but lack an understanding of the domain's context.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 12 ---

KNOWLEDGE REPRESENTATION: 
FIRST-ORDER LOGIC (FOL)
Why FOL?
FOL introduces objects, properties, and relations for complex reasoning.
Key Features of FOL:
Objects: Entities (e.g., people, numbers, cars).
Relations: Describe interactions (e.g., IsRed(x), ParentOf(x, y)).
Functions: Map objects to other objects (e.g., FatherOf(x)).
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 13 ---

FIRST-ORDER LOGIC (FOL) 
SYNTAX OF FOL
Basic Components:
Constants: Specific objects (e.g., John, PSAU). a.
Variables: General placeholders (e.g., x, y). b.
Functions: Map objects to other objects (e.g., FatherOf(John)). c.
Predicates: Return True/False for relations (e.g., Greater(x, y)). d.
Logical Connectives: ‚àß , ‚à® , ¬¨, ‚Üí, ‚Üî. e.
Quantifiers: f.
Universal ( ‚àÄ ): "For all" (e.g., ‚àÄ x Smart(x)).
Existential ( ‚àÉ ): "There exists" (e.g., ‚àÉ x Knows(x, Math)).
FOL Translation
Translating natural language to FOL:
"Everybody loves Ali" ‚Üí ‚àÄ x Loves(x, Ali).
"Someone at Tuwaiq is smart" ‚Üí ‚àÉ x (At(x, Tuwaiq) ‚àß  Smart(x)).
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 14 ---

FIRST-ORDER LOGIC (FOL) 
FOL INFERENCE METHODS
Universal Instantiation (UI):
Substitutes universally quantified variables with specific constants.
Example: ‚àÄ x Parent(x)‚ÜíCaring(x). Substitute {x = Sara}, yielding Parent(Sarah) ‚Üí Caring(Sarah)
Existential Instantiation (EI):
Replaces existential variables with a new constant.
Example: ‚àÉ x Crown(x) ‚Üí Crown(C1), where C1 is a new constant.
Propositionalization:
Converts FOL into propositional logic by grounding sentences.
Unification:
Finds substitutions to make two predicates identical.
Example: UNIFY(Knows(John, x), Knows(John, Jane)) ‚Üí {x/Jane}.
Forward and Backward Chaining:
Forward Chaining: Start with data, derive conclusions.
Backward Chaining: Start with a goal, trace back to data.
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 15 ---

¬¨Brother(LeftLeg(R), J)
Brother(R, J) ‚àß  Brother(J, R) 
King(R) ‚à®  King(J)
 ¬¨ King(R) ‚áí  King(J)
FIRST-ORDER LOGIC (FOL) 
FOL INFERENCE METHODS
Poole & Mackworth, 2010 (http://artint.info/slides/), and Russell & Norvig, 2005 (http://aima.cs.berkeley.edu/)

--- Page 16 ---

AI Vs.
Augmented
Intelligence

--- Page 17 ---

Definition of AI: Artificial Intelligence (AI) refers to the simulation of human intelligence processes
by computer systems.
Key Components:
Utilizes algorithms and data.
Enables machines to perform tasks typically requiring human intelligence.
Capabilities of AI:
Learning.
Reasoning.
Problem-solving.
Decision-making.WHAT IS ARTIFICIAL INTELLIGENCE? 
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 18 ---

Impact of the Internet:
Revolutionized connectivity.
Provided faster access to a vast amount of information.
Role of Distributed Computing:
Scales data processing.
Enhances efficiency in handling large datasets.
Influence of IoT (Internet of Things):
Proliferates connected devices.
Generates massive amounts of data.
Effect of Social Networking:
Encourages unstructured data creation by users.
Collective Impact:
Reshapes the digital landscape.
Accelerates access to information and drives innovation.
THE DIGITAL REVOLUTION  
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 19 ---

WHAT AUGMENTED INTELLIGENCE DOES:
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 20 ---

HOW DO WE DEFINE INNATE INTELLIGENCE?
Human beings have innate intelligence, which is defined as the intelligence that governs every
activity in our body. 
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 21 ---

HOW AI LEARNS ? 
AI learns from the intelligence and instructions we
provide.
Machines analyze examples to create models that link
inputs to desired outputs
.
Learning Methods:
Supervised Learning: Learning from labeled data
where inputs and outputs are provided.
Unsupervised Learning: Finding patterns in data
without labeled outputs.
Reinforcement Learning: Learning through trial and
error to achieve a goal, guided by rewards and
penalties.
Key Idea: Machines only learn based on the methods
and information we supply.
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 22 ---

TYPES OF AI:
Weak AI Strong AI Super AI
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 23 ---

THREE FORMS OF INTELLIGENCE IN EVERYDAY LIFE
Human
IntelligenceArtificial
IntelligenceAugmented
Intelligence
Operates the vehicle
(steering, mirrors).
Involves decision-
making and manual
control.Self-driving features
manage tasks like
lane keeping, speed
control, and safe
following distance.
Replaces human
input entirely for
specific tasks.Driver-assist features
like collision
detection and blind
spot warnings.
Enhances human
abilities by working
alongside us.
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 24 ---

Artificial
IntelligenceTHREE FORMS OF INTELLIGENCE IN EVERYDAY LIFE
Augmented
Intelligence
Performs tasks requiring human
intelligence.
Examples: Reasoning, problem-
solving, natural communication.
Fully replaces human involvement
in tasks.Combines human and machine
efforts to improve performance.
Examples: Voice navigation,
collision avoidance, screen
readers.
Complements human abilities
rather than replacing them.VS
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 25 ---

By combining these strengths in augmented intelligence, humans and machines achieve optimal results,
enabling us to thrive in a rapidly evolving world.Aspect Machines Humans
Data ProcessingIngest large amounts of data quickly and
tirelessly.Generalize concepts from limited data.
AccuracyReliable in repetitive tasks with no
margin for error.Prone to mistakes in repetitive tasks.
Creativity Limited to programmed algorithms.Generate new ideas and solve complex
problems.
Emotional Intelligence None. Understand and respond to emotions.
Best Use Repetitive, data-heavy tasks. Customer service, caregiving, innovation.WHICH ONE IS BETTER ..!
Strengths of Machines vs. Humans
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 26 ---

INTORUDCING GENERATIVE AI AND ITS APPLICATION
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 27 ---

TRADITIONAL AI: HOW IT WORKED
Key Components:
Repository: Stores historical data (tables,
images, documents).1.
Analytics Platform: Tools like SPSS Modeler or
Watson Studio to analyze data and build
models.2.
Application Layer: Uses insights from models to
take action (e.g., prevent customer churn).3.
Feedback Loop:
Automates learning from successes and
mistakes.
Continuously improves model accuracy over
time.
Example:
Telco predicts customer churn, takes action to
retain customers, and learns from results.GENERATIVE AI: HOW IT WORKED
What‚Äôs Different:
Massive Data: Uses vast, global information,
not limited to an organization‚Äôs repository.
Large Language Models (LLMs):
Built on diverse, expansive data.
General-purpose but powerful.
Challenges with LLMs:
Lack specific organizational context.
Solution:
Prompting & Tuning: Fine-tune general
models for specific business needs (e.g., telco
customer churn).
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 28 ---

Aspect Traditional AI Generative AI
Data Source Organization-specific repository.Global, diverse datasets ("data from
earth").
Model TypePredictive models tailored to specific
tasks.Large language models with broad
knowledge.
CustomizationRequires in-house analytics and
feedback loops.Fine-tuned using prompting and tuning
layers.
Scalability Limited by organizational data. Handles massive quantities of data.KEY DIFFERENCES: TRADITIONAL AI VS. GENERATIVE AI
Generative AI reshapes the AI landscape by leveraging expansive data and powerful models, enabling
businesses to achieve more with tailored solutions.
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 29 ---

Artificial Intelligence 
Are we there yet ?

--- Page 30 ---

(aspirational, not achieved).ARTIFICIAL INTELLIGENCE  | ARE WE THERE YET ?
‚ÄúOne definition of AI is basically the simulation of intelligent behavior in computers.‚Äù
Artificial Intelligence.
‚ÄúAGI describes an AI system equivalent to a human, as diverse as a human, and can do anything a
human can do at least as well or better.‚Äù
Artificial General Intelligence.
2 3  5   1
X 6 10 1 
‚ÄúIf I tell you
the answer to
this math
equation in my
head, is that
intelligence?‚Äù‚ÄúKnowing all the
elements in the
periodic table,
their atomic
numbers‚Äîis that
intelligence?‚ÄùChess grandmasters
invest years in
learning patterns,
moves, and
strategies. Is that
intelligence?‚ÄùA computer passes
the Turing test if a
human cannot
distinguish it from
another person
during
communication, Is
that intelligence?‚Äù‚ÄúI talk with
ChatGPT daily,
and sometimes I
could be
convinced it‚Äôs a
real person, Is
that
intelligence?‚Äù‚ÄúI have a self-
driving car that
brings me to
work. It‚Äôs
basically a
computer that
takes me places.
Is that
intelligence?‚Äù ‚ÄúI‚Äôm not a great
photographer,
but I use AI
prompting to
generate
professional
pictures.‚Äù.  Is
that
intelligence?‚Äù 
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 31 ---

DISCUSSION QUESTIONS
Do you think tasks like math calculation or
memorization qualify as intelligence? Why or
why not?
How would you define intelligence in humans
versus machines?
How close do you think we are to achieving
AGI?
What would be the potential benefits and risks
of AGI?

--- Page 32 ---

ARTIFICIAL INTELLIGENCE  | ARE WE THERE YET ?
 Applied 
AI
2 3  5   1
X 6 10 1 
‚ÄúIf I tell you
the answer to
this math
equation in my
head, is that
intelligence?‚Äù‚ÄúKnowing all the
elements in the
periodic table,
their atomic
numbers‚Äîis that
intelligence?‚ÄùChess grandmasters
invest years in
learning patterns,
moves, and
strategies. Is that
intelligence?‚ÄùA computer passes
the Turing test if a
human cannot
distinguish it from
another person
during
communication, Is
that intelligence?‚Äù‚ÄúI talk with
ChatGPT daily,
and sometimes I
could be
convinced it‚Äôs a
real person, Is
that
intelligence?‚Äù‚ÄúI have a self-
driving car that
brings me to
work. It‚Äôs
basically a
computer that
takes me places.
Is that
intelligence?‚Äù ‚ÄúI‚Äôm not a great
photographer,
but I use AI
prompting to
generate
professional
pictures.‚Äù.  Is
that
intelligence?‚Äù Generative
 AIconversational
AIgoal yet to
be fully
achieveddatabase
lookupSpecialized
narrow AIbasic
computational
ability
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 33 ---

AI IN DAILY LIFE
Statista: Digital voice assistants increased from 4.2B
to 8.4B in 5 years.
AI enhances:
Personalized experiences.
Task automa tion and efficiency.
Accessibility and convenience.
Safety and security.
Navigation and healthcare.
Efficiency and Convenience:
Automating Daily Tasks
Key Points:
Virtual Assistants: Siri, Alexa, Google Assistant.
Manage reminders, answer questions, control
devices.
Smart Homes:
AI automates temperature, lighting, and security for
convenience and energy efficiency.
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 34 ---

AI IN DAILY LIFE
Personalized Experiences:
Tailored to Your Preferences
Key Points:
Streaming Platforms (Netflix, Spotify): Recommend movies, music, etc.
Social Media (Facebook, Instagram): Personalized content feed.
E-Commerce (Amazon, eBay): Suggest products based on browsing history.
Enhanced Security:
Boosting Safety with AI
Key Points:
Cybersecurity: Encryption, access control, risk assessment.
Fraud Detection: Identifies unusual activities in transactions.
Biometric Authentication: Facial recognition, fingerprint, voice ID.
Video Analytics: Improves surveillance and fraud prevention.
https://www.coursera.org/learn/introduction-to-ai/home/module/1

--- Page 35 ---

WHAT ARE AI CHATBOTS AND SMART ASSISTANTS?
Key Points:
AI-driven programs that understand queries, provide information, and perform tasks.
Use Natural Language Processing (NLP) to simulate human-like conversations.
Evolution:
From rule-based systems to AI-powered assistants.
Generative AI chatbots: Context-based conversations and personalized recommendations.
Examples of Chatbots and Assistants
Key Points:
Chatbot Platforms: IBM Watson Assistant, Chatfuel, wit.ai.
Smart Assistants: Siri, Google Assistant, Alexa, Cortana.
Generative AI Chatbots: ChatGPT, Google Gemini, LLaMA 
https://www.coursera.org/learn/introduction-to-ai/home/module/1

### 05

--- Page 1 ---

Semester One | Course OneIntroduction to Artificial Intelligence and Applications 
Unit 2 : AI Concepts,
Terminology and
Application Domains

--- Page 2 ---

Recap From Unit 1
In the early days of AI, the top-down approach to creating intelligent systems. The idea was to extract the
knowledge from people into some machine-readable form, and then use it to automatically solve problems.
This approach was based on two big ideas:
It is important to differentiate knowledge from information or data.
‚úÖ
‚úÖ Knowledge is something which is contained in our head and represents our understanding of the world. 
 It is obtained by an active learning process, which integrates pieces of information that we receive into
our active model of the world.Knowledge
RepresentationReasoning

--- Page 3 ---

Recap From Unit 1
It is important to differentiate knowledge from information or data.
Data is something represented in physical media, such as written
text or spoken words. Data exists independently of human beings
and can be passed between people.
Information is how we interpret data in our head. For example,
when we hear the word computer, we have some understanding
of what it is.
Knowledge is information being integrated into our world model.
For example, once we learn what a computer is, we start having
some ideas about how it works, how much it costs, and what it
can be used for. This network of interrelated concepts forms our
knowledge.
Wisdom is yet one more level of our understanding of the world,
and it represents meta-knowledge, eg. some notion on how and
when the knowledge should be used.Image from Wikipedia, By Longlivetheux - Own work, CC BY-SA 4.0

--- Page 4 ---

Expert Systems

--- Page 5 ---

Expert Systems
One of the early successes of AI were so-called expert systems 
Computer systems that were designed to act as an expert in some limited problem domain. 
They were based on a knowledge base extracted from one or more human experts, and they
contained an inference engine that performed some reasoning on top of it.
https://github.com/microsoft/AI-For-Beginners/tree/main

--- Page 6 ---

Expert Systems
Expert systems are built like the human reasoning system, which contains short-term memory and long-term
memory. Similarly, in knowledge-based systems we distinguish the following components:
Problem Memory:
Stores the current information about the problem being solved.
Example: A patient's temperature, blood pressure, or whether they have inflammation.
Known as static knowledge because it represents what we know right now‚Äîa snapshot of the problem.
Knowledge Base:
Holds long-term, general knowledge about the problem domain.
This knowledge is gathered from experts and remains the same for all consultations.
Called dynamic knowledge because it helps transition from one state of the problem to another.
Inference Engine:
Manages the entire problem-solving process.
Tasks:
Searches through the current problem state.
Asks questions to gather missing information.
Identifies and applies the correct rules to move forward.

--- Page 7 ---

Expert Systems
As an example, let's consider the following expert system of determining an animal based on its physical
characteristics:
Image by Dmitry Soshnikov. This diagram is called an
AND-OR tree, and it is a
graphical representation of a
set of production rules
Drawing a tree is useful at the
beginning of extracting
knowledge from the expert.
To represent the knowledge
inside the computer it is
more
rules.convenient to use

--- Page 8 ---

Expert Systems
You can notice that each condition on the of the
rule and the action are essentially object-
attribute-value (OAV) triplets. 
IF the animal eats meat
OR (animal has sharp teeth
 
 
eyes ) THEN the animal is a
carnivoreAND animal has claws
AND animal has forward-lookingWorking memory contains the set of OAV triplets
that correspond to the problem currently being
solved. A rules engine looks for rules for which a
condition is satisfied and applies them, adding
another triplet to the working memory.

--- Page 9 ---

Expert Systems
IF the animal eats meat
OR (animal has sharp teeth
 
 
eyes ) THEN the animal is a
carnivoreAND animal has claws
AND animal has forward-lookingConditions:
1.Animal - Eats - Meat
2.(If the animal eats meat)
3.Animal - Has Teeth - Sharp
4.(If the animal has sharp teeth)
5.Animal - Has Claws - True
6.(If the animal has claws)
7.Animal - Has Eyes - Forward-Looking
8.(If the animal has forward-looking eyes)
Action:
1.Animal - Is - Carnivore
2.(Then the animal is a carnivore)
These triplets form the basis for reasoning in the working memory, where conditions are evaluated,
and new knowledge (e.g., "Animal - Is - Carnivore") is added when rules are satisfied.

--- Page 10 ---

Forward vs. Backward Inference
Forward Inference (Data-Driven Reasoning)
Starts with what we already know (initial data in working
memory).
Works step by step to add new knowledge until we reach
the answer.
Steps:
1.Check if the goal is already known: If yes, stop and return
the result.
2.Find rules to apply: Look for rules whose conditions match
the current data (this creates a "conflict set").
3.Choose a rule to apply (Conflict Resolution):
Pick the first matching rule.
Choose randomly.
Pick the rule with the most specific conditions.
4.Apply the rule: Use the rule to add new knowledge to the
working memory.
5.Repeat: Go back to step 1 until the goal is reached.
Example: Diagnosing a patient based on existing lab results.Backward Inference (Goal-Driven Reasoning)
Starts with a specific goal (e.g., "What is the
diagnosis?").
Asks targeted questions to find the information needed
to reach the goal.
Steps:
1.Identify rules for the goal: Find rules that have the
desired answer on the Right-Hand Side (RHS).
2.If no rules exist, ask for missing data: If no rules cover
the goal, ask the user for the information.
3.Test one rule as a hypothesis: Try to prove the rule by
checking its conditions (Left-Hand Side, LHS).
4.Repeat for sub-goals: If the rule's conditions depend on
other unknowns, repeat the process for those sub-goals.
5.If a rule fails: Try another rule from step 3.
Example: In medical diagnosis, asking for specific tests
or symptoms only when needed to narrow down the
cause.

--- Page 11 ---

Implementing Expert Systems
Expert systems can be implemented using different tools:
Programming them directly in some high level programming language. This is not the best idea, because the main advantage
of a knowledge-based system is that knowledge is separated from inference, and potentially a problem domain expert should
be able to write rules without understanding the details of the inference process
Using expert systems shell, i.e. a system specifically designed to be populated by knowledge using some knowledge
representation language.
Let us implement some code

--- Page 12 ---

Ontologies and the Semantic Web
Background:
By the end of the 20th century, there was a push to improve how we find information on the Internet.
The idea: 
Use knowledge representation to annotate web resources so users can make very specific queries.
This idea became known as the Semantic Web.
Smart Knowledge Representation
A way to describe knowledge using Description Logics (DL), which provide a formal, structured, and
logical framework for understanding data.
Helps in organizing information into hierarchies and assigning properties to objects (like categories and
subcategories).
Beyond organizing, DL allows systems to reason or infer new facts automatically.
Ontology about transportation:
"All cars are vehicles."
"Toyota Camry is a car."
The system infers: "Toyota Camry is a vehicle."

--- Page 13 ---

Ontologies and the Semantic Web
Is it usable today?
Yes. Modern AI and search engines use similar techniques to infer relationships and provide better
recommendations or search results. 
Google uses ontologies to link related search terms (e.g., searching "Canine" may show results for "Dog").
Distributed Knowledge
Knowledge is distributed across multiple systems or websites, but concepts are uniquely identified using global
URIs (Uniform Resource Identifiers).
A "Dog" ontology in one database and a "Veterinary Care" ontology in another can link information seamlessly.
Makes data interoperable (easily shared and understood across systems).
Enables linking knowledge from diverse domains‚Äîbiology, transportation, healthcare, etc.‚Äîon the global web.

--- Page 14 ---

Ontologies and the Semantic Web
Special XML-Based Languages
Tools to describe, structure, and share knowledge in a machine-readable format:
RDF (Resource Description Framework): A standard way to represent information about resources.
Example: Describe "Paris" with RDF:
"Paris is a city. It is in France."
RDFS (RDF Schema): Adds hierarchies and relationships.
Example: Define "City" as a subclass of "Location."
OWL (Ontology Web Language): More expressive, allowing for rules and reasoning.
Example: Define "City" and infer, "If Paris is a city, it is also a location."
Is it usable today?
Yes. These languages are actively used in:
Knowledge Graphs: Google, Microsoft, and Amazon build these to link data intelligently.
AI and Research: Fields like healthcare use OWL to create ontologies for diseases and treatments.

--- Page 15 ---

Core Concept: Ontology
What is an Ontology?
A formal specification of a domain (a specific topic or area).
Organizes knowledge as:
A hierarchy of objects (simple version).
Rules and relationships for inference (advanced version).
Purpose of Ontologies in the Semantic Web:
Help machines understand the meaning of data.
Example: In an ontology about animals:
Define "Dog" as a type of "Mammal."
Add rules: "Mammals have hair" ‚Üí Automatically know "Dogs have hair."
Why is This Important?
Enables the web to move beyond simple keyword searches to understanding user queries.
Makes the web smarter, linking knowledge across sites.
Supports applications like intelligent search engines, AI assistants, and data integration tools.

--- Page 16 ---

Core Concept: Ontology
Real-World Applications of Ontologies in Food and Nutrition:
Health Apps:
An ontology can link food items to their nutritional content.
Query: "Which foods are rich in Vitamin C?"
The system uses the ontology to find and suggest items like oranges, apples, and strawberries.
E-commerce:
When you search for "Fruits" on a grocery website, it can automatically group apples, bananas, and other
fruits without needing manual categorization.

--- Page 17 ---

Let‚Äôs Code !!

--- Page 18 ---

Practise Quiz !! 
https://forms.gle/wSSmQejDyq5G9dae7

### 06

--- Page 1 ---

Probabilistic
Reasoning

--- Page 2 ---

Bridging Knowledge-Based
Systems and Real-World Data

--- Page 3 ---

Rule-Based Systems Vs. Real World
Expert systems and Ontologies
Rule-based
Structured approaches 
Search Algorithms (Classical & Adversarial) 
Rely on deterministic or game-based logic.
What about Real-world AI ?

--- Page 4 ---

Rule-Based Systems Vs. Real World
Com plex, Noisy Data
Real-world environments (sensors, user inputs, big data) introduce noise or errors.
Exam ple: A self-driving car‚Äôs sensors can be obstructed by weather, leading to
uncertain measurements.
Incom plete Inform ation
Agents rarely have full knowledge of the world state.
Exam ple: A medical diagnosis system does not always have results for every possible
test.

--- Page 5 ---

Rule-Based Systems Vs. Real World
Uncertain Outcomes
Dynamic & Changing Environments
The world is not static: conditions and user behaviors evolve over time
Think e-commerce trends or climate changesDecisions often need to be made with partial data (e.g., is this patient likely to have a
disease?).

--- Page 6 ---

Rule-Based Systems Vs. Real World

--- Page 7 ---

Rule-Based Systems Vs. Real World

--- Page 8 ---

Rule-Based Systems Vs. Real World

--- Page 9 ---

AI‚Äôs Shift from Rules to Data

--- Page 10 ---

AI‚Äôs Shift from Rules to Data
Real-world AI often encounters imperfect or incomplete information
Foundations for Modern AI:
Almost all modern AI applications (computer vision, NLP, recommender systems, etc.)
rely on machine learning to make predictions or decisions under uncertainty.
Machine Learning is the dominant approach‚Äîalgorithms learn patterns from data
rather than relying solely on hand-coded rules.

--- Page 11 ---

AI‚Äôs Shift from Rules to Data
Uncertainty as a Core Challenge:
Almost all modern AI fields‚Äîlike computer vision, natural language processing, and
recommender systems‚Äîmust deal with uncertain or ambiguous inputs.
Examples of AI Under Uncertainty:
Computer Vision: A system classifying images in low light or occluded scenes must
account for noise and partial views.
Natural Language Processing: Speech recognition deals with background noise and
variations in accents.
Recommender Systems: Predictions rely on partial user data, uncertain user
preferences, and shifting trends.

--- Page 12 ---

From Data to Decisions

--- Page 13 ---

From Data to Decisions
Machine learning uses statistical methods
(including Bayesian techniques) to cope with
incomplete or imperfect data.
Probabilistic models (e.g., Naive Bayes,
Hidden Markov Models, Bayesian Networks)
are used for classification, time-series
prediction, and more. Example on Konwledge representation using Bayesian networks

--- Page 14 ---

Why Probabilistic Models?
Inherent Uncertainty: Sensors, user inputs,
or natural phenomena (like weather) can
produce ambiguous data.
Adaptive Learning: Probabilistic models can
update their estimates whenever new
evidence appears, making them more robust
to fluctuations in data quality.
Decision-Making Under Risk: These
methods compute the probability of various
outcomes and let a system select the action
that maximizes some measure of success,
despite uncertainty.Example on Konwledge representation using Bayesian networks

--- Page 15 ---

Why This Matters?
Robustness: Even with noisy, incomplete, or evolving data (like constantly changing
spam strategies), the Naive Bayes approach remains effective by accumulating
evidence across many features.
Scalability: Probabilistic methods can be extended or combined with other
techniques (e.g., advanced NLP tools) as data grows.
Continuous Learning: Because these methods are rooted in probability, they offer
a natural mechanism for updating beliefs when the environment or data distribution
shifts.

--- Page 16 ---

AI Uncertainty

--- Page 17 ---

Uncertainty
Suppose A and B are two statements, If we implement if-then rule to these
statements, we might write A‚ÜíB, Which means if A is true Then B is true, or if A is
false then B is false.
But consider a situation where we are not sure about whether A is true or false >>
So we cannot express this statement, this situation is called uncertainty.
So to represent uncertain knowledge, where we are not sure about the predicates,
>>> we need uncertain reasoning or probabilistic reasoning.

--- Page 18 ---

Causes of uncertainty
1. Information occurred from unreliable sources.
2. Experimental Errors
3. Equipment fault
4. Temperature variation
5. Climate change.

--- Page 19 ---

Causes of uncertainty
https://www.the-waves.org/2023/11/06/ai-uncertainty-from-technology-to-liability/

--- Page 20 ---

Probabilistic
Reasoning

--- Page 21 ---

Probabilistic Reasoning
Probabilistic reasoning is a way of knowledge representation where we apply the
concept of probability to indicate the uncertainty in knowledge. 
In probabilistic reasoning, we combine probability theory with logic to handle the
uncertainty.
 Need of probabilistic reasoning in AI
When there are unpredictable outcomes.
When specifications or possibilities of predicates becomes too large to handle.
When an unknown error occurs during an experiment.

--- Page 22 ---

Probabilistic Reasoning
In probabilistic reasoning, there are two ways to solve problems with uncertain
knowledge:
Bayes' rule
Bayesian Statistics
 Bayes' rule

--- Page 23 ---

Probability

--- Page 24 ---

A

--- Page 25 ---

P(A)
where P(A) is the probability of an event A.

--- Page 26 ---

0 ‚â§ P(A) ‚â§ 1

--- Page 27 ---

P(A) = 0
indicates total uncertainty in an event A.

--- Page 28 ---

P(A) = 1
 indicates total certainty in an event A.

--- Page 29 ---

P(Ai) = 1
Sum of probabilities

--- Page 35 ---

P(Sum = 7) =
P(Sum = 12) =

--- Page 36 ---

P(Sum = 7) = 6/36 >> 1/6
P(Sum = 12) = 1/36

--- Page 37 ---

Probability
Probability can be defined as a chance that an uncertain event will occur. It is the
numerical measure of the likelihood that an event will occur.
The value of probability always remains between 0 and 1 that represent ideal
uncertainties.
1. 0 ‚â§ P(A) ‚â§ 1, where P(A) is the probability of an event A.
2. P(A) = 0, indicates total uncertainty in an event A.
3. P(A) =1, indicates total certainty in an event A.

--- Page 38 ---

P(¬¨A) = probability of a not happening event.
P(¬¨A) + P(A) = 1.We can find the probability of an uncertain event by using the below formula.Probability

--- Page 39 ---

Probabilistic Reasoning
Event: Each possible outcome of a variable is called an event. Sample space: The
collection of all possible events is called sample space. Random variables: Random
variables are used to represent the events and objects in the real world. Prior
probability: The prior probability of an event is probability computed before observing
new information. Posterior Probability: The probability that is calculated after all
evidence or information has taken into account. It is a combination of prior probability
and new information.

--- Page 40 ---

Conditional probability
Conditional probability is a probability of occurring an event when another event
has already happened.
Let's suppose, we want to calculate the event A when event B has already occurred,
Then, ‚Äúthe probability of A under the conditions of B", it can be written as:
P(A|B) = 
‚ãÄWhere P(AB)= Joint probability of A and B
P(B)= Marginal probability of B.P(A‚ãÄB) 
P(B)

--- Page 41 ---

Conditional probability
P(rain today | rain yesterday)
P(route change | traffic conditions)
P(disease | test results)

--- Page 42 ---

Venn Diagram

--- Page 43 ---

Venn Diagram Example
 In a class, there are 70% of the students who like English and 40% of the students
who likes English and mathematics, and
.... Then what is the percent of students those who like English also like
mathematics?
in other words,
What is the probability of (Students like Math if they like English)

--- Page 44 ---

Venn Diagram Example
Let, A is an event that a student likes Mathematics
B is an event that a student likes English In a class, there are 70% of the students who like English and 40% of the students
who likes English and mathematics, and
.... Then what is the percent of students those who like English also like
mathematics?

--- Page 45 ---

Venn Diagram Example
Hence, 57% are the students who like English also like Mathematics.P(A|B) = = =P(A‚ãÄB) 0.4
0.757%P(B)

--- Page 46 ---

Bayes' theorem
Bayes' Theorem is a mathematical rule that helps us figure out the probability of
something happening based on new information, given what we already know.
Is it going to rain today, I can see clouds in the sky.
You know how often it rains on any given day in your city. Let‚Äôs say it rains 20% of the time P(A)
Incorporate new evidence:You look up and see clouds. You know that when it rains, it‚Äôs cloudy 90% of the time P(B‚à£ A).
But you also know that it‚Äôs cloudy even when it doesn‚Äôt rain, say 50% of the time
P(B‚à£ notA)).
Combine the two: Bayes' Theorem combines this information to update the probability of rain
today, taking the clouds into account.

--- Page 47 ---

Bayes' theorem
Let‚Äôs break it down:P(A‚à£ B): The probability of event AA happening, given that event BB has already
happened. This is what we are trying to find.
P(B‚à£ A): The probability of event BB happening if AA is true.
P(A): The overall probability of AA happening (before considering BB).
P(B): The overall probability of BB happening.Bayes' Theorem is a mathematical rule that helps us figure out the probability of
something happening based on new information, given what we already know.
Here‚Äôs the formula:

--- Page 48 ---

Bayes' theorem can be derived using product rule and conditional probability of
event A with known event B:
As from product rule we can write:
P(A ‚ãÄ B)= P(A|B) P(B)
or
Similarly, the probability of event B with known event A:
P(A ‚ãÄ B)= P(B|A) P(A)
Equating right hand side of both the equations, we will get:Bayes' theorem

--- Page 49 ---

It shows the simple relationship between joint and conditional probabilities. Here,
P(A|B) is known as posterior, which we need to calculate, and it will be read as Probability of
hypothesis A when we have occurred an evidence B.
P(B|A) is called the likelihood, in which we consider that hypothesis is true, then we calculate the
probability of evidence.
P(A) is called the prior probability, probability of hypothesis before considering the evidence
P(B) is called marginal probability, pure probability of an evidence.
In the equation (a), in general, we can write P (B) = P(A)*P(B|Ai)Bayes' theorem
Where A1, A2, A3,........, An is a set of mutually exclusive and exhaustive events

--- Page 50 ---

Application of Bayes' theorem in
Artificial intelligence

--- Page 51 ---

Following are some applications of Bayes' theorem:
It is used to calculate the next step of the robot when the already executed step is given.
Bayes' theorem is helpful in weather forecasting.
Bayesian belief network is key computer technology for dealing with probabilistic events and to solve a
problem which has uncertainty. We can define a Bayesian network as:
It is also called a Bayes network, belief network, decision network, or Bayesian model.
Bayesian networks are probabilistic, because these networks are built from a probability distribution,
and also use probability theory for prediction and anomaly detection.
It can also be used in various tasks including prediction, anomaly detection, diagnostics, automated
insight, reasoning, time series prediction, and decision making under uncertainty."A Bayesian network is a probabilistic graphical model which represents a set of variables and their
conditional dependencies using a directed acyclic graph. "Bayes' theorem in AI

--- Page 52 ---

Bayes' theorem in AI
Bayesian Network can be used for building models from
data and experts opinions, and it consists of two parts:
A Bayesian network graph is made up of 
nodes
Arcs (directed links)Directed Acyclic Graph
Table of conditional probabilities.
The generalized form of Bayesian network that represents
and solve decision problems under uncertain knowledge is
known as an Influence diagram.

--- Page 53 ---

Bayes' theorem in AI
Alarm ‚ÄòA‚Äô ‚Äì a node, say installed in a house of a person‚ÄòX‚Äô, which rings
upon two probabilities i.e burglary ‚ÄòB‚Äô and Earthquake ‚ÄòE‚Äô, which are‚Äì
parent nodes of the alarm node.
The alarm is the parent node of two probabilities D calls ‚ÄòD‚Äô & S calls
‚ÄòS‚Äô person nodes.
Upon the instance of burglary and Earthquake, ‚ÄòD‚Äô and ‚ÄòS‚Äô call person
‚ÄòX‚Äô, respectively. 
But, there are few drawbacks in this case, as sometimes ‚ÄòD‚Äô may forget
to call the person‚ÄòX‚Äô, even after hearing the alarm, as he has a tendency
to forget things, quick. 
Similarly, ‚ÄòS‚Äô, sometimes fails to call the person‚ÄòX‚Äô, as he is only able to
hear the alarm, from a certain distance.

### 07

--- Page 1 ---

Bayes' theorem in AI | Example 1
1.Goal: Calculate P(B|A) (likelihood it will rain given there are clouds).(likelihood)
2.What we know:
P(A): How likely it is to rain in general (Prior Knowledge).
P(A|B): How likely it is to rain if there are clouds (Evidence).
3. Question:
What is the likelihood of clouds given it rains (P(B|A))? (posterior)
What We Know
1.P(A): Probability it will rain >>> 30% chance it rains today.
2.P(A|B): Probability it will rain if there are clouds. >>> 80% chance of rain.
3.P(B): Probability of clouds 50% chance of clouds on any given day.
What is P(B|A) = ? 
https://www.cecmohali.org/public/documents/cse/material/ppt/ai-ppt-3.pdf

--- Page 2 ---

Bayes' theorem in AI | Example 2 
1.Given clouds in the morning, what's the probability of rain in the afternoon?
2.80% of rainy afternoons start with cloudy mornings. 3.40% of days have
cloudy mornings. 4.10% of days have rainy afternoons.
P(rain|clouds) = P(clouds|rain) P(rain)
P(clouds) 
0.8 0.1= 0.20.4
https://www.slideshare.net/slideshow/probabilistic-reasoning-aipptx/257807391

--- Page 3 ---

Bayes' theorem in AI
Knowing
 
P(cloudy morning | rainy afternoon)
we can calculate
 P(rainy afternoon | cloudy morning)
https://www.slideshare.net/slideshow/probabilistic-reasoning-aipptx/257807391

--- Page 4 ---

Bayes' theorem in AI
Knowing
 
P(visible effect | unknown cause)
we can calculate
 P(unknown cause | visible effect)
https://www.slideshare.net/slideshow/probabilistic-reasoning-aipptx/257807391

--- Page 5 ---

Bayes' theorem in AI | Eample 3
What We Know
1.P(A): Probability it will rain >>> 20% chance it rains today.
2.P(A|B): Probability it will rain if there are clouds. >>> 70% chance of rain.
3.P(B): Probability of clouds 50% chance of clouds on any given day.
What is P(B|A) = ? 
Example Calculation
1.What we know:
P(A) (rain): 0.20
P(A|B) (rain if clouds): 0.70
P(B) (clouds): 0.50= 87.5%
Reducing Uncertainty:
Initially, the probability of clouds was 50% . After observing rain, this updates to 87.5%, making the
prediction much more confident.

--- Page 6 ---

P(Temperature-High)=30% (likelihood of high temperature on any day).
P(Rain|Clouds) = 70% (probability of rain given clouds).
P(Temperature-High ‚à£ Clouds)=20%
P(Temperature-High ‚à£ Rain)=50%
What is Bayes' theorem in AI
If it rained and the temperature is high, the probability of clouds decreases to 70%

--- Page 7 ---

Company A supplies 40% of the computers sold and is late 5% of the time.
Company B supplies 30% of the computers sold and is late 3% of the time.
Company C supplies another 30% and is late 2.5% of the time. A computer
arrives late - what is the probability that it came from Company A?Bayes' theorem | Practice Example

--- Page 8 ---

Company A supplies 40% of the computers sold and is late 5% of the time.
Company B supplies 30% of the computers sold and is late 3% of the time.
Company C supplies another 30% and is late 2.5% of the time.
A computer arrives late - what is the probability that it came from Company A?Bayes' theorem | Practice Example
P(A) = 0.4
P(B) = 0.3
P(C) = 0.3
P( Late | A) = 0.05
P( Late | B ) = 0.03
P(Late | C ) = 0.025
P( A | Late) = P(Late|A) P(A) 
________________
 P(late) 
P(late) = 
Joint Probability 
P(Late|A)*P(A) + P(Late Joint Probability 
P(Late|A)*P(A) + P(Late |B)P(B) + P(Late | C) P(C) = 0.0365
p(A | Late) = (0.05)(0.4) \ 0.0365 = 0.54 = 54%

--- Page 9 ---

A mechanic knows these facts:
If a car has engine trouble, it makes a loud noise 80% of the time.
The chance of a car having engine trouble is 0.000033.
The chance of a car making a loud noise is 2%.
What is the chance that a car making a loud noise actually has engine trouble?Bayes' theorem | Practice Example

--- Page 10 ---

A mechanic knows these facts:
If a car has engine trouble, it is likely to make a loud noise 80% of the time.
(This means P(A‚à£ B)=0.80, where A = loud noise and B = engine trouble).
The chance of a car having engine trouble is very small, only 0.000033.
(This means P(B)=0.000033).
The chance of any car making a loud noise is 2%.
(This means P(A)=0.02).
The question is:
If a car is making a loud noise, what is the chance it actually has engine trouble?
(You need to find P(B‚à£ A))Bayes' theorem | Example Simplified

--- Page 11 ---

Bayes' theorem | Example Solution
= 0.0013%

### 08

--- Page 1 ---

Introduction to 
Machine Learning

--- Page 2 ---

Where We Left Off:
AI Concepts and Knowledge Representation:
Knowledge-based Agents and Models
Ontologies and Knowledge Graphs
Rule-Based Systems and Semantic Web
Probabilistic Reasoning and Bayes' Rule
Probabilistic reasoning lays the groundwork for ML‚Äôs handling of uncertainty.
Knowledge representation inspires how features are extracted for ML models.

--- Page 3 ---

MACHINE LEARNING IS CHANGING
OUR WORLD !
Search engines learn what you want 
Recommender systems learn your taste in books, music, movies,... 
Algorithms do automatic stock trading 
Google Translate learns how to translate text 
Siri learns to nderstand speech 
DeepMind beats humans at Go 
Cars drive themselves 
Smart-watches monitor your health

--- Page 4 ---

THE WORLD OF AI
... and the connections to Machine Learning and Deep Learning
A r t i f i c i a l
I n t e l l i g e n c eM a c h i n e
L e a r n i n gD e e p
L e a r n i n gMany people are confused what these terms actually mean.
AI refers to machines trained to perform
tasks requiring "intelligence."
Originated in the 1940s with the invention of
computers.
Key early contributors: Turing and John von
Neumann.
AI encompasses fields like machine
learning, NLP, computer vision, robotics,
and more.
Often confused with ML or basic data analysis
in modern usage.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 5 ---

MACHINE LEARNING
ML focuses on mathematically well-defined,
narrow tasks.
Constructs predictive/decision models from
data rather than explicit programming.
Learning is defined as improvement in task
performance (T) based on experience (E),
measured by performance (P).
Tom Mitchell's 1998 definition highlights this
concept.
https://www.oreilly.com/library/view/java-deep-learning/9781788997454/assets/899ceaf3-c710-4675-ae99-33c76cd6ac2f.png
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 6 ---

ML VS. STATS
ML and Statistics originated in different fields but share equivalent mathematical
foundations.
ML models focus on precise predictions; statistical models emphasize pattern
interpretation and sound inference.
ML and predictive modeling in statistics address similar problems using similar tools.
Communities remain divided with inconsistent terminology causing confusion.
ML can often be viewed as nonparametric statistics combined with efficient numerical
optimization.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 7 ---

UNSUPERVISED LEARNING
Unsupervised learning involves data without labels
(no "true" output to optimize against).
Focuses on finding patterns within input data (x).
Common methods in unsupervised learning:
Dimensionality reduction (e.g., PCA, Auto
encoders) > Compress information.
Clustering: Group similar observations.
Outlier and anomaly detection.
Association rules.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 8 ---

REINFORCEMENT LEARNING
Reinforcement Learning (RL) is a general-purpose
AI framework.
Interaction with the environment involves:
Observing the state.
Receiving a reward.
Executing an action.
Goal: Maximize future rewards.
Challenges: Reward signals can be sparse, noisy,
and delayed.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 9 ---

SUPERVISED LEARNING
Supervised learning for Regression and Classification.
Predict labels (y) based on features (x) by learning patterns from
labeled data.
Key foundational concepts in supervised ML:
Types of data used for learning.
Formalizing the learning goal.
Understanding prediction models.
Quantifying predictive performance.
Defining learning algorithms.
Operationalizing the learning process. Image from
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 10 ---

SUPERVISED LEARNING
Regression:
Predicts a continuous numerical value.
Example: Predicting house prices based on
features like size, location, and number of
rooms.
Classification:
Predicts a categorical label (class) from
predefined categories.
Example: Determining if an email is "spam" or
"not spam." Image from
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 11 ---

SUPERVISED OR UNSUPERVISED LEARNING 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 12 ---

SUPERVISED OR UNSUPERVISED LEARNING 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 13 ---

SUPERVISED OR UNSUPERVISED LEARNING 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 14 ---

ML-BASICS DATA
10
5
0
‚àí5
‚àí10
‚àí10 ‚àí5 0
x5 10yUnderstand structure of tabular data in ML
Understand difference between target and
features
Understand difference between labeled
and unlabeled data
Know concept of data-generating processYou Must Understand .. 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 15 ---

IRIS DATA SET
Classify iris subspecies based on Ô¨Çower measurements.
150 iris Ô¨Çowers: 50 versicolor, 50 virginica, 50 setosa.
Sepal length / width and petal length / width in [cm].Introduced by the statistician Ronald Fisher and one of the most frequently used toy examples.
Source:https://rpubs.com/vidhividhi/irisdataeda
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 16 ---

DATA IN SUPERVISED LEARNING
The data we deal with in supervised learning usually consists of observations on different aspects of objects:
Target: the output variable / goal of prediction
Features: measurable properties that provide a concise description of the object
We assume some kind of relationship between the features and the target, in a sense that the value of the
target variable can be explained by a combination of the features.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 17 ---

ATTRIBUTE TYPES
Feature and Target Variable Types:
Numerical variables: Real-valued data (R).
Integer variables: Discrete whole numbers (Z).
Categorical variables: Defined categories (e.g., colors, types).
Binary variables: Two possible values (e.g., 0 or 1, True/False).
Tasks Based on Target Variable:
Regression: For continuous numerical targets.
Classification: For categorical or binary targets.
Handling Features:
Most learning algorithms work with numerical features.
Some algorithms, like decision trees, can handle integers and categorical features directly.
For other cases, features must be encoded into numerical formats (e.g., one-hot encoding, label encoding).
Assumption: Unless specified, features are considered numerical.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 18 ---

ENCODING FOR CATEGORICAL FEATURES
One-Hot EncodingEncoding Methods:
Dummy Encoding
The machine learning algorithm can handle redundancy in input
data (e.g., decision trees, random forests, neural networks).
You want to maintain all category distinctions without losing
any information.
You don't have constraints on the input matrix being non-
singular.
Example: For neural networks or clustering tasks where
categorical information needs complete representation.The machine learning algorithm requires non-singular input
matrices (e.g., linear regression, logistic regression).
You need to reduce redundancy in encoded features to improve
computational efficiency and avoid multicollinearity.
Example: In linear regression, removing one column prevents
singularity in the design matrix.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 19 ---

OBSERVATION LABELS
We call the entries of the target column labels. We distinguish two basic forms our data may come in:
For l a b e l e d data we have a l r e a d y observed the t a r g e t
For u n l a b e l e d data the target labels are u n k n o w n
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 20 ---

DATA-GENERATING PROCESS IN ML
Assumption: Data from a Distribution:
The observed data (D) is assumed to be generated by an underlying process characterized
by a probability distribution: 
                                                                                             P(x,y)
This distribution defines the joint behavior of input features (x) and target variables (y).
Random Variables:
x: Random variable representing input features, sampled from the input space X.
y: Random variable representing target values, sampled from the output space Y.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 21 ---

DATA-GENERATING PROCESS IN ML
True Distribution is Unknown:
The actual distribution P(x,y) is not directly observable or known.
It represents the real-world process that generates the data.
Goal of Machine Learning:
Learning involves uncovering or approximating the structure of P(x,y), or parts of it, to
make predictions or understand relationships.
This approximation is done using:
A model f(x) for predictions.
Algorithms to estimate parameters or identify patterns in P(x,y).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 22 ---

DATA-GENERATING PROCESS IN ML
Why This Matters:
The assumption of a data-generating process underpins statistical and machine learning
methods.
Models trained on observed data aim to generalize to unseen data, relying on the
assumption that the data was sampled from P(x,y)
Key Challenges:
The true distribution P(x,y) is often complex, high-dimensional, and only partially
represented in finite datasets.
Learning methods must handle this uncertainty and approximate P(x,y) effectively for the
task at hand.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 23 ---

DATA-GENERATING PROCESS IN ML
Assumption: Data is assumed to be independent and
identically distributed (i.i.d.).
All samples come from the same probability
distribution.
Each sample is independent of others.
Why This Matters:
This assumption simplifies theoretical foundations in
machine learning.
It ensures the model learns from a consistent and
unbiased data source.
Limitation:
The i.i.d. assumption may not hold in real-world
scenarios (e.g., time series or dependent data).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

### 09

--- Page 1 ---

Introduction to
Deep Learning

--- Page 2 ---

FROM  MACHINE LEARNING TO DEEP
LEARNING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Why ML Cannot Handle Large, Complex Data?
Scalability Issues: Traditional ML models struggle to process
massive datasets efficiently.
Feature Engineering Dependency: Requires manual feature
extraction, which becomes impractical with large, high-
dimensional data like images or videos.
Unstructured Data Limitation: ML models are not inherently
designed for unstructured formats (e.g., audio, text, images).
Complex Relationships: Fails to capture hierarchical or
layered patterns effectively.
https://machinelearningmastery.com/what-is-deep-learning/

--- Page 3 ---

FROM  MACHINE LEARNING TO DEEP
LEARNING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.https://levity.ai/blog/difference-machine-learning-deep-learning

--- Page 4 ---

DEEP LEARNING AS A SUBSET
A r t i f i c i a l
I n t e l l i g e n c eM a c h i n e
L e a r n i n gD e e p
L e a r n i n gDeep learning is a subÔ¨Åeld of ML based on artiÔ¨Åcial neural networks..
Deep learning itself is not new:
Neural networks have been around since the 70s.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial
Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 5 ---

FROM  MACHINE LEARNING TO DEEP
LEARNING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.‚ÄúMachine learning algorithms, inspired by the brain, based on learning multiple levels of
representation/abstraction.‚Äù
The algorithm is given an image, e.g.,
of a bird.
It begins by analyzing small, simple
features, such as edges, corners, or
textures from the input image.

--- Page 6 ---

FROM  MACHINE LEARNING TO DEEP
LEARNING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.‚ÄúMachine learning algorithms, inspired by the brain, based on learning multiple levels of
representation/abstraction.‚Äù
The algorithm combines these
simple features (e.g., edges) to
recognize slightly more complex
patterns, such as shapes or
contours.
Each layer learns to represent the
input data at a higher level of
abstraction

--- Page 7 ---

FROM  MACHINE LEARNING TO DEEP
LEARNING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.‚ÄúMachine learning algorithms, inspired by the brain, based on learning multiple levels of
representation/abstraction.‚Äù
Deeper Layers 
Further layers combine the learned
patterns to identify more significant
features, like parts of the bird (e.g.,
wings, beak, or feathers).
At this stage, the algorithm can
identify complex structures from the
input.

--- Page 8 ---

FROM  MACHINE LEARNING TO DEEP
LEARNING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.‚ÄúMachine learning algorithms, inspired by the brain, based on learning multiple levels of
representation/abstraction.‚Äù
Output 
At the highest level, the algorithm
understands the image as a whole
and determines the final classification:
"Oh, it‚Äôs a bird!"
The "magic" refers to the layers of
abstraction (neural networks) that
learn from raw data without human
intervention.

--- Page 9 ---

POSSIBLE USE-CASES
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Deep learning can be extremely valuable if the data has these properties:
It is high dimensional. 
Each single feature itself is not very informative but only a combination of them might be.
There is a large amount of training data.
This implies that for tabular data, deep learning is rarely the correct model choice.
Without extensive tuning, models like random forests or gradient boosting will outperform deep
learning most of the time.
One exception is data with categorical features with many levels.

--- Page 10 ---

POSSIBLE USE-CASE: IMAGES
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.An image is made up of tiny squares called
pixels. Each pixel has a color value (e.g.,
red, green, blue).
A 100x100 image has 10,000 pixels, and if
it‚Äôs a color image, each pixel has 3 values
(R, G, B), giving us 30,000 values to
analyze.
What is an Image ?

--- Page 11 ---

POSSIBLE USE-CASE: IMAGES
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.A single pixel alone doesn‚Äôt tell us much.
But if we look at groups of pixels, we start
seeing patterns like edges, shapes, or
textures.
Imagine zooming out from pixels in a photo
of a bird. First, you see feathers, then the
shape of wings, and finally the whole bird.
W hat is Pixel ?

--- Page 12 ---

POSSIBLE USE-CASE: IMAGES
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Input: The image is fed into a deep
learning model, like a Convolutional Neural
Network (CNN).
Early Layers: Detect simple patterns (e.g.,
edges or small shapes).
Middle Layers: Combine these patterns
into meaningful parts (e.g., wings, beak).
Final Layers: Recognize the whole object
(e.g., ‚ÄúIt‚Äôs a bird!‚Äù).
How Does Deep Learning Process Images?

--- Page 13 ---

POSSIBLE USE-CASE: IMAGES
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Image Classification with CIFAR-10
Figure:Example for image classiÔ¨Åcation (Krizhevsky, 2009)CIFAR-10 is a popular dataset used for training
and evaluating image classification models.
It contains 60,000 color images.
Each image is 32x32 pixels in size (small and
low resolution).
There are 10 object classes, such as airplanes,
cars, birds, and cats.
Each class has 6,000 images.
It is widely used for building and testing models that
classify images into one of the 10 categories

--- Page 14 ---

POSSIBLE USE-CASE: IMAGES
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Object Detection with Mask R-CNN
Mask R-CNN is a powerful framework for object
detection and instance segmentation.
Detects Objects: Identifies and locates
multiple objects in an image.
Segmentation Mask: Creates a detailed
mask (outline) for each object, showing its
exact shape and boundaries.
It combines object detection with precise instance
segmentation, making it useful for tasks that require
detailed understanding of each object in the image.
Figure:Example for object detection (He et al., 2017)

--- Page 15 ---

POSSIBLE USE-CASE: IMAGES
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Image Segmentation
Image segmentation is a process that divides an image
into multiple segments or regions, making it easier to
analyze and understand.
Partitions the image into meaningful sections, like
objects, backgrounds, or specific regions of interest.
Helps identify boundaries and shapes of objects
within the image.
Types of Segmentation:
Semantic Segmentation: Labels each pixel in the
image with a class (e.g., sky, road, car).a.
Instance Segmentation: Separates individual objects
of the same class (e.g., two cars are segmented
separately).b.
Figure:Example for image segmentation (Noh et al., 2015))

--- Page 16 ---

POSSIBLE USE-CASE: TEXTS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Texts in Deep Learning 
High Dimensional:
In text data, each word can be treated as a feature.
For example, the German language has over 300,000 words, making text highly
complex to process.
Informative:
A single word alone doesn't convey much meaning.
Context from surrounding words or sentences is crucial for understanding.
Training Data:
There is an abundance of text data available, from books, articles, social media,
etc., providing rich material for training models.
Architecture:
Recurrent Neural Networks (RNNs):
Specialized models for handling sequential data like text.
They process words in sequence and retain context from earlier words to
understand the meaning better.

--- Page 17 ---

POSSIBLE USE-CASE: TEXTS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.TEXT CLASSIFICATION
Sentiment Analysis is the application of natural language processing to systematically
identify the emotional and subjective information in texts.

--- Page 18 ---

POSSIBLE USE-CASE: TEXTS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Machine Translation
 Machine Translation (e.g. google translate) Neural machine translation exploits neural networks to predict
the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.

--- Page 19 ---

POSSIBLE USE-CASE: SPEECH
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Speech Recognition and Generation
Speech Recognition and Generation (e.g. google assistant) Neural network extracts features from audio data
for downstream tasks, e.g., to classify emotions in speech.

--- Page 20 ---

REFERENCES
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images. 
He, K., Gkioxari, G., Doll√°r, P., & Girshick, R. (2017). Mask R-CNN. 
Noh, H., Hong, S., & Han, B. (2015). Learning Deconvolution Network for Semantic Segmentation. 
Google. (n.d.). Smart speaker mit google assistant. Google.
https://assistant.google.com/intl/de_de/platforms/speakers/

--- Page 21 ---

Single Neuron

--- Page 22 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.https://www.lucentinnovation.com/blogs/technology-posts/understanding-the-perceptron

--- Page 23 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.What is a Perceptron?
In the domain of artificial intelligence and machine learning, the term "perceptron" is frequently used. The
fundamental building block of artificial neural networks, the perceptron is the most fundamental part of machine
learning and deep learning technologies.
A single-layer neural network linear or machine learning
approach called a perceptron is used to learn different
binary classifiers under supervision. 
Perceptron is a linear classifier (binary) and is a
collection of straightforward logical assertions that
combine to form a neural network, which is an array
of sophisticated logical assertions. It is employed in
supervised learning as well. Classifying the provided
input data is helpful. https://www.lucentinnovation.com/blogs/technology-posts/understanding-the-perceptron

--- Page 24 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.How Perceptrons Are Inspired by Biological Neurons? 
Inputs (Dendrites):
Biological Neurons: Dendrites receive signals from other neurons.
Perceptrons: Inputs (numbers) act like dendrites, receiving data from
other perceptrons.
Connections (Synapses):
Biological Neurons: Synapses connect dendrites to neurons and
determine signal strength.
Perceptrons: Weights represent the importance of each input, similar to
synapse strength.
Processing (Nucleus):
Biological Neurons: The nucleus processes input signals to generate an
output.
Perceptrons: A mathematical function (nucleus) processes inputs to
produce an output value.
Output (Axon):
Biological Neurons: Axons carry the output signal to other neurons.
Perceptrons: The output value serves as input for the next perceptron.https://www.lucentinnovation.com/blogs/technology-posts/understanding-the-perceptron

--- Page 25 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.How Does the Perceptron Work?
The perceptron is a simple model for solving binary classification problems (e.g., output 0 or 1).
Key Components: 1.
Inputs (x1,x2,x3 ): Data features passed into the model.
Weights (w1,w2,w3 ): Represent the importance of each input.
Threshold: A value used to decide if the output is 0 or 1 based on the weighted sum of inputs.
Working Mechanism: 2.
Compute the weighted sum:     z= w1x1+w2x2+w3x3+bias
Apply the threshold:
If   z>threshold, output = 1.
If   z‚â§threshold, output = 0.
Learning Process: 3.
Adjust Weights:
The perceptron modifies the weights during training to minimize the error between the expected and actual outputs.
Iterative Updates:
This process is repeated until the weights converge to a stable solution.
Binary Classifier: 4.
The perceptron classifies data into two categories (e.g., 0 or 1).https://www.lucentinnovation.com/blogs/technology-posts/understanding-the-perceptron

--- Page 26 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Basic Components of a Perceptron
Inputs (x1,x2,...,xm):
Represent characteristics of the data.
Can be binary values or real numbers.
Inputs are often expressed as a vector.
Weights (w1,w2,...,wn ):
Assigned to each input to determine its importance.
Initially random and updated during training.
Summation Function:
Computes the weighted sum:   z=w1 x1 +w2 x2 +...+wn xn 
Represents the dot product of the input vector and the weight vector.
Activation Function (f(z):
Adds non-linearity to the output.
Examples: Sigmoid, ReLU, Step, Tanh, Softmax.
Determines whether the perceptron activates or remains dormant.
Bias (b):
A constant added to the weighted sum to adjust the output.
Allows learning even when inputs are zero.
Output (y):
The final output of the perceptron: y=f(z+b)
Represents the prediction or decision made based on inputs.https://www.lucentinnovation.com/blogs/technology-posts/understanding-the-perceptron

--- Page 27 ---

Let‚Äôs Do
Some Math

--- Page 28 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Types of Perceptron:
The Perceptron can be categorized into two primary types: 
Single-layer Perceptron 
Multi-layer Perceptron. 
Single layer Perceptron
The single-layer Perceptron is made up of a single layer of
neurons that adds up all of the inputs and uses an activation
function to determine the output. 
It works especially well for issues that can be solved linearly,
or in which a straight line may divide the input data into two
categories. https://www.lucentinnovation.com/blogs/technology-posts/understanding-the-perceptron

--- Page 29 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Types of Perceptron:
Multi-layer Perceptron 
A multi-layer perceptron, in contrast to a single-layer
perceptron, consists of multiple layers of neurons, with one or
more hidden layers positioned between the input and output
layers. 
The model's hidden layers enable it to identify more complex
patterns in the input data, which makes it suitable for handling
problems that are not linearly separable. 
https://www.lucentinnovation.com/blogs/technology-posts/understanding-the-perceptron

--- Page 30 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Strengths of Perceptron
Versatility: 1.
Multi-layer perceptrons can solve complex, non-linear problems.
Works with both small and large datasets efficiently.
Quick Predictions: 2.
Provides fast predictions once the model is trained.
Linearly Separable Data: 3.
Effective for problems where classes can be separated by a straight line (or hyperplane).
Supervised Learning: 4.
Uses labeled data and adjusts weights to minimize errors.
Efficient Online Learning: 5.
Updates weights after analyzing each input, making it suitable for large datasets.

--- Page 31 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Limitations of Perceptron
Linearly Separable Only:
Cannot handle non-linear problems; requires advanced models like support vector machines or multi-layer
perceptrons.
Non-Convergence:
Fails to converge if data cannot be split linearly, leading to endless weight updates.
Bias-Variance Trade-off:
Increasing complexity can lead to overfitting or underfitting.
No Probabilistic Outputs:
Does not provide probabilities for predictions, limiting its use in probabilistic decision-making.
Training Dependency:
Performance heavily depends on the quality of training data.

--- Page 32 ---

SINGLE NEURON
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Applications of Perceptron
Foundation for Neural Networks:
Forms the building blocks of more advanced neural network architectures.
Educational Use:
Helps teach the basics of machine learning and neural networks.
Simple Classification Tasks:
Useful for straightforward problems with linearly separable data.
Future Potential:
Predicted to evolve with advancements in computing and technology.
Potential to impact industries like healthcare, finance, and robotics through explainable AI and intelligent systems.

--- Page 33 ---

Let‚Äôs Code
Single Neuron (Linear Unit) in Keras

### 10

--- Page 1 ---

Semester One | Course OneIntroduction to Artificial Intelligence and Applications 
Unit 3 : AI Concepts,
Terminology and
Application Domains
PART II

--- Page 2 ---

Key Concepts
in ML

--- Page 3 ---

SUPERVISED LEARNING
Supervised tasks are data situations where learning the functional relationship between inputs (features)
and output (target) is useful.
The two most basic tasks are regression and classiÔ¨Åcation, depending on whether the target is numerical
or categorical.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.REGRESSION VS CLASSIFICATION
Regression:Our observed labels ClassiÔ¨Åcation:Observations are
categorized:y‚ÜëY={C1,...,Cg}. come fromY‚ÜíR.

--- Page 4 ---

SUPERVISED LEARNING
We can distinguish two main reasons to learn this relationship:
Learning to Predict
Focus: The outcome, not the structure or interpretability of the model.
Goal: Make accurate predictions for new data.
Example: Forecasting stock prices for practical decision-making.
Value: Directly benefits by providing actionable predictions.
Learning to Explain
Focus: Gaining insights into the relationships within the data.
Goal: Understand factors and their influence on outcomes.
Example: Identifying risk factors for diseases.
Value: Used to inform scientific, medical, or social discussions rather than operational predictions.
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.PREDICT VS. EXPLAIN

--- Page 5 ---

SUPERVISED LEARNING
Predict the price for a house in a certain area
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.REGRESSION EXAMPLE: HOUSE PRICES
In this example, we might need to learn to explain so we can understand which features that influence the house price the
most. But maybe we are also looking for underpriced houses and the predictor is of direct use, too.

--- Page 6 ---

SUPERVISED LEARNING
Predict days a patient has to stay in hospital at time of admission
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.REGRESSION EXAMPLE: LENGTH-OF-STAY
Learn to predict can be immensely beneficial, also it might be good for learning to explain.

--- Page 7 ---

SUPERVISED LEARNING
Predict one of Ô¨Åve risk categories for a life insurance customer to determine the insurance premium
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.CLASSIFICATION EXAMPLE: RISK CATEGORY
Probably learn to predict, but the company might be required to explain its predictions to its customers.

--- Page 8 ---

MODELS & PARAMETERS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.A model (or hypothesis) f:X‚ÜíRg is a function that maps feature vectors to predicted target values.
Regression: g=1, where the output is a single predicted value.
Classification: g is the number of classes, and the output consists of scores or class probabilities.

--- Page 9 ---

MODELS & PARAMETERS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.WHAT IS A MODEL?
The function f is designed to capture intrinsic patterns in the data, with the assumption that these patterns hold for all data
drawn from Pxy .
Models can vary in complexity, from simple (e.g., linear models, tree stumps) to complex (e.g., deep neural networks).
There are infinite ways to construct such functions, depending on the task and data.

--- Page 10 ---

MODELS & PARAMETERS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.HYPOTHESIS SPACES
Think of a hypothesis space as a menu of all possible models that we can choose from for a specific problem.
Each model on the menu has a specific way of making predictions.
Without narrowing down the options, finding the "best model" would be impossible because the choices would be
unlimited.
Key Idea: To make this manageable, we restrict the menu to a specific type of models (like linear models, neural networks,
etc.), which is called a structural prior
Imagine each model in the hypothesis space as a recipe, and the ingredients (parameters) in the recipe determine the
specific dish (model).
Parametrization means using these ingredients (parameters) to adjust and fine-tune the recipe to make the model work
for the given data.
All the models in the hypothesis space share the same basic structure (e.g., all are linear equations or neural networks).
The specific differences between the models come from the parameters we use to define them.PARAMETRIZATION

--- Page 11 ---

MODELS & PARAMETERS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Parameters are like knobs or dials you can adjust to make the model perform
better.
In a straight-line model (like predicting trends), the slope and intercept are
parameters that decide how steep or flat the line is.
When we "set" these parameters, the model becomes fully defined and ready
to make predictions.
Instead of thinking of a hypothesis space as just a menu of models, think of it as a
collection of all possible ways you can tweak the knobs (parameters) to get the
best model for your data.
The whole point of parametrization is to help us find the best possible model
for our data within the hypothesis space by tuning the parameters.
This process is what makes machine learning models adaptable to different
problems!HYPOTHESIS SPACES
https://www.educba.com/hypothesis-in-machine-learning/

--- Page 12 ---

LEARNER
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.TRAINING PROCEDURE OF ML SUPERVISED MODEL:
Imagine we want to investigate how working conditions affect productivity of employees.
It is a regression task since the target productivity is continuous.
Data Collected: Worked minutes per week (productivity), how many people work in the same ofÔ¨Åce as the employee in question, and the
employee‚Äôs salary.Understand that a supervised learner Ô¨Åts models automatically from training data

--- Page 13 ---

LEARNER
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.TRAINING PROCEDURE OF ML SUPERVISED MODEL:
Automatically identify the fundamental functional
relation in the data that maps an object‚Äôs features to
the target.
Supervised learning means we make use of labeled data for
which we observed the outcome.
We use the labeled data to learn a model f.
Ultimately, we use our model to compute predictions for new data
whose target values are unknown.

--- Page 14 ---

LEARNER
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.WHAT IS LEARNER ? ! 
A learner (or learning algorithm/inducer) is the method used to find the best model (f) for a given dataset.
Role: Selects the best function from a prescribed hypothesis space (H) based on
the training data.
Input:
Training data (D): The dataset used to train the model.
Hyperparameters (œâ): Settings that control how the model is built or
optimized.
Output: A specific model that best fits the data.
Mapping Concept:
Learner: D+œâ ‚Üí f‚àà H
In simple terms, the learner takes the data and hyperparameter settings and
uses them to choose the best-fitting model.

--- Page 15 ---

LOSS IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.HOW TO EVALUATE MODELS? 
When training a learner, we optimize over our hypothesis space, to Ô¨Ånd the function which matches our training data best.
This means, we are looking for a function, where the predicted output per training point is as close as possible to the
observed label.
To make this precise, we need to deÔÄÅne now how we measure the difference between a
prediction and a ground truth label pointwise.

--- Page 16 ---

LOSS IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.WHAT IS LOSS?
A loss function measures how far off a model‚Äôs prediction is from
the true value for a single observation. 
It quantifies the "quality" of the prediction.
Role: 
Smaller loss means better predictions; larger loss indicates
poor predictions.
Types of Loss Functions:
L2 Loss (Squared Error):
Penalizes larger errors more heavily. 
Common in regression tasks.

--- Page 17 ---

LOSS IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.TYPES OF LOSS
A loss function measures how far off a model‚Äôs prediction is from
the true value for a single observation. 
Absolute Loss:
Treats all errors equally, regardless of size.
Mapping:
The loss function takes:
True value (y)
Predicted value f(x)
and computes a numerical value indicating error.
In simple terms, the loss function tells us how "wrong" the
model's predictions are for each observation.

--- Page 18 ---

Optimization is the process of finding the best model parameters that minimize a given loss function 
(i.e., reduce prediction errors).
It ensures the model learns patterns in the data effectively.
Gradient Descent
Gradient Descent is an iterative algorithm used to minimize the loss function by updating model parameters
step by step.
Calculate the gradient (slope) of the loss function with respect to the parameters. a.
Update parameters in the opposite direction of the gradient to reduce the loss. b.
Formula:
Œ∏: Parameters to update.
Œ∑: Learning rate (step size).
‚àáL(Œ∏): Gradient of the loss function.
OPTIMIZATION IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.WHAT DO WE MEAN BY OPTIMIZATION ?

--- Page 19 ---

Learning Rate:
The learning rate (Œ∑) is the step size used to update model
parameters during optimization.
Too small: Slow learning (takes many iterations).
Too large: Can overshoot the minimum or cause
instability.
Finding the balance is crucial for efficient training.OPTIMIZATION IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.WHAT DO WE MEAN BY OPTIMIZATION ? 
https://towardsdatascience.com/https-medium-com-dashingaditya-rakhecha-understanding-learning-rate-dd5da26bb6de

--- Page 20 ---

REGRESSION IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.https://arunp77.medium.com/regression-algorithms-29f112797724Linear Regression
Definition: Predicts a continuous outcome by finding the straight-line
relationship between input variables (features) and the output.
Where to Use: When the relationship between input and output is roughly
linear.
Example: Predicting house prices based on size, location, and number of
rooms.
Polynomial Regression
Definition: Fits a curve to the data, rather than a straight line, to capture
non-linear relationships.
Where to Use: When the data shows a curved or complex trend instead of
a straight-line relationship.
Example: Predicting the growth of plants over time when the growth rate
slows down after an initial burst.
Logistic Regression
Definition: Used to predict probabilities and classify data into categories
(e.g., yes/no, true/false).
Where to Use: For binary or multi-class classification problems.
Example: Predicting whether an email is spam (yes or no).

--- Page 21 ---

REGRESSION IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.https://arunp77.medium.com/regression-algorithms-29f112797724Ridge Regression (L2 Regularization)
Definition: Adds a penalty to prevent the model from being too sensitive to
small changes in the data.
Where to Use: When the data has many correlated variables, and you want
to avoid overfitting.
Example: Predicting demand for products with many related factors like
price, weather, and season.
Lasso Regression (L1 Regularization)
Definition: Similar to Ridge Regression but also shrinks some variables to
zero, effectively selecting only the most important features.
Where to Use: For feature selection in high-dimensional data.
Example: Identifying the most important factors in predicting a patient‚Äôs risk
of heart disease from a large set of health indicators.
Support Vector Regression (SVR)
Definition: Finds the best-fit line or curve within a certain margin of
tolerance to capture the data‚Äôs trends.
Where to Use: For complex, non-linear relationships with high-dimensional
data.
Example: Predicting stock prices based on a variety of technical indicators.

--- Page 22 ---

CLASSIFICATION IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Learn functions that assign class labels to observation / feature vectors. Each observation belongs to exactly one class. The main
difference to regression is that the target is categorical.WHAT IS CLASSIFICATION ?
BINARY AND MULTICLASS TASKS
Tasks have a Ô¨Ånite number of (unordered) classes. They can be binary or multiclass.

--- Page 23 ---

CLASSIFICATION IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.What are Decision Boundaries?
A decision boundary is a line (or surface in higher dimensions) that separates different classes in the feature space.
It helps the model decide which class a data point belongs to based on its features.
In machine learning, decision boundaries can be straight lines (linear models) or curved/complex shapes (non-linear models) depending
on the algorithm used!

--- Page 24 ---

CLASSIFICATION IN ML
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html. Logistic Regression
Decision Boundary: A straight line, as logistic regression is a linear classifier.
Behavior: Works well when the data is linearly separable (can be split using a
straight line).
Naive Bayes
Decision Boundary: Curved lines, showing how Naive Bayes considers
probabilistic assumptions about the data.
Behavior: Handles more complex relationships compared to logistic regression
but may not perfectly separate classes.
Decision Tree (rpart)
Decision Boundary: Step-like, creating rectangular regions based on feature
splits.
Behavior: Divides the space using simple rules, useful for interpretable models,
but can miss smooth transitions between classes.
Support Vector Machine (SVM)
Decision Boundary: Smooth, non-linear curves, showing SVM's ability to handle
more complex patterns.
Behavior: Creates flexible boundaries using kernels, great for datasets with
intricate class separations

### 11

--- Page 1 ---

XOR-Problem

--- Page 2 ---

XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example problem  on a single
neuron can not solve but a single
hidden layer net can

--- Page 3 ---

Suppose we have four data points:
The XOR gate (exclusive or) returns true, when an odd number of
inputs are true:
Can you learn the target function with a logistic regression model?
XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.X = {(0, 0)>, (0, 1)>, (1, 0)>, (1, 1)>}

--- Page 4 ---

Answer : CAN‚ÄôT , Why ? 
XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.XOR Problem Basics:
XOR (Exclusive OR) outputs 1 when inputs are different (e.g., 0,1 or 1,0) and 0
when inputs are the same (e.g., 0,0 or 1,1).
XOR is not linearly separable, meaning a straight line cannot separate the output
classes.
Why Logistic Regression Fails:
Logistic regression works by finding a straight line (or hyperplane) to separate
data.
Since XOR data is not linearly separable, logistic regression cannot classify it
correctly.

--- Page 5 ---

Consider the following model:XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.
Figure: A neural network with two neurons in the hidden layer.
The matrix W describes the mapping from x to z. The vector u from z to y.

--- Page 6 ---

Let‚Äôs solve XOR using a perceptron model with ReLU and step activation functions.XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.ReLU Activation Function:
Thresholding (Step Function):

--- Page 7 ---

Let‚Äôs solve XOR using a perceptron model with ReLU and step activation functions.XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Architecture of the Model:
Model Equation:
Input Layer:
The input, x, is a 2D vector representing the XOR input values (x1,x2 ).
Weights (W):
A weight matrix (W) is applied to the inputs to transform them into a new space.
This transformation helps make the data linearly separable.
Bias (b):
A bias vector is added to the weighted input to shift the transformation, improving flexibility.
ReLU Activation ( œÉ ):
The ReLU function. œÉ(z)=max(0,z) is applied to the result, introducing non-linearity.
Non-linearity is crucial for solving non-linear problems like XOR.
Output Layer:
The transformed values are further processed using:  A weight vector (u) and bias (c).
The final result is passed through a thresholding function (œÑ) that outputs 0 or 1.

--- Page 8 ---

Let‚Äôs solve XOR using a perceptron model with ReLU and step activation functions.XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Input and Weight Matrices Bias and Transformation

--- Page 9 ---

Let‚Äôs solve XOR using a perceptron model with ReLU and step activation functions.XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Applying ReLU and Step Functions Final Output Transformation

--- Page 10 ---

we learned:XOR-PROBLEM
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Non-Linearity is crucial for solving non-linear problems like XOR.
Neural networks use layers and activation functions to:
Transform data into separable spaces.
Solve problems beyond the capability of linear models.

--- Page 11 ---

Let‚Äôs Code
Implementing XOR Problem with Keras

--- Page 12 ---

Single Hidden
Layer NN

--- Page 13 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Graphical Representation of Models:
Simple functions/models (e.g., logistic regression) can be visually represented as
graphs.
This helps break down complex concepts into understandable pieces.
Building Blocks for Complex Functions:
Individual neurons act as basic units or "building blocks."
Combining neurons creates networks capable of handling complex tasks.
Representation of Complex Hypotheses:
Neural networks can model highly intricate patterns and relationships in data.
They provide access to vast "hypothesis spaces" for learning.
Learning Common Patterns Efficiently:
Neural networks enable us to focus on hypothesis spaces aligned with real-world
data.
This data-efficient learning approach mirrors common patterns found in nature
and our universe.
Example: Recognizing handwritten digits or identifying objects in images.

--- Page 14 ---

Can a single neuron perform binary classiÔ¨Åcation of these points?SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 15 ---

Can a single neuron perform binary classiÔ¨Åcation of these points?SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.As a single neuron is restricted to learning only linear
decision boundaries, its performance on the following
task is quite poor
However, the neuron can easily separate the classes if
the original features are transformed (e.g., from
Cartesian to polar coordinates):

--- Page 16 ---

Can a single neuron perform binary classiÔ¨Åcation of these points?SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Instead of classifying the data in the original representation,
we classify it in a new feature space.

--- Page 17 ---

Can a single neuron perform binary classiÔ¨Åcation of these points?SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.So, Instead of a single neuron,
we use more complex networks.

--- Page 18 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.What is Feature Engineering?
Before deep learning, domain experts manually designed features for tasks like
image recognition and speech processing.
This process is called feature engineering and is crucial for model performance.
Instead of manually designing features, deep learning models learn features
automatically from raw data.
This automation is a core strength of deep learning.Deep Learning Automates This Process:

--- Page 19 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Why is this Important?
Feeding a classifier the "right" features is critical for success.
Deep learning eliminates the dependency on manual expertise, enabling models to find
the most effective representation on their own.
Examples:
In image recognition, instead of manually extracting edges or textures, deep learning
discovers these patterns in layers.
In speech recognition, it learns phonemes and patterns directly from audio data without
requiring hand-crafted features.
Key Insight:
Deep learning replaces traditional feature engineering with automated representation
learning, improving efficiency and adaptability.

--- Page 20 ---

Single Hidden Layer NetworksSINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Single Neurons Perform Two Steps:
Structure of a Single Hidden Layer Network:
Hidden Layer:
Contains a set of neurons that process the input.
Each neuron performs the 2-step computation (affine transformation + activation).
Output Layer:
Consists of one or more neurons that produce the network's final result.
These neurons also perform the same 2-step computation.Affine Transformation: Compute a weighted sum of inputs plus a bias.
Activation: Apply a non-linear transformation to the weighted sum (e.g., ReLU, Sigmoid).

--- Page 21 ---

SINGLE HIDDEN LAYER NETWORKS: EXAMPLESINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Graphical Representation of Models:
As a single neuron is restricted to learning only linear decision boundaries, its
performance on the following task is quite poor:

--- Page 22 ---

Input to Hidden Layer:
Each input is multiplied by its respective weight (w), and a bias (b) is added.
This operation is called an affine transformation:
SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example
Hidden Neuron Example (Calculations):
For one hidden neuron:
Activation Function:
After the affine transformation, a non-linear activation function (e.g., ReLU or Sigmoid) is applied to z.
This introduces non-linearity to the network.
Output Layer:
Outputs from the hidden layer neurons are passed to the output layer, where a similar 2-step computation (affine transformation + activation)
produces the final output.

--- Page 23 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example: Try to do it !

--- Page 24 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example: Try to do it !

--- Page 25 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example: Try to do it !

--- Page 26 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example: We End Up With:

--- Page 27 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example: We End Up With:

--- Page 28 ---

00.51
‚àí6 ‚àí4 ‚àí2 0 2 4 6SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example: Applying Activation Function 
Each hidden neuron performs a non-linear                                                               the weight sum: transformation on a c t i v a t i o n

--- Page 29 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example: Applying Activation Function 
transformation on a c t i v a t i o n
Weights (u1 ,u2 ,u3 ,u4 ) and Bias (c):
These weights determine how much influence each hidden neuron has on the final output.

--- Page 30 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Example: Applying Activation Function 
Each hidden neuron performs a non-linear                                                               the weight sum: transformation on a c t i v a t i o n

--- Page 31 ---

SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Activation Function 
Sigmoid Function
Formula:
Range: (0, 1)
When to Use:
Binary classification tasks (e.g., for the output layer).
Probabilistic outputs (e.g., the probability of a class).
Pros:
Outputs are easily interpretable as probabilities.
Cons:
Can suffer from vanishing gradients in deep networks.
Slower convergence during training.

--- Page 32 ---

ReLU (Rectified Linear Unit)
Formula:
Range: [0, ‚àû)
When to Use:
Hidden layers in most neural networks (e.g., image recognition,
NLP).
General-purpose activation for intermediate layers.
Pros:
Computationally efficient.
Avoids vanishing gradient problems.
Cons:
Can suffer from dying neurons (outputs stuck at 0 for certain
weights).
SINGLE HIDDEN LAYER NN
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Activation Function

### 12

--- Page 1 ---

Example of Single
Neuron with
Activation
Function

--- Page 2 ---

Age Buys Insurance
22 0
25 0
47 1
52 0
46 1
56 1EXAMPLE OF SINGLE NEURON WITH
ACTIVATION FUNCTION
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Problem Statement: Binary Classification for Insurance
Purchase Prediction
Objective:
Given the age of a person, predict whether the person will
buy insurance or not.
Input:
A single feature: Age (numerical).
Output:
A binary label:
1: Person will buy insurance.
0: Person will not buy insurance.

--- Page 3 ---

EXAMPLE OF SINGLE NEURON WITH
ACTIVATION FUNCTION
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Let‚Äôs consider a sigmoid activation function  to 
solve the classification problem 
Euler‚Äôs Number (e): e‚âà2.71828.
The S i g m o i d function converts any input into a
range between 0 and 1, making it ideal for
probability-based outputs
https://media.licdn.com/dms/document/media/v2/D4D1FAQER6f4_6s6s6Q/feedshare-document-pdf-analyzed/B4DZQ7goaBGgAY-/0/1736165331087?e=1737590400&v=beta&t=P-RrkuoV7HpgyEMIUWq3a4JfG2_VAzhE07uZZMukB-Q

--- Page 4 ---

EXAMPLE OF SINGLE NEURON WITH
ACTIVATION FUNCTION
Two Steps to solve the problem .. and this is the baisc of NN with signle neuron !! 
STEP#1  
y = wx+b | Transformation
STEP#2 
Z = ... | Activation 
https://media.licdn.com/dms/document/media/v2/D4D1FAQER6f4_6s6s6Q/feedshare-document-pdf-analyzed/B4DZQ7goaBGgAY-/0/1736165331087?e=1737590400&v=beta&t=P-RrkuoV7HpgyEMIUWq3a4JfG2_VAzhE07uZZMukB-Q

--- Page 5 ---

EXAMPLE OF SINGLE NEURON WITH
ACTIVATION FUNCTION
Two Steps to solve the problem .. and this is the baisc of NN with signle neuron !! 
https://media.licdn.com/dms/document/media/v2/D4D1FAQER6f4_6s6s6Q/feedshare-document-pdf-analyzed/B4DZQ7goaBGgAY-/0/1736165331087?e=1737590400&v=beta&t=P-RrkuoV7HpgyEMIUWq3a4JfG2_VAzhE07uZZMukB-Q

--- Page 6 ---

EXAMPLE OF SINGLE NEURON WITH
ACTIVATION FUNCTION
Two Steps to solve the problem .. and this is the baisc of NN with signle neuron !! 
https://media.licdn.com/dms/document/media/v2/D4D1FAQER6f4_6s6s6Q/feedshare-document-pdf-analyzed/B4DZQ7goaBGgAY-/0/1736165331087?e=1737590400&v=beta&t=P-RrkuoV7HpgyEMIUWq3a4JfG2_VAzhE07uZZMukB-Q

--- Page 7 ---

Layer Networks for
Multi-Class
ClassiÔ¨Åcation

--- Page 8 ---

We have only considered regression and binary classiÔ¨Åcation problems so far.
How can we get a neural network to perform multiclass classiÔ¨Åcation?LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.What About Multi-Class Classification ? 
https://www.google.com/url?sa=i&url=https%3A%2F%2Faman.ai%2Fprimers%2Fai%2Fmulticlass-vs-multilabel-classification%2F&psig=AOvVaw0J39FfDxrr6WGkBu7bzvo3&ust=1736691083883000&source=images&cd=vfe&opi=89978449&ved=0CBcQjhxqFwoTCMDA7czs7YoDFQAAAAAdAAAAABAT

--- Page 9 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
How does it work ? 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.
The Ô¨Årst step is to add additional neurons to the
output layer. 
Each neuron in the layer will represent a
specific class 
Number of neurons in the output layer =
number of classes.
Input features:x=(x1 ,x2 ,...,xn )
Hidden neurons: Derived features from the hidden layer are represented as z=(z1 ,z2 ,...,zm ).
Weights:
Wj : Weights for connections between input and hidden layer.
Uk: Weights for connections between hidden layer and output layer.

--- Page 10 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
How deos it work ? 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.The second step is to apply a softmax activation function to the
output layer.
Softmax provides a probability distribution over multiple classes,
enabling multi-class predictions.
Converts the model's output into probabilities for each class.
Ensures all probabilities sum to 1.

--- Page 11 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Example on MULTI-CLASS CLASSIFICATION
Forward pass (Hidden: Sigmoid, Output: Softmax).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 12 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Example on MULTI-CLASS CLASSIFICATION
Forward pass (Hidden: Sigmoid, Output: Softmax).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 13 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Example on MULTI-CLASS CLASSIFICATION
Forward pass (Hidden: Sigmoid, Output: Softmax).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 14 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Example on MULTI-CLASS CLASSIFICATION
Forward pass (Hidden: Sigmoid, Output: Softmax).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 15 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Example on MULTI-CLASS CLASSIFICATION
Forward pass (Hidden: Sigmoid, Output: Softmax).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 16 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Example on MULTI-CLASS CLASSIFICATION
Forward pass (Hidden: Sigmoid, Output: Softmax).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 17 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Example on MULTI-CLASS CLASSIFICATION
Forward pass (Hidden: Sigmoid, Output: Softmax).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 18 ---

LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Example on MULTI-CLASS CLASSIFICATION
Forward pass (Hidden: Sigmoid, Output: Softmax).
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 19 ---

Revising: Squared Error:
Measures the difference between predicted and true values by squaring the error.
Works well for regression or simple problems but not ideal for classification.
Introducing: Cross-Entropy Loss:
Used in multi-class classification.
Measures how far the predicted probability is from the correct class.
Example: If the true class is 3, it compares the predicted probability for class 3 to 1 (perfect match).
Why Cross-Entropy?
It penalizes wrong predictions more effectively by focusing on the probabilities assigned to the correct class.
LAYER NETWORKS FOR MULTI-CLASS
CLASSIFICATION
Cross-Entropy Loss 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 20 ---

Let‚Äôs Code
Implementing Multi-Class Classification Problem with Keras

### 13

--- Page 1 ---

Multi-Layer Feed
forward Neural
Networks

--- Page 2 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
We will now extend the model class once
again, such that we allow an arbitrary amount l
of hidden layers.
The general term for this model class is (multi-
layer) feedforward networks (inputs are
passed through the network from left to right,
no feedback-loops are allowed)
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.

--- Page 3 ---

Non-Linear Activations
Without non-linear activations, networks can only learn linear decision boundaries.
Layer Components
Each hidden layer has:
Weight matrix (W·µ¢)
Bias (b·µ¢)
Activation (z·µ¢)
Activation Calculation
For hidden layer i, the activation is:
z‚ÅΩ‚Å∞‚Åæ = x (input data).
Chain Structure of the Network
The network can be described as
œÑ and œÜ are for the output layer.
MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks

--- Page 4 ---

Non-Linear Activations
Without non-linear activations, networks can only learn linear decision boundaries.
Layer Components
Each hidden layer has:
Weight matrix (W·µ¢)
Bias (b·µ¢)
Activation (z·µ¢)
Activation Calculation
For hidden layer i, the activation is:
z‚ÅΩ‚Å∞‚Åæ = x (input data).
Chain Structure of the Network
The network can be described as
œÑ and œÜ are for the output layer.
MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks

--- Page 5 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 6 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 7 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 8 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 9 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 10 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 11 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 12 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 13 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 14 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 15 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 16 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Feedforward Neural Networks | Example

--- Page 17 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.WHY ADD MORE LAYERS?
Multiple layers allow for the extraction of more and more abstract representations.
Each layer in a feed-forward neural network adds its own degree of non-linearity to the model.
Figure: An intuitive, geometric explanation of the exponential advantage of deeper
networks formally (Mont√∫far et al., 2014).

--- Page 18 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.DEEP NEURAL NETWORKS
Neural networks today can have hundreds of hidden layers. 
The greater the number of layers, the "deeper" the network. 
Historically DNNs were very challenging to train and not popular until the late ‚Äô00s for several reasons:
The use of sigmoid activations (e.g., logistic sigmoid and tanh) signiÔÄÅcantly slowed down training due
to a phenomenon known as ‚Äúvanishing gradients‚Äù. The introduction of the ReLU activation largely
solved this problem.
Training DNNs on CPUs was too slow to be practical. Switching over to GPUs cut down training
time by more than an order of magnitude.
When dataset sizes are small, other models (such as SVMs) and techniques (such as feature
engineering) often outperform them.

--- Page 19 ---

MULTI-LAYER FEED FORWARD NEURAL
NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.DEEP NEURAL NETWORKS
The availability of large datasets and novel architectures that are capable of handling even complex tensor-
shaped data (e.g. CNNs for image data), faster hardware, and better optimization and regularization methods
made it feasible to successfully implement deep neural networks.
An increase in depth often translates to an increase in performance on a given task. State-of-the-art neural
networks, however, are much more sophisticated than the simple architectures we have encountered so far.
The term "deep learning" encompasses all of these developments and refers to the ÔÄÅeld as a whole.
Mont√∫farr, G., Pascanu, R., Cho, K., & Bengio, Y. (2014). Linear Regions of Deep Neural Networks.

--- Page 20 ---

CONVOLUTIONAL
NEURAL
NETWORKS

--- Page 21 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.CONVOLUTIONAL NEURAL NETWORKS

--- Page 22 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.CONVOLUTIONAL NEURAL NETWORKS
What Are CNNs?
CNNs are a type of neural network designed to process
visual data (like images).
They are inspired by how the human brain processes
visual information, particularly the visual cortex.
How Do CNNs Work?
CNNs detect patterns in images, starting with simple
features (like edges or corners) and building up to more
complex features (like faces or objects).
This happens in a hierarchical way, similar to how the
brain processes vision.
Brain Analogy :
Visual Cortex Layers:
The brain processes visual inputs in stages:
Simple features: Edges and corners (early stages).
Intermediate features: Shapes or outlines.
Complex objects: Faces or objects (later stages).
CNN Mimics This Process:
Early layers of CNNs detect edges and textures (like the brain detecting edges).
Middle layers find shapes or parts of objects.
Final layers identify the whole object, like a face.

--- Page 23 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Key Features of CNNs
CNNs are made up of building blocks and components that automatically extract important features from data.
They excel in extracting spatial patterns (like shapes, textures, and positions) from input data.
Though originally designed for images, CNNs are also used in other domains like:
Natural Language Processing (NLP)
Audio Processing
Time-Series Analysis
Image Classification:
Identifying the category or label of an image (e.g., cat, dog, car).
Object Detection/Localization:
Finding and identifying objects in an image (e.g., detecting a car and its position in the image).
Semantic Segmentation:
Dividing an image into regions and labeling each region (e.g., labeling every pixel as "sky," "tree," or "road").Applications of CNNs in Computer Vision

--- Page 24 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.CNNS - WHAT FOR?
F i g u r e :All Tesla cars being produced now have full self-driving hardwareA convolutional neural network is used to map raw pixels
from a single front-facing camera directly into steering
commands. The system learns to drive in trafÔ¨Åc, on local
roads, with or without lane markings as well as on highways.
(Source: Tesla website)

--- Page 25 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.CNNS - WHAT FOR?
F i g u r e :Image Colorization is another interesting application of CNN incomputer vision (Zhang et al. (2016)). Given a grayscale photo
as the input (top row), this network solves the problem of
hallucinating a plausible color version of the photo (bottom
row, i.e. the prediction of the network).

--- Page 26 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.CNNS - WHAT FOR?
CNN for personalized medicine Examples: Tracking, diagnosis
and localization of Covid-19 patients. CNN based method
(RADLogists) for personalized Covid-19 detection: three CT
scans from a single Corona virus patient diagnosed by
RADLogists.

--- Page 27 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.CNNS - A FIRST GLIMPSE
Input Layer
Takes input data, e.g., an image, audio, or other spatial data.
Convolution Layers
Extract feature maps by applying filters to detect patterns like
edges, corners, or textures.
Pooling Layers
Reduce dimensionality of feature maps while keeping important
information.
Helps to filter meaningful features and make computations faster.
Fully Connected Layers
Connect all features from previous layers.
Combines extracted features for final decision-making.
Output Neurons
Generate final predictions.
Softmax Layer
Converts raw output values into probability scores, making it easy
to interpret predictions.

--- Page 28 ---

CONVOLUTIONAL 
OPERATION

--- Page 29 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.CONVOLUTIONAL OPERATION
FILTERS TO EXTRACT FEATURES
Filters are widely applied in Computer Vision (CV) since the 70‚Äôs.
 
One prominent example: Sobel-Filter. It detects edges in images.
F i g u r e :Sobel-Ô¨Åltered image.What Are Edges?
Edges happen where pixel intensity changes quickly (e.g., between
light and dark areas).
How to Detect Edges?
Approximate the gradient (rate of intensity change) for each pixel.
Sobel Gradient in the X-Direction:
Sobel filter approximates the gradient Gx  in the x-dimension using this
formula:Gx =Sx ‚àó A
A: Original image.
Sx : Sobel filter matrix.
‚àó: Convolution (a special operation, not standard multiplication).
Applying the Sobel filter detects edges by calculating intensity changes
across the image in specific directions (x or y).

--- Page 30 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.FILTERS TO EXTRACT FEATURES:
F i g u r e :Sobel Ô¨Åltered images. Outputs are normalized in each case.

--- Page 31 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.FILTERS TO EXTRACT FEATURES
How to represent a digital image?
Basically as an array of integers.
Digital image is just numbers organized in an array that computers can process, with each number representing how bright or
colorful a pixel is.
For simplicity, the image is converted to black and white (grayscale):
Black = 0 (low intensity).
White = 255 (high intensity).
Every pixel is assigned a value between 0 and 255, representing brightness.Width
length

--- Page 32 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.FILTERS TO EXTRACT FEATURES
S_x: enables us to to detect vertical edges!
 The Sobel Operator Sx  is used as a filter to
detect vertical edges in the image.
The filter slides (convolves) across the
image matrix.
At each position, it performs:
Element-wise multiplication between
the filter and the image region.
Summation of the results.
The output matrix highlights vertical edges
by assigning high values where edges occur.
A small section of the image matrix is
shown being processed with Sx , resulting in
a new value in the feature map.

--- Page 33 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.FILTERS TO EXTRACT FEATURES
Applying the Sobel-Operator to every location in the input yields
the feature map.Normalized feature map reveals vertical edges. 
Note the dimensional reduction compared to the dummy
image.
The filter is applied to every possible position in the
input matrix.
A new matrix (feature map) is produced where:
High values correspond to detected edges.
Low values indicate areas with no edges.
Normalization: The feature map is scaled or normalized
to better highlight the patterns.

--- Page 34 ---

CONVOLUTIONAL NEURAL NETWORKS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.FILTERS TO EXTRACT FEATURES
F i l t e r s like the Sobel operator are applied to input images in CNNs to
create feature maps, enabling the network to detect meaningful
patterns.
I n p u t : The original black-and-white image.
F i l t e r: The predefined Sobel operator Sx .
O u t p u t: The resulting feature map highlights vertical edges in the
image.
P u r p o s e: These feature maps are used as the foundation for deeper
layers of a CNN to detect more complex features (e.g., shapes, objects).
The convolution process transforms raw image data into feature maps
that are essential for tasks like object detection and classification.

### 14

--- Page 1 ---

Other Deep
Learning
Algorithms

--- Page 2 ---

OTHER DEEP LEARNING ALGORITHMS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Why Explore More Architectures?
What are Neural Networks?
Building blocks of AI designed to mimic the human brain.
Consist of layers of interconnected nodes (neurons).
Basic networks (MLPs, single-layer NNs) have limitations.
Advanced architectures solve specific problems like sequential data,
image generation, and language understanding.

--- Page 3 ---

OTHER DEEP
LEARNING
ALGORITHMS

--- Page 4 ---

OTHER DEEP LEARNING ALGORITHMS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Why Explore More Architectures?

--- Page 5 ---

OTHER DEEP LEARNING ALGORITHMS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Recurrent Neural Networks (RNNs)
P u r p o s e: Handling sequential data (e.g., time-series, language).
How it Works:
Neurons maintain a "memory" of previous inputs via loops.
Outputs depend on both current input and past computations.
Limitations:
Struggles with long-term dependencies due to vanishing
gradients.
Applications:
Text generation, speech recognition, and financial modeling.

--- Page 6 ---

OTHER DEEP LEARNING ALGORITHMS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.how a Recurrent Neural Network (RNN) works
Input Layer 
Data is fed into the network one step at a time (e.g., words in a sentence
or time-series values).
Each input at a time step is processed sequentially.
Hidden Layers 
These layers process the input and maintain a "memory" by passing
information from one time step to the next through loops (recurrent
connections).
The hidden state captures both the current input and the context from
previous time steps.
Output Layer 
The network produces an output at each time step (e.g., predicting the
next word or value).
The final output (blue) represents the network's prediction after
processing all time steps.
Recurrent Connections:
The feedback loops in the hidden layers allow the RNN to "remember"
past data, which makes it suitable for sequential data processing.

--- Page 7 ---

OTHER DEEP LEARNING ALGORITHMS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.vanishing gradient problem
The vanishing gradient problem happens when gradients (used to update neural network weights) become very
small during backpropagation. This makes it hard for the network to learn because the weights in earlier layers are
barely updated. It‚Äôs common in deep networks like RNNs when processing long sequences, as information from
earlier steps "fades away" over time.

--- Page 8 ---

Why LSTMs?
Overcome RNN's vanishing gradient problem.
Designed for learning long-term dependencies.
Key Components:
Forget Gate: Decides what to discard from the memory.
Input Gate: Updates the cell state with new information.
Output Gate: Controls the final output based on
memory.
Applications:
Language modeling, video analysis, and predictive
analytics.
OTHER DEEP LEARNING ALGORITHMS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Long Short-Term Memory (LSTM) Networks

--- Page 9 ---

Simpler Alternative to LSTMs:
Combines forget and input gates into a single gate.
Fewer parameters make it computationally efficient.
How it Works:
Update Gate: Determines how much past
information to retain.
Reset Gate: Controls how much of the past to forget.
Applications:
Real-time applications requiring quick computation.OTHER DEEP LEARNING ALGORITHMS
Gated Recurrent Units (GRUs)

--- Page 10 ---

Converts input data into a compressed representation (Encoder) and reconstructs it into the desired output (Decoder).
Encoder: Uses Convolutional Layers to extract features from input data (e.g., an image). Reduces data dimensions using pooling layers for a
compact representation.
Decoder: Uses Transposed Convolutions to upsample and reconstruct the original data. Rebuilds spatial details layer by layer to match the
original input.
Key Features:
Encoder: Captures key features in a smaller, efficient format.
Decoder: Learns how to recreate data from the compressed format.
Shared Representation: Bridges the input and output for seamless reconstruction.
Applications:
Image-to-Image Translation (e.g., colorization).
Denoising Autoencoders.
Generating high-resolution images from compressed data.OTHER DEEP LEARNING ALGORITHMS
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Encoder-Decoder Architecture (CNN-based)

--- Page 11 ---

DEEP LEARNING RESOURCES 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Resources 
Types of Neural Networks - GeeksforGeeks : GeeksforGeeks
8 Common Types of Neural Networks - Coursera : Coursera
Types of Neural Networks and their Applications - Analytics Vidhya : Analytics Vidhya
Arabic books for ML and DL : https://dlarabic.com

--- Page 12 ---

Trends in Deep
Learning Training

--- Page 13 ---

TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Agenda
Understanding Overfitting and Underfitting
Recognizing Overtraining in Deep Learning
Techniques to Prevent Overfitting
Early Stopping as a Regularization Method
Hyperparameter Tuning for Better Models
Summary and Best Practices

--- Page 14 ---

TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Overfitting
Overfitting occurs when a model learns the training data too well, including its noise and
irrelevant details, which hurts its ability to generalize to new data.
Signs of Overfitting:
High accuracy on training data but poor accuracy on validation/testing data.
Example:
A neural network memorizes specific training examples instead of learning general patterns.
Solution Approaches:
Use more training data.
Apply regularization (e.g., L2 or dropout).
Simplify the model by reducing its complexity.

--- Page 15 ---

TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Overfitting

--- Page 16 ---

TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Underfitting
Underfitting happens when a model is too simple to capture the underlying patterns in the
data, resulting in poor performance on both training and testing data.
Signs of Underfitting:
Low accuracy on both training and validation/testing data.
Example:
A shallow neural network trying to classify complex images.
Solution Approaches:
Use a more complex model.
Train for more epochs.
Reduce regularization.

--- Page 17 ---

TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Overfitting

--- Page 18 ---

TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Optimal Fitting

--- Page 19 ---

TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Putting All Together

--- Page 20 ---

Overtraining is when a model is trained for too many epochs, leading to overfitting and
decreased performance on unseen data.
How to Recognize Overtraining:
Validation accuracy starts to drop after a certain number of epochs.
Prevention:
Use validation data to monitor performance.
Apply early stopping.
TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Overtraining in Deep Learning

--- Page 21 ---

A technique where training is stopped when the model's performance on validation data no
longer improves.
How It Works:
Track validation loss during training. 1.
Stop training if loss does not decrease for a specified number of epochs (patience). 2.
Advantages:
Prevents overfitting.
Saves training time.
TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Early Stopping

--- Page 22 ---

Data Augmentation: 1.
Create new training examples by modifying existing
ones (e.g., rotating or flipping images).
Regularization: 2.
L2 Regularization: Penalizes large weights to simplify
the model.
Dropout: Randomly drops neurons during training to
prevent co-adaptation.
Cross-Validation: 3.
Split data into multiple subsets for training and
validation.
Reduce Model Complexity: 4.
Use fewer layers or neurons if the model is too
complex for the data.TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Techniques to Prevent Overfitting

--- Page 23 ---

Settings that control the learning process, such as
learning rate, batch size, and number of layers.
How to Tune Hyperparameters:
Grid Search: Test combinations of hyperparameters
systematically.
Random Search: Randomly sample hyperparameter
values.
Automated Methods: Use tools like Optuna or
Hyperband.
Find the optimal balance between underfitting and overfitting.TRENDS IN DEEP LEARNING TRAINING
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Hyperparameter Tuning

--- Page 24 ---

Assignemnt

### 15

--- Page 1 ---

Semester One | Course OneIntroduction to Artificial Intelligence and Applications 
Unit 5: Introduction to
Generative AI and
Course Review

--- Page 2 ---

Introduction To
Generative AI

--- Page 3 ---

In this week, students will do the following: 
Solve Assignment about building a Binary Classification Neural Network 
Cross presentation sessions for Introduction to Generative AI with the following topics:
Introduction to Generative AI
Understanding Generative Models
Generative Adversarial Networks (GANs)
Transformers and Large Language Models
Applications of Generative AI in Creative Industries
Future Trends in Generative AI
Generative AI for Text Generation
Image and Video Synthesis with Generative AI
The Role of Generative AI in Business and Innovation
Ethics and Challenges in Generative AI
Course review of introduction to AI course 
Summary of the Course.  INTORDUCTION TO GENERATIVE AI
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Week Agenda

--- Page 4 ---

Assignment
Diabetes Classification with a Feedforward Neural Network

--- Page 5 ---

Diabetes Classification with a Feedforward Neural Network
This notebook demonstrates how to:
Load a CSV dataset with columns such as gender, age, hypertension, heart_disease, smoking_history,
bmi, HbA1c_level, blood_glucose_level, and diabetes.1.
Perform exploratory data analysis (EDA). 2.
Preprocess the data (encoding categorical features, handling missing values, etc.). 3.
Split the data into train, validation, and test sets. 4.
Build and train a feedforward neural network (using TensorFlow Keras). 5.
Visualize the training history (loss, accuracy). 6.
Evaluate on the test set. 7.
Submission of the Assignment and Student Solution can be found here: Assignmeent Submission LinkINTORDUCTION TO GENERATIVE AI
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Assignment #2

--- Page 6 ---

Cross Presentations  
Cross Presentations activity for Preparing generative AI Slides

--- Page 7 ---

Objective:
To create a 7-10 slide presentation on a specific topic related to Generative AI. This assignment will help you
practice presentation skills, reinforce understanding of Generative AI concepts, and demonstrate your ability to
communicate technical topics effectively.
Instructions:
Form teams of two students. Each team must choose a unique topic from the list below to avoid repetition.
Coordinate with your classmates to finalize your topic.
Presentation Requirements: Length: 7-10 slides.
Content:
Clearly explain the chosen topic. 
Include definitions, key concepts, and examples.
Highlight practical applications or case studies where applicable.
Use diagrams, charts, or images to enhance understanding.
Ensure visual elements are clear and relevant.
Provide a reference section at the end of the presentation.CROSS PRESENTATIONS 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Assignment #3

--- Page 8 ---

Guidelines:
Do **not** use ChatGPT or similar AI tools to create the presentation.
Ensure that all team members understand the content of each slide.
Focus on clarity and simplicity; avoid overloading slides with text.
Upload your presentation to the following link by the end of the day: Submission LinkCROSS PRESENTATIONS 
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Assignment #3
Bonus Opportunity:
ONE GRADE BONUS will be awarded to the best three teams. Make sure your presentation is well-designed,
engaging, and you can explain it effectively!

--- Page 9 ---

Course Summary

--- Page 10 ---

COURSE SUMMARY
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Course Overview & Key Concepts
1. Foundations of AI
Intelligence & AI Systems: Definitions of intelligence, weak vs. strong AI, rational agents, PEAS framework, and environment types.
History & Philosophy: Turing‚Äôs contributions/objections, technological singularity, classical vs. modern AI.
Core Techniques:
Search algorithms (Greedy, Minimax, Alpha-Beta Pruning).
Knowledge representation (ontologies, RDF/SPARQL, first-order logic).
Probabilistic reasoning (Bayes‚Äô theorem, uncertainty, Bayesian statistics).
2. Machine Learning & Neural Networks
ML Basics: Supervised/unsupervised/reinforcement learning, regression vs. classification, hypothesis space, loss functions, gradient
descent.
Deep Learning:
Perceptrons, activation functions, feedforward networks.
CNNs (filtering, image applications), RNNs/LSTMs (sequence modeling).
Overfitting prevention (early stopping, hyperparameter tuning).
3. Generative AI
Frameworks, GANs, transformers, text/image synthesis, ethical challenges.

--- Page 11 ---

COURSE SUMMARY
Bothmann, L., Strickroth, S., Casalicchio, G., R√ºgamer, D., Lindauer, M., Scheipl, F. &amp; Bischl, B.. (2023). Developing Open Source Educational Resources for Machine Learning and Data Science. <i>Proceedings of the Third Teaching Machine Learning and Artificial Intelligence Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 207:1-6 Available from
https://proceedings.mlr.press/v207/bothmann23a.html.Practical Applications & Outcomes
1. Hands-On Skills:
Python Programming: NumPy, file I/O, loops, dictionaries, Keras/TensorFlow.
AI Implementation:
Expert systems (forward/backward inference).
Bayesian reasoning (probability problems).
Neural networks (XOR solver, diabetes classifier, multi-class CNN/RNN).
2. Key Projects & Assignments:
Turing‚Äôs objections essay.
Car price prediction (ML regression).
Diabetes classification (feedforward NN).
Generative AI team presentations (GANs, LLMs, ethics).
3. Course Outcomes:
Technical Skills: Problem-solving with search algorithms, probabilistic reasoning, ML model design, and neural network
implementation.
Critical Thinking: Evaluating AI ethics, limitations (e.g., XOR problem), and real-world applications.
Future Readiness: Exposure to generative AI trends, industry tools, and collaborative problem-solving.

--- Page 12 ---

Done
