{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Grid Search and Random Search | Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 1-4: All examples** - Data processing, regression, classification, clustering\n",
    "- âœ… **Unit 2, Example 2: Cross-Validation** - Understanding CV for model evaluation\n",
    "- âœ… **Understanding of hyperparameters**: Parameters that control model behavior (not learned from data)\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why hyperparameter tuning is needed\n",
    "- Knowing when to use Grid Search vs Random Search\n",
    "- Understanding how cross-validation is used in hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is Unit 5, Example 1** - it's about finding the best hyperparameters automatically!\n",
    "\n",
    "**Why this example FIRST in Unit 5?**\n",
    "- **Before** you can use advanced techniques, you need to tune hyperparameters properly\n",
    "- **Before** you can compare models, you need each model at its best (tuned hyperparameters)\n",
    "- **Before** you can use boosting, you need to understand hyperparameter tuning\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Unit 2, Example 2: Cross-Validation (we know how to evaluate models)\n",
    "- ğŸ““ All previous examples (we've seen hyperparameters like C, gamma, max_depth, etc.)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 2: Boosting (uses hyperparameter tuning)\n",
    "- ğŸ““ All ML projects (hyperparameter tuning is essential for best performance!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Grid Search teaches **systematic hyperparameter tuning** (tries all combinations)\n",
    "2. Random Search shows **efficient alternative** (tries random combinations, often faster)\n",
    "3. Both methods use **cross-validation** (reliable evaluation during tuning)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Finding the Best Settings | Ø§Ù„Ù‚ØµØ©: Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n",
    "\n",
    "Imagine you're tuning a radio. **Before** Grid Search, you try settings randomly (might miss the best one). **After** Grid Search, you try all combinations systematically - guaranteed to find the best settings!\n",
    "\n",
    "Same with machine learning: **Before** Grid Search, we guess hyperparameters (might not be optimal). **After** Grid Search, we try all combinations and pick the best - much better performance!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Grid Search and Random Search Matter | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ ÙˆØ§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØŸ\n",
    "\n",
    "Hyperparameter tuning is essential for model performance:\n",
    "- **Grid Search**: Tries all combinations systematically (thorough but slow)\n",
    "- **Random Search**: Tries random combinations (faster, often finds good solutions)\n",
    "- **Cross-Validation**: Uses CV during tuning for reliable evaluation\n",
    "- **Automatic**: Finds best hyperparameters without manual guessing\n",
    "- **Industry Standard**: Used in all professional ML projects\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Perform Grid Search for hyperparameter tuning\n",
    "2. Perform Random Search for hyperparameter tuning\n",
    "3. Compare Grid Search vs Random Search\n",
    "4. Use cross-validation in hyperparameter tuning\n",
    "5. Visualize hyperparameter search results\n",
    "6. Know when to use each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We've been using default hyperparameters or guessing values manually.\n",
    "\n",
    "**AFTER**: We'll use Grid Search and Random Search to automatically find the best hyperparameters!\n",
    "\n",
    "**Why this matters**: Hyperparameters dramatically affect model performance. Automatic tuning finds optimal values much better than manual guessing!\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: Why not just use default hyperparameters?**\n",
    "  - Answer: Defaults are generic (work for average cases), but your data might need different values\n",
    "  - Example: Default C=1.0 for SVM might not be optimal â†’ Grid Search finds best C for your data\n",
    "  - Tuning hyperparameters can improve accuracy by 5-20% (significant improvement!)\n",
    "- **Q: What's the difference between parameters and hyperparameters?**\n",
    "  - Answer: Parameters = learned from data (e.g., coefficients in linear regression)\n",
    "  - Hyperparameters = set before training (e.g., C, gamma, max_depth, learning_rate)\n",
    "  - Parameters are learned, hyperparameters are tuned!\n",
    "- **Q: Why use Grid Search instead of trying values manually?**\n",
    "  - Answer: Grid Search tries ALL combinations systematically â†’ guaranteed to find best (within grid)\n",
    "  - Manual = might miss optimal values, slow, error-prone\n",
    "  - Grid Search = systematic, automatic, finds best combination\n",
    "- **Q: Why is Grid Search slow?**\n",
    "  - Answer: Grid Search tries ALL combinations â†’ if you have 3 hyperparameters with 5 values each = 5Ã—5Ã—5 = 125 combinations\n",
    "  - Each combination needs cross-validation (e.g., 5-fold CV) â†’ 125 Ã— 5 = 625 model trainings!\n",
    "  - Solution: Use smaller grids or Random Search (tries random combinations, faster)\n",
    "- **Q: When should I use Grid Search vs Random Search?**\n",
    "  - Answer: Grid Search = thorough but slow (use when you have few hyperparameters or small grids)\n",
    "  - Random Search = faster, often finds good solutions (use when you have many hyperparameters or large grids)\n",
    "  - Rule of thumb: Start with Random Search, use Grid Search to fine-tune around best Random Search result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us perform hyperparameter tuning\n",
    "\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,      # For splitting data\n",
    "    GridSearchCV,         # Grid Search with cross-validation\n",
    "    RandomizedSearchCV,   # Random Search with cross-validation\n",
    "    cross_val_score       # For cross-validation scores\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier  # Model to tune\n",
    "from sklearn.svm import SVC  # Another model to tune\n",
    "from sklearn.metrics import accuracy_score, classification_report  # For evaluation\n",
    "from sklearn.datasets import make_classification  # For generating sample data\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each tool does:\")\n",
    "print(\"   - GridSearchCV: Tries ALL combinations systematically\")\n",
    "print(\"   - RandomizedSearchCV: Tries random combinations (faster)\")\n",
    "print(\"   - cross_val_score: Evaluates models using cross-validation\")\n",
    "print(\"   - make_classification: Generates sample classification data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Generating sample classification data...\n",
      "Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. Generating sample classification data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mØ¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# For reproducibility\u001b[39;00m\n\u001b[1;32m      8\u001b[0m X, y \u001b[38;5;241m=\u001b[39m make_classification(\n\u001b[1;32m      9\u001b[0m     n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     10\u001b[0m     n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# pd.DataFrame(data)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# - data: Dictionary where keys become column names, values become column data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#   - Each key-value pair: key = column name, value = list of values for that column\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# - Returns DataFrame with rows and columns\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate sample classification data\n",
    "# We'll use this to demonstrate hyperparameter tuning\n",
    "\n",
    "print(\"\\n1. Generating sample classification data...\")\n",
    "print(\"Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\")\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,  # Only 5 features are informative\n",
    "    n_redundant=2,    # 2 features are redundant\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column data\n",
    "#   - Each key-value pair: key = column name, value = list of values for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])\n",
    "df['target'] = y\n",
    "\n",
    "print(f\"\\nğŸ“Š Data Shape: {df.shape}\")\n",
    "print(f\"ğŸ“Š Target distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(\"\\nğŸ” Notice:\")\n",
    "print(\"   - 1000 samples with 10 features\")\n",
    "print(\"   - We'll tune hyperparameters to get best performance on this data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_data = df.drop('target', axis=1)\n",
    "y_data = df['target']\n",
    "# train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=42: Seed for reproducibility (same split every time)\n",
    "# - stratify=y: Maintains class distribution in train/test (for classification)\n",
    "# - Returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. Grid Search for Random Forest\")\n",
    "print(\"Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ Ù„Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define parameter grid\n",
    "# Grid Search will try ALL combinations of these values\n",
    "# Example: n_estimators=50, max_depth=3, min_samples_split=2, min_samples_leaf=1\n",
    "#          n_estimators=50, max_depth=3, min_samples_split=2, min_samples_leaf=2\n",
    "#          ... and so on for ALL combinations\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],      # Number of trees\n",
    "    'max_depth': [3, 5, 7, None],        # Maximum tree depth\n",
    "    'min_samples_split': [2, 5, 10],     # Minimum samples to split\n",
    "    'min_samples_leaf': [1, 2, 4]        # Minimum samples in leaf\n",
    "}\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in param_grid_rf.values()])\n",
    "print(f\"\\nğŸ“Š Parameter Grid:\")\n",
    "print(f\"   Total combinations: {total_combinations}\")\n",
    "print(f\"   Grid Search will try ALL {total_combinations} combinations!\")\n",
    "print(f\"   This is thorough but can be slow for large grids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (for SVM)\n",
    "scaler = StandardScaler()\n",
    "# .fit_transform(data)\n",
    "# - Two operations in one: .fit() then .transform()\n",
    "#   1. .fit(): Learns parameters from data (mean/std, categories, etc.)\n",
    "#   2. .transform(): Applies transformation using learned parameters\n",
    "# - Use on training data\n",
    "# - For test data, use only .transform() (don't refit!)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grid Search for Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "print(f\"\\nParameter Grid:\")\n",
    "print(f\"  Total combinations: {np.prod([len(v) for v in param_grid_rf.values()])}\")\n",
    "print(f\"  Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {np.prod([len(v) for v in param_grid_rf.values()])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Random Search for Random Forest\")\n",
    "print(\"Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Same parameter grid as Grid Search\n",
    "# But Random Search will try only RANDOM combinations\n",
    "param_distributions_rf = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "n_iter = 20  # Try only 20 random combinations (vs 108 for Grid Search!)\n",
    "print(f\"\\nğŸ“Š Random Search Configuration:\")\n",
    "print(f\"   Total possible combinations: {np.prod([len(v) for v in param_distributions_rf.values()])}\")\n",
    "print(f\"   Random Search will try: {n_iter} random combinations\")\n",
    "print(f\"\\n   ğŸ“ Common Student Questions:\")\n",
    "print(f\"   Q: Why is Random Search faster than Grid Search?\")\n",
    "print(f\"      Answer: Grid Search tries ALL combinations (108 in this case)\")\n",
    "print(f\"      Random Search tries only RANDOM combinations (20 in this case)\")\n",
    "print(f\"      Fewer combinations = less time = faster results\")\n",
    "print(f\"   Q: Will Random Search find the best hyperparameters?\")\n",
    "print(f\"      Answer: Random Search often finds GOOD solutions (not always best)\")\n",
    "print(f\"      Grid Search finds BEST solution (within grid), but slower\")\n",
    "print(f\"      In practice: Random Search finds 90% of best performance in 10% of time!\")\n",
    "print(f\"   Q: When should I use Grid Search vs Random Search?\")\n",
    "print(f\"      Answer: Grid Search = few hyperparameters, small grids, need best solution\")\n",
    "print(f\"      Random Search = many hyperparameters, large grids, need fast results\")\n",
    "print(f\"      Best practice: Start with Random Search, use Grid Search to fine-tune\")\n",
    "print(f\"   Much faster than Grid Search!\")\n",
    "print(f\"   Often finds good solutions with fewer tries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    rf, param_grid_rf, cv=5, scoring='accuracy',\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "print(\"\\nStarting Grid Search...\")\n",
    "print(\"Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ...\")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(f\"\\nBest Parameters:\")\n",
    "print(f\"Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:\")\n",
    "for param, value in grid_search_rf.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV Score: {grid_search_rf.best_score_:.4f}\")\n",
    "print(f\"Ø£ÙØ¶Ù„ Ø¯Ø±Ø¬Ø© CV: {grid_search_rf.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test set\n",
    "y_pred_rf = grid_search_rf.best_estimator_.predict(X_test)\n",
    "test_acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Test Accuracy: {test_acc_rf:.4f}\")\n",
    "print(f\"Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_acc_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interpretation after Grid Search results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ Interpreting Grid Search Results | ØªÙØ³ÙŠØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_cv = grid_search_rf.best_score_\n",
    "n_combinations = len(grid_search_rf.cv_results_['params'])\n",
    "\n",
    "print(f\"\\nğŸ“Š Grid Search Summary:\")\n",
    "print(f\"   - Total combinations tried: {n_combinations}\")\n",
    "print(f\"   - Best CV Score: {best_cv:.4f} ({best_cv*100:.2f}%)\")\n",
    "print(f\"   - Best Parameters: {grid_search_rf.best_params_}\")\n",
    "\n",
    "if best_cv >= 0.9:\n",
    "    quality = \"âœ… EXCELLENT\"\n",
    "elif best_cv >= 0.8:\n",
    "    quality = \"âœ… GOOD\"\n",
    "elif best_cv >= 0.7:\n",
    "    quality = \"âš ï¸  FAIR\"\n",
    "else:\n",
    "    quality = \"âš ï¸  POOR\"\n",
    "\n",
    "print(f\"   - Quality: {quality}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Cross-Validation Score:\")\n",
    "print(f\"   - CV score is average across 5 folds\")\n",
    "print(f\"   - More reliable than single train/test split\")\n",
    "print(f\"   - This score estimates model's generalization ability\")\n",
    "\n",
    "print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "print(f\"   - Grid Search tries ALL combinations (thorough but slow)\")\n",
    "print(f\"   - CV score shows expected performance on new data\")\n",
    "print(f\"   - Best parameters are those with highest CV score\")\n",
    "print(f\"   - Always test on separate test set after tuning\")\n",
    "print(f\"   - Grid Search is exhaustive - guarantees finding best in grid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Search for Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interpretation after test accuracy\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ Interpreting Test Results | ØªÙØ³ÙŠØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cv_score = grid_search_rf.best_score_\n",
    "test_acc = test_acc_rf\n",
    "gap = abs(cv_score - test_acc)\n",
    "\n",
    "print(f\"\\nğŸ“Š Performance Comparison:\")\n",
    "print(f\"   - CV Score: {cv_score:.4f} ({cv_score*100:.2f}%)\")\n",
    "print(f\"   - Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   - Difference: {gap:.4f} ({gap*100:.2f} percentage points)\")\n",
    "\n",
    "if gap < 0.01:\n",
    "    status = \"âœ… Excellent\"\n",
    "    meaning = \"CV score matches test accuracy perfectly\"\n",
    "elif gap < 0.05:\n",
    "    status = \"âœ… Good\"\n",
    "    meaning = \"CV score is close to test accuracy\"\n",
    "else:\n",
    "    status = \"âš ï¸  Warning\"\n",
    "    meaning = \"CV score differs from test accuracy\"\n",
    "\n",
    "print(f\"   - Status: {status}\")\n",
    "print(f\"   - Meaning: {meaning}\")\n",
    "\n",
    "if test_acc > cv_score:\n",
    "    print(f\"   - Test accuracy is HIGHER than CV score (good!)\")\n",
    "    print(f\"   - Model generalizes well to test data\")\n",
    "elif test_acc < cv_score:\n",
    "    print(f\"   - Test accuracy is LOWER than CV score\")\n",
    "    print(f\"   - Model may have overfitted to CV folds\")\n",
    "\n",
    "print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "print(f\"   - CV score estimates performance, test accuracy is actual performance\")\n",
    "print(f\"   - Small gap = CV is reliable predictor of test performance\")\n",
    "print(f\"   - Large gap = CV may not be reliable (overfitting or data issues)\")\n",
    "print(f\"   - Always compare CV score with test accuracy to validate tuning\")\n",
    "print(f\"   - Grid Search finds best hyperparameters based on CV performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Random Search for Random Forest\")\n",
    "print(\"Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same parameter grid\n",
    "param_distributions_rf = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Random Search (try fewer combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20  # Try only 20 random combinations\n",
    "print(f\"\\nRandom Search will try {n_iter} random combinations\")\n",
    "print(f\"Ø³ÙŠØ¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ {n_iter} ØªØ±ÙƒÙŠØ¨Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\")\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    rf, param_distributions_rf, n_iter=n_iter, cv=5,\n",
    "    scoring='accuracy', n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "print(\"\\nStarting Random Search...\")\n",
    "print(\"Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ...\")\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "print(f\"\\nBest Parameters:\")\n",
    "print(f\"Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:\")\n",
    "for param, value in random_search_rf.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV Score: {random_search_rf.best_score_:.4f}\")\n",
    "print(f\"Ø£ÙØ¶Ù„ Ø¯Ø±Ø¬Ø© CV: {random_search_rf.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test set\n",
    "y_pred_rf_rs = random_search_rf.best_estimator_.predict(X_test)\n",
    "test_acc_rf_rs = accuracy_score(y_test, y_pred_rf_rs)\n",
    "print(f\"Test Accuracy: {test_acc_rf_rs:.4f}\")\n",
    "print(f\"Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_acc_rf_rs:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Grid Search for SVM\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Grid Search for SVM\")\n",
    "print(\"Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ Ù„Ù€ SVM\")\n",
    "print(\"=\" * 60)\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "print(f\"\\nParameter Grid:\")\n",
    "print(f\"  Total combinations: {np.prod([len(v) for v in param_grid_svm.values()])}\")\n",
    "svm = SVC(random_state=42, probability=True)\n",
    "grid_search_svm = GridSearchCV(\n",
    "    svm, param_grid_svm, cv=5, scoring='accuracy',\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "print(\"\\nStarting Grid Search for SVM...\")\n",
    "grid_search_svm.fit(X_train_scaled, y_train)\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in grid_search_svm.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV Score: {grid_search_svm.best_score_:.4f}\")\n",
    "y_pred_svm = grid_search_svm.best_estimator_.predict(X_test_scaled)\n",
    "test_acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Test Accuracy: {test_acc_svm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interpretation after comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ Interpreting Method Comparison | ØªÙØ³ÙŠØ± Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø·Ø±Ù‚\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_cv_idx = comparison['Best CV Score'].idxmax()\n",
    "best_test_idx = comparison['Test Accuracy'].idxmax()\n",
    "best_method_cv = comparison.loc[best_cv_idx, 'Method']\n",
    "best_method_test = comparison.loc[best_test_idx, 'Method']\n",
    "\n",
    "print(f\"\\nğŸ“Š Best Method by CV Score: {best_method_cv}\")\n",
    "print(f\"   - CV Score: {comparison.loc[best_cv_idx, 'Best CV Score']:.4f}\")\n",
    "print(f\"   - This method has best cross-validation performance\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Best Method by Test Accuracy: {best_method_test}\")\n",
    "print(f\"   - Test Accuracy: {comparison.loc[best_test_idx, 'Test Accuracy']:.4f}\")\n",
    "print(f\"   - This method generalizes best to new data\")\n",
    "\n",
    "print(f\"\\nğŸ” Grid Search vs Random Search:\")\n",
    "grid_cv = comparison[comparison['Method'] == 'Grid Search (RF)']['Best CV Score'].values[0]\n",
    "random_cv = comparison[comparison['Method'] == 'Random Search (RF)']['Best CV Score'].values[0]\n",
    "grid_test = comparison[comparison['Method'] == 'Grid Search (RF)']['Test Accuracy'].values[0]\n",
    "random_test = comparison[comparison['Method'] == 'Random Search (RF)']['Test Accuracy'].values[0]\n",
    "\n",
    "print(f\"   - Grid Search: CV={grid_cv:.4f}, Test={grid_test:.4f}\")\n",
    "print(f\"   - Random Search: CV={random_cv:.4f}, Test={random_test:.4f}\")\n",
    "\n",
    "if grid_cv > random_cv:\n",
    "    print(f\"   - Grid Search has better CV score (tried all combinations)\")\n",
    "else:\n",
    "    print(f\"   - Random Search has better CV score (got lucky with random selection)\")\n",
    "\n",
    "if random_test > grid_test:\n",
    "    print(f\"   - Random Search generalizes better (better test accuracy)\")\n",
    "    print(f\"   - Sometimes random search finds better hyperparameters!\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Efficiency Analysis:\")\n",
    "grid_combos = comparison[comparison['Method'] == 'Grid Search (RF)']['Total Combinations Tried'].values[0]\n",
    "random_combos = comparison[comparison['Method'] == 'Random Search (RF)']['Total Combinations Tried'].values[0]\n",
    "efficiency = grid_combos / random_combos\n",
    "\n",
    "print(f\"   - Grid Search tried: {grid_combos} combinations\")\n",
    "print(f\"   - Random Search tried: {random_combos} combinations\")\n",
    "print(f\"   - Random Search is {efficiency:.1f}x faster (tried {efficiency:.1f}x fewer combinations)\")\n",
    "\n",
    "print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "print(f\"   - Grid Search: Exhaustive (tries all), slow but thorough\")\n",
    "print(f\"   - Random Search: Random sampling, fast but might miss best\")\n",
    "print(f\"   - Both methods use cross-validation for reliable evaluation\")\n",
    "print(f\"   - Test accuracy validates that tuning worked correctly\")\n",
    "print(f\"   - Choose method based on time constraints and search space size\")\n",
    "print(f\"   - Random Search often finds good solutions faster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Comparison Table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Comparison of Methods\")\n",
    "print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø·Ø±Ù‚\")\n",
    "print(\"=\" * 60)\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column data\n",
    "#   - Each key-value pair: key = column name, value = list of values for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['Grid Search (RF)', 'Random Search (RF)', 'Grid Search (SVM)'],\n",
    "    'Best CV Score': [\n",
    "        grid_search_rf.best_score_,\n",
    "        random_search_rf.best_score_,\n",
    "        grid_search_svm.best_score_\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        test_acc_rf,\n",
    "        test_acc_rf_rs,\n",
    "        test_acc_svm\n",
    "    ],\n",
    "    'Total Combinations Tried': [\n",
    "        grid_search_rf.cv_results_['params'].__len__(),\n",
    "        n_iter,\n",
    "        grid_search_svm.cv_results_['params'].__len__()\n",
    "    ]\n",
    "})\n",
    "print(\"\\nComparison Table:\")\n",
    "print(comparison.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Grid Search Results (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Visualize Grid Search Results\")\n",
    "print(\"ØªØµÙˆØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for visualization\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column data\n",
    "#   - Each key-value pair: key = column name, value = list of values for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "results_df = pd.DataFrame(grid_search_svm.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for RBF kernel\n",
    "rbf_results = results_df[results_df['param_kernel'] == 'rbf']\n",
    "if len(rbf_results) > 0:\n",
    "    # Create heatmap of C vs gamma\n",
    "    pivot_table = rbf_results.pivot_table(\n",
    "        values='mean_test_score',\n",
    "        index='param_gamma',\n",
    "        columns='param_C'\n",
    "    )\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='viridis', cbar_kws={'label': 'Mean CV Score'})\n",
    "    plt.title('Grid Search Results: SVM (RBF Kernel)\\nC vs Gamma')\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('Gamma')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('grid_search_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nâœ“ Plot saved as 'grid_search_heatmap.png'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Compare Computation Time\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Computation Time Comparison\")\n",
    "print(\"Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆÙ‚Øª Ø§Ù„Ø­Ø³Ø§Ø¨\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search timing (use smaller grid for demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "rf_small = RandomForestClassifier(random_state=42)\n",
    "grid_small = GridSearchCV(rf_small, small_param_grid, cv=3, n_jobs=-1)\n",
    "start_time = time.time()\n",
    "grid_small.fit(X_train[:500], y_train[:500])  # Use subset for speed\n",
    "grid_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Decision Framework - Grid Search vs Random Search | Ø§Ù„Ø®Ø·ÙˆØ© 8: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ\n",
    "\n",
    "**BEFORE**: You've learned how to use Grid Search, but when should you use Grid Search vs Random Search?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right hyperparameter tuning method!\n",
    "\n",
    "**Why this matters**: Using the wrong search method can:\n",
    "- **Waste computation** â†’ Grid Search tries all combinations, may be unnecessary\n",
    "- **Miss better parameters** â†’ Random Search may miss optimal combinations\n",
    "- **Poor efficiency** â†’ Wrong method takes too long or doesn't find good parameters\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Grid Search vs Random Search | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ\n",
    "\n",
    "**Key Question**: Should I use **GRID SEARCH** or **RANDOM SEARCH** for hyperparameter tuning?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you need to tune hyperparameters?\n",
    "â”œâ”€ NO â†’ No search needed âœ…\n",
    "â”‚   â””â”€ Why? Use default parameters or known good values\n",
    "â”‚\n",
    "â””â”€ YES â†’ Check hyperparameter space:\n",
    "    â”œâ”€ Few hyperparameters (<5)? â†’ Check parameter ranges\n",
    "    â”‚   â”œâ”€ Small ranges (few values each)? â†’ Use GRID SEARCH âœ…\n",
    "    â”‚   â”‚   â””â”€ Why? Can try all combinations efficiently\n",
    "    â”‚   â”‚\n",
    "    â”‚   â””â”€ Large ranges (many values each)? â†’ Use RANDOM SEARCH âœ…\n",
    "    â”‚       â””â”€ Why? Too many combinations for grid search\n",
    "    â”‚\n",
    "    â”œâ”€ Many hyperparameters (â‰¥5)? â†’ Use RANDOM SEARCH âœ…\n",
    "    â”‚   â””â”€ Why? Grid search becomes too expensive\n",
    "    â”‚\n",
    "    â”œâ”€ Need exhaustive search? â†’ Use GRID SEARCH âœ…\n",
    "    â”‚   â””â”€ Why? Grid search tries all combinations\n",
    "    â”‚\n",
    "    â””â”€ Need faster results? â†’ Use RANDOM SEARCH âœ…\n",
    "        â””â”€ Why? Random search is faster, often finds good parameters\n",
    "```\n",
    "\n",
    "#### Detailed Decision Process:\n",
    "\n",
    "```\n",
    "Step 1: Number of Hyperparameters\n",
    "â”œâ”€ Few (<5) â†’ Continue to Step 2\n",
    "â””â”€ Many (â‰¥5) â†’ Use RANDOM SEARCH âœ…\n",
    "    â””â”€ Why? Grid search becomes computationally expensive\n",
    "\n",
    "Step 2: Parameter Space Size\n",
    "â”œâ”€ Small space (few values per parameter) â†’ Use GRID SEARCH âœ…\n",
    "â”‚   â””â”€ Why? Can try all combinations efficiently\n",
    "â”‚\n",
    "â””â”€ Large space (many values per parameter) â†’ Use RANDOM SEARCH âœ…\n",
    "    â””â”€ Why? Too many combinations for grid search\n",
    "\n",
    "Step 3: Computational Resources\n",
    "â”œâ”€ Limited resources, need speed â†’ Use RANDOM SEARCH âœ…\n",
    "â”‚   â””â”€ Why? Random search is faster\n",
    "â”‚\n",
    "â””â”€ Have resources, need best parameters â†’ Use GRID SEARCH âœ…\n",
    "    â””â”€ Why? Grid search is exhaustive\n",
    "\n",
    "Step 4: Search Strategy\n",
    "â”œâ”€ Need exhaustive search (all combinations)? â†’ Use GRID SEARCH âœ…\n",
    "â””â”€ Need efficient search (sample combinations)? â†’ Use RANDOM SEARCH âœ…\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: Grid Search vs Random Search | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Aspect | Grid Search | Random Search | Winner |\n",
    "|--------|-------------|---------------|--------|\n",
    "| **Search Strategy** | Exhaustive (tries all combinations) | Random sampling | Grid (exhaustive) |\n",
    "| **Speed** | Slow (tries all combinations) | Fast (samples combinations) | Random |\n",
    "| **Best Parameters** | Guaranteed to find best in grid | May miss best, but often finds good | Grid (guaranteed) |\n",
    "| **Computational Cost** | High (many combinations) | Low (fewer combinations) | Random |\n",
    "| **Hyperparameter Count** | Good for <5 parameters | Good for any number | Random (scalable) |\n",
    "| **Parameter Space** | Good for small spaces | Good for large spaces | Random (scalable) |\n",
    "| **Best For** | Small spaces, few parameters | Large spaces, many parameters | Depends |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use Grid Search | Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ\n",
    "\n",
    "**Use Grid Search when:**\n",
    "\n",
    "1. **Few Hyperparameters** âœ…\n",
    "   - Less than 5 hyperparameters to tune\n",
    "   - Grid search is manageable\n",
    "   - **Example**: Tuning C and gamma for SVM (2 parameters)\n",
    "\n",
    "2. **Small Parameter Space** âœ…\n",
    "   - Few values per parameter\n",
    "   - Total combinations < 1000\n",
    "   - **Example**: 3 values for C, 3 values for gamma = 9 combinations\n",
    "\n",
    "3. **Need Exhaustive Search** âœ…\n",
    "   - Want to try all combinations\n",
    "   - Need guaranteed best parameters in grid\n",
    "   - **Example**: Critical application, need best parameters\n",
    "\n",
    "4. **Have Computational Resources** âœ…\n",
    "   - Can afford to try all combinations\n",
    "   - Time is not critical\n",
    "   - **Example**: Research project, can run overnight\n",
    "\n",
    "5. **Discrete Parameters** âœ…\n",
    "   - Parameters have discrete values\n",
    "   - Easy to define grid\n",
    "   - **Example**: max_depth = [3, 5, 7, 10]\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use Random Search | Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ\n",
    "\n",
    "**Use Random Search when:**\n",
    "\n",
    "1. **Many Hyperparameters** âœ…\n",
    "   - 5+ hyperparameters to tune\n",
    "   - Grid search becomes too expensive\n",
    "   - **Example**: Tuning 10+ hyperparameters for neural networks\n",
    "\n",
    "2. **Large Parameter Space** âœ…\n",
    "   - Many values per parameter\n",
    "   - Total combinations > 1000\n",
    "   - **Example**: 10 values for C, 10 for gamma = 100 combinations (still manageable, but if more...)\n",
    "\n",
    "3. **Need Speed** âœ…\n",
    "   - Limited computational resources\n",
    "   - Need results quickly\n",
    "   - **Example**: Quick prototyping, limited time\n",
    "\n",
    "4. **Continuous Parameters** âœ…\n",
    "   - Parameters are continuous\n",
    "   - Hard to define grid\n",
    "   - **Example**: Learning rate (0.0001 to 0.1, continuous)\n",
    "\n",
    "5. **Some Parameters More Important** âœ…\n",
    "   - Some parameters matter more than others\n",
    "   - Random search explores more efficiently\n",
    "   - **Example**: Learning rate more important than batch size\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When NOT to Use Each Method | Ù…ØªÙ‰ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©\n",
    "\n",
    "#### Don't use Grid Search when:\n",
    "1. **Many Hyperparameters** âŒ\n",
    "   - 5+ parameters\n",
    "   - Grid search becomes too expensive\n",
    "   - **Use Instead**: Random Search\n",
    "\n",
    "2. **Large Parameter Space** âŒ\n",
    "   - Many values per parameter\n",
    "   - Too many combinations\n",
    "   - **Use Instead**: Random Search\n",
    "\n",
    "3. **Limited Resources** âŒ\n",
    "   - Can't afford many combinations\n",
    "   - **Use Instead**: Random Search\n",
    "\n",
    "4. **Continuous Parameters** âŒ\n",
    "   - Parameters are continuous\n",
    "   - Hard to grid\n",
    "   - **Use Instead**: Random Search\n",
    "\n",
    "#### Don't use Random Search when:\n",
    "1. **Need Guaranteed Best** âŒ\n",
    "   - Need to find best parameters in space\n",
    "   - **Use Instead**: Grid Search\n",
    "\n",
    "2. **Small Space** âŒ\n",
    "   - Few combinations (< 100)\n",
    "   - Grid search is fast enough\n",
    "   - **Use Instead**: Grid Search\n",
    "\n",
    "3. **Few Parameters** âŒ\n",
    "   - Less than 3 parameters\n",
    "   - Grid search is manageable\n",
    "   - **Use Instead**: Grid Search\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: SVM Tuning (C and gamma) âœ… GRID SEARCH\n",
    "- **Hyperparameters**: 2 (C, gamma)\n",
    "- **Parameter Space**: Small (3 values each = 9 combinations)\n",
    "- **Need**: Best parameters\n",
    "- **Decision**: âœ… Use Grid Search\n",
    "- **Reasoning**: Few parameters, small space, can try all combinations\n",
    "\n",
    "#### Example 2: Neural Network Tuning (10+ parameters) âœ… RANDOM SEARCH\n",
    "- **Hyperparameters**: 10+ (learning rate, batch size, layers, etc.)\n",
    "- **Parameter Space**: Very large (thousands of combinations)\n",
    "- **Need**: Good parameters quickly\n",
    "- **Decision**: âœ… Use Random Search\n",
    "- **Reasoning**: Many parameters, large space, need speed\n",
    "\n",
    "#### Example 3: Random Forest (3 parameters) âœ… GRID SEARCH\n",
    "- **Hyperparameters**: 3 (n_estimators, max_depth, min_samples_split)\n",
    "- **Parameter Space**: Small (5 values each = 125 combinations)\n",
    "- **Need**: Best parameters\n",
    "- **Decision**: âœ… Use Grid Search\n",
    "- **Reasoning**: Few parameters, manageable space, can try all\n",
    "\n",
    "#### Example 4: XGBoost (6+ parameters) âœ… RANDOM SEARCH\n",
    "- **Hyperparameters**: 6+ (learning_rate, max_depth, n_estimators, etc.)\n",
    "- **Parameter Space**: Large (hundreds of combinations)\n",
    "- **Need**: Good parameters efficiently\n",
    "- **Decision**: âœ… Use Random Search\n",
    "- **Reasoning**: Many parameters, large space, need efficiency\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Few parameters â†’ Grid Search** - Use when <5 parameters\n",
    "2. **Many parameters â†’ Random Search** - Use when â‰¥5 parameters\n",
    "3. **Small space â†’ Grid Search** - Use when <1000 combinations\n",
    "4. **Large space â†’ Random Search** - Use when >1000 combinations\n",
    "5. **Need speed â†’ Random Search** - Use when time is limited\n",
    "6. **Need best â†’ Grid Search** - Use when need exhaustive search\n",
    "7. **Try both** - Sometimes try both, compare results\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Tuning SVM with C and gamma (3 values each)\n",
    "- **Hyperparameters**: 2\n",
    "- **Combinations**: 9 (small)\n",
    "- **Decision**: âœ… Grid Search (few parameters, small space)\n",
    "\n",
    "**Scenario 2**: Tuning neural network with 8 hyperparameters\n",
    "- **Hyperparameters**: 8\n",
    "- **Combinations**: Thousands\n",
    "- **Decision**: âœ… Random Search (many parameters, large space)\n",
    "\n",
    "**Scenario 3**: Tuning Random Forest with 4 parameters (need best)\n",
    "- **Hyperparameters**: 4\n",
    "- **Combinations**: 256 (manageable)\n",
    "- **Decision**: âœ… Grid Search (few parameters, manageable space, need best)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 2: Boosting** - Uses hyperparameter tuning for XGBoost and LightGBM\n",
    "- ğŸ““ **All ML Models** - Hyperparameter tuning improves all models\n",
    "- ğŸ““ **Production Systems** - Proper tuning is critical for performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search timing\n",
    "random_small = RandomizedSearchCV(rf_small, small_param_grid, n_iter=4, cv=3, n_jobs=-1, random_state=42)\n",
    "start_time = time.time()\n",
    "random_small.fit(X_train[:500], y_train[:500])\n",
    "random_time = time.time() - start_time\n",
    "print(f\"\\nGrid Search Time: {grid_time:.2f} seconds\")\n",
    "print(f\"Random Search Time: {random_time:.2f} seconds\")\n",
    "print(f\"Speedup: {grid_time/random_time:.2f}x\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 1 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 1! âœ“\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
