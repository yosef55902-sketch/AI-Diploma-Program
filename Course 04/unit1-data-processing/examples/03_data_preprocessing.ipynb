{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Data Preprocessing | Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - Know your data structure\n",
    "- âœ… **Example 2: Data Cleaning** - Have clean data to preprocess\n",
    "- âœ… **Basic ML concepts**: Features, targets, training vs testing\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why preprocessing is needed\n",
    "- Knowing which preprocessing method to use\n",
    "- Understanding the difference between scaling methods\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the THIRD example** - it prepares clean data for machine learning!\n",
    "\n",
    "**Why this example THIRD?**\n",
    "- **Before** you can build ML models, you need preprocessed data\n",
    "- **Before** you can train models, you need scaled features\n",
    "- **Before** you can make predictions, you need encoded categories\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading (we know our data)\n",
    "- ğŸ““ Example 2: Data Cleaning (we have clean data)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 4: Linear Regression (needs preprocessed data)\n",
    "- ğŸ““ Example 5: Polynomial Regression (needs scaled features)\n",
    "- ğŸ““ All ML models (all need preprocessing!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Preprocessing transforms **clean data into ML-ready format**\n",
    "2. Preprocessing teaches you **scaling vs encoding** (different problems, different solutions)\n",
    "3. Preprocessing shows you **train-test split** (essential for evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Preparing Ingredients for Cooking | Ø§Ù„Ù‚ØµØ©: ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„Ø·Ø¨Ø®\n",
    "\n",
    "Imagine you're cooking. **Before** you can cook, you need to prepare ingredients - cut vegetables to the same size, marinate meat, measure spices. **After** preparing everything uniformly, you can cook a perfect meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we preprocess data - scale features to same range, encode categories to numbers, split into train/test. **After** preprocessing, we can build accurate models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Preprocessing Matters | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Preprocessing is essential for ML success:\n",
    "- **Feature Scaling**: Algorithms work better when features are on similar scales\n",
    "- **Encoding**: ML algorithms need numbers, not text categories\n",
    "- **Train-Test Split**: We need separate data to evaluate model performance\n",
    "- **Without Preprocessing**: Models fail or perform poorly\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Scale features using Standardization and Normalization\n",
    "2. Encode categorical variables (Label vs One-Hot)\n",
    "3. Split data into training and testing sets\n",
    "4. Understand when to use each preprocessing method\n",
    "5. Build a complete preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ğŸ“š What each preprocessing tool does:\n",
      "   - StandardScaler: Scale features to mean=0, std=1 (z-score)\n",
      "   - MinMaxScaler: Scale features to range [0, 1]\n",
      "   - LabelEncoder: Convert categories to numbers (ordinal)\n",
      "   - OneHotEncoder: Convert categories to binary columns (nominal)\n",
      "   - train_test_split: Split data into train/test sets\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us preprocess data for machine learning\n",
    "\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,    # For standardization (mean=0, std=1)\n",
    "    MinMaxScaler,      # For normalization (range 0-1)\n",
    "    LabelEncoder,      # For ordinal encoding\n",
    "    OneHotEncoder      # For nominal encoding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each preprocessing tool does:\")\n",
    "print(\"   - StandardScaler: Scale features to mean=0, std=1 (z-score)\")\n",
    "print(\"   - MinMaxScaler: Scale features to range [0, 1]\")\n",
    "print(\"   - LabelEncoder: Convert categories to numbers (ordinal)\")\n",
    "print(\"   - OneHotEncoder: Convert categories to binary columns (nominal)\")\n",
    "print(\"   - train_test_split: Split data into train/test sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We have clean data, but it's not ready for machine learning - features have different scales, categories are text, and we haven't split the data.\n",
    "\n",
    "**AFTER**: We'll preprocess the data - scale features, encode categories, split into train/test - making it ready for ML models!\n",
    "\n",
    "**Why this matters**: Most ML algorithms require preprocessed data. Without it, models fail or give poor results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Creating sample data...\n",
      "Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\n",
      "\n",
      "ğŸ“Š Original Data:\n",
      "   age  salary  experience    city education  target\n",
      "0   58   65222    9.899761  Jeddah  Bachelor       0\n",
      "1   48   93335   12.258333  Dammam    Master       0\n",
      "2   34   40965    8.328012  Riyadh    Master       0\n",
      "3   27   54538    7.944759  Jeddah    Master       0\n",
      "4   40   38110    3.627784  Dammam  Bachelor       1\n",
      "\n",
      "ğŸ“ Data Shape: (100, 6)\n",
      "\n",
      "ğŸ” Notice:\n",
      "   - Numeric features have VERY different scales (age: 20-60, salary: 30k-100k)\n",
      "   - Categorical features are text (city, education)\n",
      "   - ML algorithms need same-scale numbers!\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with different scales and categories\n",
    "# This simulates real-world data that needs preprocessing\n",
    "\n",
    "print(\"\\n1. Creating sample data...\")\n",
    "print(\"Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\")\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data = {\n",
    "    'age': np.random.randint(20, 60, 100),              # Range: 20-60\n",
    "    'salary': np.random.randint(30000, 100000, 100),    # Range: 30k-100k (different scale!)\n",
    "    'experience': np.random.uniform(0, 15, 100),        # Range: 0-15 (different scale!)\n",
    "    'city': np.random.choice(['Riyadh', 'Jeddah', 'Dammam', 'Khobar'], 100),  # Categorical (text)\n",
    "    'education': np.random.choice(['Bachelor', 'Master', 'PhD'], 100),        # Categorical (text)\n",
    "    'target': np.random.choice([0, 1], 100)  # Binary target for classification\n",
    "}\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column data\n",
    "#   - Each key-value pair: key = column name, value = list of values for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nğŸ“Š Original Data:\")\n",
    "print(df.head())\n",
    "print(f\"\\nğŸ“ Data Shape: {df.shape}\")\n",
    "print(\"\\nğŸ” Notice:\")\n",
    "print(\"   - Numeric features have VERY different scales (age: 20-60, salary: 30k-100k)\")\n",
    "print(\"   - Categorical features are text (city, education)\")\n",
    "print(\"   - ML algorithms need same-scale numbers!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Data | Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©\n",
    "\n",
    "**BEFORE**: We need to learn preprocessing, but we need sample data with different scales and categories.\n",
    "\n",
    "**AFTER**: We'll create data with numeric features (different scales) and categorical features (text) to practice preprocessing!\n",
    "\n",
    "**Why create this data?** Real datasets have these characteristics - we need to learn how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Feature Scaling - Standardization\n",
      "Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Feature Scaling - Standardization\")\n",
    "print(\"Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Scaling - Standardization | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\n",
    "\n",
    "**BEFORE**: Features have very different scales (age: 20-60, salary: 30k-100k). Algorithms will be biased toward larger numbers!\n",
    "\n",
    "**AFTER**: We'll standardize features so they all have mean=0 and std=1, putting them on the same scale!\n",
    "\n",
    "**Why Standardization?**\n",
    "- Formula: `(x - mean) / std`\n",
    "- Result: Mean = 0, Standard Deviation = 1\n",
    "- Use when: Data is normally distributed, outliers are important\n",
    "- Good for: Linear models, neural networks, distance-based algorithms\n",
    "\n",
    "**Note**: Remove problematic outliers first (Example 2), then standardize. Standardization preserves outliers, so clean data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… StandardScaler created\n",
      "   Formula: (x - mean) / std\n"
     ]
    }
   ],
   "source": [
    "# Create StandardScaler object\n",
    "# Standardization formula: (x - mean) / std\n",
    "# This transforms data so mean=0 and std=1\n",
    "\n",
    "# StandardScaler()\n",
    "# - Creates scaler object for standardization\n",
    "# - Standardization: (x - mean) / std\n",
    "#   - Transforms data so mean = 0, standard deviation = 1\n",
    "#   - Centers data around 0, scales to unit variance\n",
    "# - Two-step process:\n",
    "#   1. .fit(): Learn mean and std from data\n",
    "#   2. .transform(): Apply transformation\n",
    "# .fit_transform(data)\n",
    "# - Two operations in one: .fit() then .transform()\n",
    "#   1. .fit(): Learns parameters from data (mean/std, categories, etc.)\n",
    "#   2. .transform(): Applies transformation using learned parameters\n",
    "# - Use on training data\n",
    "# - For test data, use only .transform() (don't refit!)\n",
    "\n",
    "# - Or use .fit_transform(): Do both in one step\n",
    "# - Important: Fit on training data, transform both train and test\n",
    "scaler_standard = StandardScaler()\n",
    "print(\"   âœ… StandardScaler created\")\n",
    "print(\"   Formula: (x - mean) / std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Scaling columns: ['age', 'salary', 'experience']\n"
     ]
    }
   ],
   "source": [
    "# Select numeric columns for scaling\n",
    "# Why only numeric? Categorical columns need encoding, not scaling!\n",
    "numeric_cols = ['age', 'salary', 'experience']\n",
    "df_scaled_standard = df.copy()\n",
    "print(f\"   Scaling columns: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… After Standardization (mean=0, std=1):\n",
      "Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…ØªÙˆØ³Ø·=0ØŒ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ=1):\n",
      "   Age - Mean: 0.0000, Std: 1.0050\n",
      "   Salary - Mean: 0.0000, Std: 1.0050\n",
      "   Experience - Mean: 0.0000, Std: 1.0050\n",
      "\n",
      "   âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1!\n",
      "\n",
      "   ğŸ“ Note: Values like 1.67, -1.36 are normal for standardized data!\n",
      "   ğŸ“ They represent how many standard deviations away from the mean\n",
      "   ğŸ“ (e.g., 1.67 means 1.67 standard deviations above the mean)\n",
      "\n",
      "ğŸ“„ Scaled data (first 5 rows):\n",
      "        age    salary  experience\n",
      "0  1.670713 -0.057025    0.366648\n",
      "1  0.801003  1.290892    0.918449\n",
      "2 -0.416591 -1.220060   -0.001071\n",
      "3 -1.025388 -0.569284   -0.090735\n",
      "4  0.105235 -1.356947   -1.100715\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the data\n",
    "# .fit_transform() learns the mean/std from data, then transforms it\n",
    "# Why fit_transform? We learn parameters from training data, then apply to all data\n",
    "\n",
    "# scaler_standard.fit_transform(df[numeric_cols])\n",
    "# - .fit_transform(): Two operations in one\n",
    "#   1. .fit(): Learns mean and std from data\n",
    "#      - Calculates mean and std for each column\n",
    "#      - Stores these values in scaler object\n",
    "#   2. .transform(): Applies standardization using learned parameters\n",
    "#      - Formula: (x - learned_mean) / learned_std\n",
    "#      - Transforms each value in column\n",
    "# - df[numeric_cols]: Selects only numeric columns to scale\n",
    "#   - List of column names: ['age', 'salary', 'experience']\n",
    "#   - Returns DataFrame with only those columns\n",
    "# - Result: All numeric columns now have meanâ‰ˆ0 and stdâ‰ˆ1\n",
    "# - Note: In real ML, fit on training data, transform on both train and test\n",
    "df_scaled_standard[numeric_cols] = scaler_standard.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(\"\\nâœ… After Standardization (mean=0, std=1):\")\n",
    "print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…ØªÙˆØ³Ø·=0ØŒ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ=1):\")\n",
    "print(f\"   Age - Mean: {df_scaled_standard['age'].mean():.4f}, Std: {df_scaled_standard['age'].std():.4f}\")\n",
    "print(f\"   Salary - Mean: {df_scaled_standard['salary'].mean():.4f}, Std: {df_scaled_standard['salary'].std():.4f}\")\n",
    "print(f\"   Experience - Mean: {df_scaled_standard['experience'].mean():.4f}, Std: {df_scaled_standard['experience'].std():.4f}\")\n",
    "print(\"\\n   âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1!\")\n",
    "print(\"\\nğŸ“„ Scaled data (first 5 rows):\")\n",
    "print(\"   (Values like 1.67 = 1.67 standard deviations above mean, -1.36 = below mean)\")\n",
    "print(df_scaled_standard[numeric_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Scaling - Normalization (Min-Max) | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªØ·Ø¨ÙŠØ¹\n",
    "\n",
    "**BEFORE**: Features have different ranges. We want them all in the same range [0, 1].\n",
    "\n",
    "**AFTER**: We'll normalize features so they all range from 0 to 1!\n",
    "\n",
    "**Why Normalization (Min-Max)?**\n",
    "- Formula: `(x - min) / (max - min)`\n",
    "- Result: Range [0, 1]\n",
    "- Use when: Data is not normally distributed, you want bounded values\n",
    "- Good for: Neural networks, algorithms that need [0,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Min-Max Normalization (range 0-1):\n",
      "Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø§Ù„Ø£Ø¯Ù†Ù‰-Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø§Ù„Ù†Ø·Ø§Ù‚ 0-1):\n",
      "Age - Min: 0.0000, Max: 1.0000\n",
      "Salary - Min: 0.0000, Max: 1.0000\n",
      "        age    salary  experience\n",
      "0  0.974359  0.507795    0.664901\n",
      "1  0.717949  0.915484    0.824535\n",
      "2  0.358974  0.156025    0.558521\n",
      "3  0.179487  0.352858    0.532582\n",
      "4  0.512821  0.114622    0.240399\n"
     ]
    }
   ],
   "source": [
    "# Normalization: (x - min) / (max - min) -> range [0, 1]\n",
    "# Ø§Ù„ØªØ·Ø¨ÙŠØ¹: (x - Ø§Ù„Ø£Ø¯Ù†Ù‰) / (Ø§Ù„Ø£Ø¹Ù„Ù‰ - Ø§Ù„Ø£Ø¯Ù†Ù‰) -> Ø§Ù„Ù†Ø·Ø§Ù‚ [0, 1]\n",
    "\n",
    "# MinMaxScaler()\n",
    "# - Creates scaler object for min-max normalization\n",
    "# - Normalization: (x - min) / (max - min)\n",
    "#   - Transforms data to range [0, 1]\n",
    "#   - Minimum value becomes 0, maximum becomes 1\n",
    "# - Two-step process:\n",
    "#   1. .fit(): Learn min and max from data\n",
    "#   2. .transform(): Apply transformation\n",
    "# - Or use .fit_transform(): Do both in one step\n",
    "# - Use when: Data not normally distributed, need bounded [0,1] range\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_scaled_minmax = df.copy()\n",
    "df_scaled_minmax[numeric_cols] = scaler_minmax.fit_transform(df[numeric_cols])\n",
    "print(\"\\nAfter Min-Max Normalization (range 0-1):\")\n",
    "print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø§Ù„Ø£Ø¯Ù†Ù‰-Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø§Ù„Ù†Ø·Ø§Ù‚ 0-1):\")\n",
    "print(f\"Age - Min: {df_scaled_minmax['age'].min():.4f}, Max: {df_scaled_minmax['age'].max():.4f}\")\n",
    "print(f\"Salary - Min: {df_scaled_minmax['salary'].min():.4f}, Max: {df_scaled_minmax['salary'].max():.4f}\")\n",
    "print(df_scaled_minmax[numeric_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Label Encoding\n",
      "Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. Label Encoding (for ordinal categories)\n",
    "# Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª (Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„ØªØ±ØªÙŠØ¨ÙŠØ©)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Label Encoding\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LabelEncoder()\n",
    "# - Creates encoder for ordinal categorical data\n",
    "# - Ordinal: Categories have order (e.g., Bachelor < Master < PhD)\n",
    "# - Converts text categories to integers (0, 1, 2, ...)\n",
    "# - Two-step process:\n",
    "#   1. .fit(): Learn unique categories\n",
    "#   2. .transform(): Convert categories to numbers\n",
    "# - Or use .fit_transform(): Do both in one step\n",
    "# - Use when: Categories have meaningful order\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Encoding for Education:\n",
      "Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªØ¹Ù„ÙŠÙ…:\n",
      "Mapping / Ø§Ù„ØªØ¹ÙŠÙŠÙ†:\n",
      "  Bachelor: 0\n",
      "  Master: 1\n",
      "  PhD: 2\n",
      "\n",
      "Encoded values:\n",
      "  education  education_encoded\n",
      "0  Bachelor                  0\n",
      "1    Master                  1\n",
      "2    Master                  1\n",
      "3    Master                  1\n",
      "4  Bachelor                  0\n",
      "5    Master                  1\n",
      "6    Master                  1\n",
      "7  Bachelor                  0\n",
      "8       PhD                  2\n",
      "9  Bachelor                  0\n"
     ]
    }
   ],
   "source": [
    "# Encode education (if we assume ordinal: Bachelor < Master < PhD)\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# label_encoder.fit_transform(df['education'])\n",
    "# - .fit_transform(): Two operations in one\n",
    "#   1. .fit(): Learns unique categories from data\n",
    "#      - Finds all unique values (e.g., 'Bachelor', 'Master', 'PhD')\n",
    "#      - Assigns numbers in alphabetical order (or order found)\n",
    "#   2. .transform(): Converts categories to integers\n",
    "#      - 'Bachelor' â†’ 0, 'Master' â†’ 1, 'PhD' â†’ 2\n",
    "# - df['education']: Passes education column as Series\n",
    "# - Returns numpy array with encoded integers\n",
    "# - label_encoder.classes_: Shows mapping (original categories)\n",
    "df_encoded['education_encoded'] = label_encoder.fit_transform(df['education'])\n",
    "\n",
    "# label_encoder.classes_\n",
    "# - Returns array of unique categories in order they were encoded\n",
    "# - Used to see mapping: classes_[0] = first category, etc.\n",
    "print(\"\\nLabel Encoding for Education:\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªØ¹Ù„ÙŠÙ…:\")\n",
    "print(\"Mapping / Ø§Ù„ØªØ¹ÙŠÙŠÙ†:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {i}\")\n",
    "print(\"\\nEncoded values:\")\n",
    "print(df_encoded[['education', 'education_encoded']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. One-Hot Encoding\n",
      "Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. One-Hot Encoding (for nominal categories)\n",
    "# Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù† (Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„Ø§Ø³Ù…ÙŠØ©)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. One-Hot Encoding\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Hot Encoded columns:\n",
      "Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø´ÙØ±Ø© Ø¨Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†:\n",
      "['age', 'salary', 'experience', 'target', 'city_Dammam', 'city_Jeddah', 'city_Khobar', 'city_Riyadh', 'edu_Bachelor', 'edu_Master', 'edu_PhD']\n",
      "\n",
      "Sample of One-Hot Encoded data:\n",
      "   age  salary  city_Dammam  city_Jeddah  city_Khobar  city_Riyadh  \\\n",
      "0   58   65222        False         True        False        False   \n",
      "1   48   93335         True        False        False        False   \n",
      "2   34   40965        False        False        False         True   \n",
      "3   27   54538        False         True        False        False   \n",
      "4   40   38110         True        False        False        False   \n",
      "\n",
      "   edu_Bachelor  edu_Master  edu_PhD  \n",
      "0          True       False    False  \n",
      "1         False        True    False  \n",
      "2         False        True    False  \n",
      "3         False        True    False  \n",
      "4          True       False    False  \n"
     ]
    }
   ],
   "source": [
    "# Using pandas get_dummies (easier)\n",
    "# Ø§Ø³ØªØ®Ø¯Ø§Ù… pandas get_dummies (Ø£Ø³Ù‡Ù„)\n",
    "\n",
    "# pd.get_dummies(df, columns=['city', 'education'], prefix=['city', 'edu'])\n",
    "# - pd.get_dummies(): Creates one-hot encoded columns for categorical data\n",
    "# - df: DataFrame to encode\n",
    "# - columns=['city', 'education']: Columns to encode (list of column names)\n",
    "# - prefix=['city', 'edu']: Prefix for new column names\n",
    "#   - 'city' column with values ['Riyadh', 'Jeddah'] â†’ 'city_Riyadh', 'city_Jeddah'\n",
    "#   - 'education' column â†’ 'edu_Bachelor', 'edu_Master', 'edu_PhD'\n",
    "# - One-hot encoding: Each category becomes a binary column (0 or 1)\n",
    "#   - Example: If city='Riyadh', then city_Riyadh=1, others=0\n",
    "# - Returns DataFrame with original columns + new binary columns\n",
    "# - Use for: Nominal categories (no order, like city names)\n",
    "df_onehot = pd.get_dummies(df, columns=['city', 'education'], prefix=['city', 'edu'])\n",
    "\n",
    "# df_onehot.columns.tolist()\n",
    "# - df_onehot.columns: Returns Index with column names\n",
    "# - .tolist(): Converts to Python list\n",
    "# - Shows all column names after encoding\n",
    "print(\"\\nOne-Hot Encoded columns:\")\n",
    "print(\"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø´ÙØ±Ø© Ø¨Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†:\")\n",
    "print(df_onehot.columns.tolist())\n",
    "print(\"\\nSample of One-Hot Encoded data:\")\n",
    "print(df_onehot[['age', 'salary'] + [col for col in df_onehot.columns if 'city_' in col or 'edu_' in col]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using sklearn OneHotEncoder ---\n",
      "City encoding classes:\n",
      "[array(['Dammam', 'Jeddah', 'Khobar', 'Riyadh'], dtype=object)]\n",
      "\n",
      "Encoded shape: (100, 3)\n",
      "First 5 encoded values:\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn OneHotEncoder (for more control)\n",
    "# Ø§Ø³ØªØ®Ø¯Ø§Ù… sklearn OneHotEncoder (Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­ÙƒÙ…)\n",
    "print(\"\\n--- Using sklearn OneHotEncoder ---\")\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "city_encoded = onehot_encoder.fit_transform(df[['city']])\n",
    "print(\"City encoding classes:\")\n",
    "print(onehot_encoder.categories_)\n",
    "print(f\"\\nEncoded shape: {city_encoded.shape}\")\n",
    "print(\"First 5 encoded values:\")\n",
    "print(city_encoded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using sklearn OneHotEncoder ---\n",
      "   City encoding classes:\n",
      "   [array(['Dammam', 'Jeddah', 'Khobar', 'Riyadh'], dtype=object)]\n",
      "\n",
      "   Encoded shape: (100, 3)\n",
      "   (4 cities â†’ 3 columns, because drop='first' removes one)\n",
      "\n",
      "   First 5 encoded values:\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "   âœ… Each row has 1 in one column (the city), 0 in others\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using sklearn OneHotEncoder (for more control)\n",
    "# Why sklearn? More control - can drop first column to avoid multicollinearity\n",
    "# drop='first' removes one column (redundant - can be inferred from others)\n",
    "\n",
    "print(\"\\n--- Using sklearn OneHotEncoder ---\")\n",
    "\n",
    "# OneHotEncoder(sparse_output=False, drop='first')\n",
    "# - OneHotEncoder(): Creates one-hot encoder object\n",
    "# - sparse_output=False: Returns dense array (True returns sparse matrix)\n",
    "# - drop='first': Drops first category column to avoid multicollinearity\n",
    "#   - If 4 cities, creates 3 columns (not 4)\n",
    "#   - One column is redundant (can be inferred from others)\n",
    "#   - Prevents perfect correlation between columns\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "# onehot_encoder.fit_transform(df[['city']])\n",
    "# - .fit_transform(): Learns categories and encodes\n",
    "#   1. .fit(): Learns unique categories\n",
    "#   2. .transform(): Creates binary columns\n",
    "# - df[['city']]: Double brackets [[]] to pass as DataFrame (not Series)\n",
    "#   - OneHotEncoder expects 2D input (DataFrame or 2D array)\n",
    "# - Returns numpy array with binary columns\n",
    "# - Example: 4 cities â†’ 3 columns (if drop='first')\n",
    "city_encoded = onehot_encoder.fit_transform(df[['city']])\n",
    "\n",
    "print(\"   City encoding classes:\")\n",
    "print(f\"   {onehot_encoder.categories_}\")\n",
    "print(f\"\\n   Encoded shape: {city_encoded.shape}\")\n",
    "print(f\"   (4 cities â†’ 3 columns, because drop='first' removes one)\")\n",
    "print(\"\\n   First 5 encoded values:\")\n",
    "print(city_encoded[:5])\n",
    "print(\"\\n   âœ… Each row has 1 in one column (the city), 0 in others\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train-Test Split | Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±\n",
    "\n",
    "**BEFORE**: We have all our data together. We can't evaluate model performance on data it has seen!\n",
    "\n",
    "**AFTER**: We'll split data into training (80%) and testing (20%) sets!\n",
    "\n",
    "**Why Train-Test Split?**\n",
    "- **Training set**: Used to train/learn the model\n",
    "- **Test set**: Used to evaluate model performance (unseen data)\n",
    "- **Why separate?** Testing on training data gives false high scores (overfitting)\n",
    "- **Standard split**: 80% train, 20% test (can vary based on dataset size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[['age', 'salary', 'experience']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data shape: (100, 3)\n",
      "Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: (100, 3)\n",
      "\n",
      "Training set:\n",
      "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n",
      "  X_train: (80, 3)\n",
      "  y_train: (80,)\n",
      "\n",
      "Test set:\n",
      "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\n",
      "  X_test: (20, 3)\n",
      "  y_test: (20,)\n",
      "\n",
      "Train percentage: 80.0%\n",
      "Test percentage: 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets (80% train, 20% test)\n",
    "# Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± (80% ØªØ¯Ø±ÙŠØ¨ØŒ 20% Ø§Ø®ØªØ¨Ø§Ø±)\n",
    "\n",
    "# train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# - train_test_split(): Splits data into training and testing sets\n",
    "# - X: Features (input variables) - DataFrame or array\n",
    "# - y: Target (output variable) - Series or array\n",
    "# - test_size=0.2: 20% for testing, 80% for training (can use 0.25 for 25%)\n",
    "# - random_state=42: Seed for random number generator\n",
    "#   - Ensures same split every time (reproducibility)\n",
    "#   - Different seeds = different splits\n",
    "# - stratify=y: Maintains class distribution in train/test\n",
    "#   - If y has 60% class 0, 40% class 1, train/test will have same ratio\n",
    "#   - Important for imbalanced datasets\n",
    "# - Returns 4 arrays: X_train, X_test, y_train, y_test\n",
    "#   - X_train: Training features (80% of X)\n",
    "#   - X_test: Testing features (20% of X)\n",
    "#   - y_train: Training targets (80% of y)\n",
    "#   - y_test: Testing targets (20% of y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "print(f\"\\nOriginal data shape: {X.shape}\")\n",
    "print(f\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {X.shape}\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "print(f\"\\nTrain percentage: {len(X_train) / len(X) * 100:.1f}%\")\n",
    "print(f\"Test percentage: {len(X_test) / len(X) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Preprocessing Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. Complete Preprocessing Pipeline\n",
      "Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Ù…Ø«Ø§Ù„ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"7. Complete Preprocessing Pipeline\")\n",
    "print(\"Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def preprocess_data(df, numeric_cols, categorical_cols, target_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline\n",
    "    Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df[numeric_cols + categorical_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_encoded[numeric_cols] = scaler.fit_transform(X_encoded[numeric_cols])\n",
    "    \n",
    "    # Split data\n",
    "# train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=42: Seed for reproducibility (same split every time)\n",
    "# - stratify=y: Maintains class distribution in train/test (for classification)\n",
    "# - Returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_encoded, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Decision Framework - When to Use Each Preprocessing Method | Ø§Ù„Ø®Ø·ÙˆØ© 8: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
    "\n",
    "**BEFORE**: You've learned different preprocessing methods, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right preprocessing method for any situation!\n",
    "\n",
    "**Why this matters**: Using the wrong preprocessing method can:\n",
    "- **Break your models** â†’ Wrong scaling causes algorithm failures\n",
    "- **Reduce performance** â†’ Inappropriate encoding loses information\n",
    "- **Waste time** â†’ Trying all methods without guidance\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Feature Scaling | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "\n",
    "**Key Question**: Should I use **STANDARDIZATION** or **NORMALIZATION**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have numeric features?\n",
    "â”œâ”€ NO â†’ No scaling needed (categorical features need encoding, not scaling)\n",
    "â”‚\n",
    "â””â”€ YES â†’ Are features on different scales?\n",
    "    â”œâ”€ NO â†’ No scaling needed (already on similar scales)\n",
    "    â”‚\n",
    "    â””â”€ YES â†’ Is data normally distributed?\n",
    "        â”œâ”€ YES â†’ Use STANDARDIZATION (StandardScaler)\n",
    "        â”‚   â””â”€ Why? Preserves distribution, handles outliers well\n",
    "        â”‚\n",
    "        â””â”€ NO â†’ Do you need bounded values [0, 1]?\n",
    "            â”œâ”€ YES â†’ Use NORMALIZATION (MinMaxScaler)\n",
    "            â”‚   â””â”€ Why? Neural networks, algorithms requiring [0,1] range\n",
    "            â”‚\n",
    "            â””â”€ NO â†’ Use STANDARDIZATION (StandardScaler)\n",
    "                â””â”€ Why? More robust to outliers, preserves relationships\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Standardization** | Normal distribution, outliers present, linear models | â€¢ Preserves distribution<br>â€¢ Robust to outliers<br>â€¢ Mean=0, Std=1 | â€¢ Can produce values outside [0,1]<br>â€¢ Assumes normal distribution | Age (20-60), Salary (30k-100k) |\n",
    "| **Normalization** | Non-normal distribution, neural networks, need [0,1] | â€¢ Bounded [0,1]<br>â€¢ Works with any distribution<br>â€¢ Good for neural networks | â€¢ Sensitive to outliers<br>â€¢ May compress data too much | Image pixels (0-255), Ratings (1-5) |\n",
    "| **No Scaling** | Features already on similar scales, tree-based models | â€¢ No transformation needed<br>â€¢ Preserves original values | â€¢ Only works if scales are similar | All features 0-1, or all 100-200 |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Categorical Encoding | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©\n",
    "\n",
    "**Key Question**: Should I use **LABEL ENCODING** or **ONE-HOT ENCODING**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have categorical features?\n",
    "â”œâ”€ NO â†’ No encoding needed (all features are numeric)\n",
    "â”‚\n",
    "â””â”€ YES â†’ Is there an inherent order (ordinal)?\n",
    "    â”œâ”€ YES â†’ Use LABEL ENCODING\n",
    "    â”‚   â””â”€ Why? Preserves order (e.g., Low < Medium < High)\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ How many unique categories?\n",
    "        â”œâ”€ < 10 â†’ Use ONE-HOT ENCODING\n",
    "        â”‚   â””â”€ Why? Creates separate columns, no false order\n",
    "        â”‚\n",
    "        â””â”€ â‰¥ 10 â†’ Consider alternatives:\n",
    "            â”œâ”€ Use ONE-HOT if important features\n",
    "            â”‚   â””â”€ Why? Each category matters\n",
    "            â”‚\n",
    "            â””â”€ Use TARGET ENCODING or frequency encoding\n",
    "                â””â”€ Why? Avoids creating too many columns\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Label Encoding** | Ordinal categories (inherent order) | â€¢ Preserves order<br>â€¢ Single column<br>â€¢ Simple | â€¢ Creates false order for nominal<br>â€¢ Algorithms may misinterpret | Education: Bachelor=0, Master=1, PhD=2 |\n",
    "| **One-Hot Encoding** | Nominal categories (no order), < 10 categories | â€¢ No false order<br>â€¢ Each category separate<br>â€¢ Works with all algorithms | â€¢ Creates many columns<br>â€¢ Can cause curse of dimensionality | Department: IT, HR, Finance â†’ 3 columns |\n",
    "| **Target Encoding** | Many categories (>10), nominal | â€¢ Keeps single column<br>â€¢ Captures target relationship | â€¢ Can overfit<br>â€¢ More complex | City with 50+ values |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Train-Test Split | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**Key Question**: What split ratio should I use?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "How much data do you have?\n",
    "â”œâ”€ < 1000 samples â†’ Use 70/30 or 80/20\n",
    "â”‚   â””â”€ Why? Need enough test data for reliable evaluation\n",
    "â”‚\n",
    "â”œâ”€ 1000-10,000 samples â†’ Use 80/20\n",
    "â”‚   â””â”€ Why? Standard split, good balance\n",
    "â”‚\n",
    "â””â”€ > 10,000 samples â†’ Use 90/10 or 95/5\n",
    "    â””â”€ Why? Large datasets, can use less for testing\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Split Ratio | When to Use | Pros | Cons | Example |\n",
    "|-------------|-------------|------|------|---------|\n",
    "| **70/30** | Small datasets (< 1000) | â€¢ More test data<br>â€¢ Reliable evaluation | â€¢ Less training data | Dataset with 500 samples |\n",
    "| **80/20** | Medium datasets (1000-10k) | â€¢ Standard split<br>â€¢ Good balance | â€¢ May need adjustment | Dataset with 5000 samples |\n",
    "| **90/10** | Large datasets (> 10k) | â€¢ More training data<br>â€¢ Still enough test data | â€¢ Less test data | Dataset with 50,000 samples |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price Prediction\n",
    "- **Features**: Size (1000-5000 sq ft), Age (0-50 years), Location (A, B, C)\n",
    "- **Scaling Decision**: STANDARDIZATION (different scales, normal-ish distribution)\n",
    "- **Encoding Decision**: ONE-HOT for Location (nominal, 3 categories)\n",
    "- **Split Decision**: 80/20 (medium dataset)\n",
    "- **Reasoning**: Size and age need scaling, location needs encoding, standard split\n",
    "\n",
    "#### Example 2: Customer Segmentation\n",
    "- **Features**: Income ($30k-$200k), Education (Bachelor, Master, PhD), Age (18-65)\n",
    "- **Scaling Decision**: STANDARDIZATION (different scales)\n",
    "- **Encoding Decision**: LABEL ENCODING for Education (ordinal: Bachelor < Master < PhD)\n",
    "- **Split Decision**: 80/20\n",
    "- **Reasoning**: Income needs scaling, education has order, standard split\n",
    "\n",
    "#### Example 3: Image Classification\n",
    "- **Features**: Pixel values (0-255), Category (10 classes)\n",
    "- **Scaling Decision**: NORMALIZATION (need [0,1] for neural networks)\n",
    "- **Encoding Decision**: ONE-HOT for Category (nominal, 10 classes)\n",
    "- **Split Decision**: 80/20\n",
    "- **Reasoning**: Pixels need [0,1] range, categories need one-hot, standard split\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Standardization for normal data** - Use when data is normally distributed\n",
    "2. **Normalization for bounded values** - Use when you need [0,1] range\n",
    "3. **Label encoding for ordinal** - Use when categories have inherent order\n",
    "4. **One-hot for nominal** - Use when categories have no order\n",
    "5. **80/20 is standard** - Use for most datasets (adjust for very small/large)\n",
    "6. **Always scale before encoding** - Scale numeric features first, then encode categorical\n",
    "7. **Test your choices** - Try different methods and compare results\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario**: You have a dataset with:\n",
    "- Income: $20,000 - $150,000 (normally distributed)\n",
    "- Education: High School, Bachelor, Master, PhD (ordinal)\n",
    "- City: 15 different cities (nominal)\n",
    "- 5,000 samples\n",
    "\n",
    "**Your task**: Decide preprocessing methods for each feature!\n",
    "\n",
    "**Answers**:\n",
    "1. **Income**: STANDARDIZATION (normal distribution, different scale)\n",
    "2. **Education**: LABEL ENCODING (ordinal: High School < Bachelor < Master < PhD)\n",
    "3. **City**: ONE-HOT ENCODING (nominal, 15 categories is acceptable)\n",
    "4. **Split**: 80/20 (medium dataset, standard split)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 4: Linear Regression** - Uses preprocessed data to build models\n",
    "- ğŸ““ **Example 5: Polynomial Regression** - Extends linear regression with preprocessed data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
