# Beginner's Guide: Complete ML Pipeline
## Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†: Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„

---

## ğŸ¯ Real-World Application | Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©

### Example: House Price Prediction System
**Imagine you're building a system like Zillow or Redfin that predicts house prices.**

**Problem:** Real estate companies need to estimate house prices quickly and accurately to:
- Help buyers make informed decisions
- Help sellers price their homes
- Enable automated valuations

**Solution:** Your ML pipeline:
1. **Loads** house data (size, bedrooms, location, etc.)
2. **Cleans** data (removes errors, handles missing values)
3. **Processes** features (scales, encodes categories)
4. **Trains** models to predict prices
5. **Evaluates** which model works best
6. **Deploys** for real-time predictions

**Real-World Impact:**
- âœ… Instant price estimates for millions of homes
- âœ… More accurate than manual appraisals
- âœ… Available 24/7
- âœ… Saves time and money

---

## ğŸ“š Step-by-Step Guide for Beginners | Ø¯Ù„ÙŠÙ„ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†

### Step 1: Understand the ML Pipeline (Day 1)

**What is an ML Pipeline?**
A series of steps that transform raw data into predictions:

```
Raw Data â†’ Clean Data â†’ Process Features â†’ Train Model â†’ Make Predictions
```

**Why Pipeline?**
- âœ… Reusable: Use same steps for new data
- âœ… Organized: Each step has clear purpose
- âœ… Testable: Test each step separately
- âœ… Maintainable: Easy to update and fix

---

### Step 2: Set Up Your Project (Day 1)

**Create this structure:**

```
ml_pipeline_project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ house_prices.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”œâ”€â”€ feature_engineer.py
â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â””â”€â”€ pipeline.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ (saved models will go here)
â”œâ”€â”€ results/
â”‚   â””â”€â”€ (plots and reports will go here)
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

**Install libraries:**
```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```

---

### Step 3: Create Data Loader (Day 2)

**File: `src/data_loader.py`**

```python
import pandas as pd
import numpy as np

class DataLoader:
    """Loads and explores data"""
    
    def __init__(self, filepath):
        self.filepath = filepath
        self.data = None
    
    def load_data(self):
        """Load data from CSV file"""
        try:
            self.data = pd.read_csv(self.filepath)
            print(f"âœ… Loaded {len(self.data)} rows, {len(self.data.columns)} columns")
            return self.data
        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            return None
    
    def explore_data(self):
        """Explore the data"""
        if self.data is None:
            print("âŒ No data loaded")
            return
        
        print("\n" + "=" * 50)
        print("Data Overview")
        print("=" * 50)
        
        # Basic info
        print(f"\nShape: {self.data.shape}")
        print(f"\nColumns: {list(self.data.columns)}")
        
        # First few rows
        print("\nFirst 5 rows:")
        print(self.data.head())
        
        # Data types
        print("\nData types:")
        print(self.data.dtypes)
        
        # Missing values
        print("\nMissing values:")
        print(self.data.isnull().sum())
        
        # Statistics
        print("\nStatistics:")
        print(self.data.describe())
    
    def get_data(self):
        """Get loaded data"""
        return self.data
```

**Example Usage:**
```python
loader = DataLoader('data/house_prices.csv')
data = loader.load_data()
loader.explore_data()
```

---

### Step 4: Create Preprocessor (Day 3)

**File: `src/preprocessor.py`**

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

class Preprocessor:
    """Preprocesses data for ML"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.encoders = {}
    
    def handle_missing_values(self, df, strategy='mean'):
        """Handle missing values"""
        df_clean = df.copy()
        
        for col in df_clean.columns:
            if df_clean[col].isnull().sum() > 0:
                if strategy == 'mean' and df_clean[col].dtype in ['int64', 'float64']:
                    df_clean[col].fillna(df_clean[col].mean(), inplace=True)
                elif strategy == 'median' and df_clean[col].dtype in ['int64', 'float64']:
                    df_clean[col].fillna(df_clean[col].median(), inplace=True)
                elif strategy == 'mode':
                    df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)
                else:
                    df_clean[col].fillna(0, inplace=True)
        
        print(f"âœ… Handled missing values using {strategy}")
        return df_clean
    
    def encode_categorical(self, df, categorical_columns):
        """Encode categorical variables"""
        df_encoded = df.copy()
        
        for col in categorical_columns:
            if col in df_encoded.columns:
                le = LabelEncoder()
                df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
                self.encoders[col] = le
                print(f"âœ… Encoded column: {col}")
        
        return df_encoded
    
    def scale_features(self, X_train, X_test):
        """Scale numerical features"""
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        print("âœ… Scaled features")
        return X_train_scaled, X_test_scaled
```

---

### Step 5: Create Feature Engineer (Day 4)

**File: `src/feature_engineer.py`**

```python
import pandas as pd
import numpy as np

class FeatureEngineer:
    """Creates and selects features"""
    
    def create_features(self, df):
        """Create new features from existing ones"""
        df_new = df.copy()
        
        # Example: Create area per room
        if 'area' in df_new.columns and 'bedrooms' in df_new.columns:
            df_new['area_per_bedroom'] = df_new['area'] / (df_new['bedrooms'] + 1)
        
        # Example: Create age feature (if year_built exists)
        if 'year_built' in df_new.columns:
            current_year = 2024
            df_new['house_age'] = current_year - df_new['year_built']
        
        print("âœ… Created new features")
        return df_new
    
    def select_features(self, X, y, top_n=10):
        """Select most important features"""
        from sklearn.feature_selection import SelectKBest, f_regression
        
        selector = SelectKBest(score_func=f_regression, k=top_n)
        X_selected = selector.fit_transform(X, y)
        
        selected_features = X.columns[selector.get_support()]
        print(f"âœ… Selected {len(selected_features)} features: {list(selected_features)}")
        
        return X_selected, selected_features
```

---

### Step 6: Create Model Trainer (Day 5)

**File: `src/model_trainer.py`**

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

class ModelTrainer:
    """Trains and evaluates ML models"""
    
    def __init__(self):
        self.models = {}
        self.results = {}
    
    def split_data(self, X, y, test_size=0.2):
        """Split data into train and test sets"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
        print(f"âœ… Split data: Train {len(X_train)}, Test {len(X_test)}")
        return X_train, X_test, y_train, y_test
    
    def train_models(self, X_train, y_train):
        """Train multiple models"""
        # Linear Regression
        lr = LinearRegression()
        lr.fit(X_train, y_train)
        self.models['Linear Regression'] = lr
        
        # Random Forest
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        self.models['Random Forest'] = rf
        
        print("âœ… Trained models")
        return self.models
    
    def evaluate_models(self, X_test, y_test):
        """Evaluate all models"""
        results = {}
        
        for name, model in self.models.items():
            y_pred = model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            results[name] = {
                'MSE': mse,
                'RMSE': np.sqrt(mse),
                'R2': r2
            }
            
            print(f"\n{name}:")
            print(f"  MSE: {mse:.2f}")
            print(f"  RMSE: {np.sqrt(mse):.2f}")
            print(f"  RÂ²: {r2:.4f}")
        
        self.results = results
        return results
    
    def get_best_model(self):
        """Get the best performing model"""
        if not self.results:
            return None
        
        best_name = max(self.results, key=lambda x: self.results[x]['R2'])
        return self.models[best_name], best_name
```

---

### Step 7: Create Pipeline Class (Day 6)

**File: `src/pipeline.py`**

```python
from data_loader import DataLoader
from preprocessor import Preprocessor
from feature_engineer import FeatureEngineer
from model_trainer import ModelTrainer

class MLPipeline:
    """Complete ML Pipeline"""
    
    def __init__(self, data_path):
        self.data_path = data_path
        self.loader = DataLoader(data_path)
        self.preprocessor = Preprocessor()
        self.feature_engineer = FeatureEngineer()
        self.trainer = ModelTrainer()
        self.best_model = None
    
    def run(self, target_column):
        """Run complete pipeline"""
        print("=" * 60)
        print("Starting ML Pipeline")
        print("=" * 60)
        
        # Step 1: Load data
        print("\n[Step 1] Loading data...")
        data = self.loader.load_data()
        self.loader.explore_data()
        
        # Step 2: Preprocess
        print("\n[Step 2] Preprocessing...")
        data = self.preprocessor.handle_missing_values(data)
        
        # Separate features and target
        X = data.drop(columns=[target_column])
        y = data[target_column]
        
        # Encode categorical
        categorical_cols = X.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            X = self.preprocessor.encode_categorical(X, categorical_cols)
        
        # Step 3: Feature engineering
        print("\n[Step 3] Feature engineering...")
        X = self.feature_engineer.create_features(X)
        
        # Step 4: Split data
        print("\n[Step 4] Splitting data...")
        X_train, X_test, y_train, y_test = self.trainer.split_data(X, y)
        
        # Scale features
        X_train_scaled, X_test_scaled = self.preprocessor.scale_features(X_train, X_test)
        
        # Step 5: Train models
        print("\n[Step 5] Training models...")
        self.trainer.train_models(X_train_scaled, y_train)
        
        # Step 6: Evaluate
        print("\n[Step 6] Evaluating models...")
        results = self.trainer.evaluate_models(X_test_scaled, y_test)
        
        # Get best model
        self.best_model, best_name = self.trainer.get_best_model()
        print(f"\nâœ… Best model: {best_name}")
        
        return results
    
    def predict(self, new_data):
        """Make predictions on new data"""
        if self.best_model is None:
            print("âŒ No model trained yet")
            return None
        
        predictions = self.best_model.predict(new_data)
        return predictions
```

---

### Step 8: Create Main Program (Day 7)

**File: `main.py`**

```python
from src.pipeline import MLPipeline

def main():
    # Create pipeline
    pipeline = MLPipeline('data/house_prices.csv')
    
    # Run pipeline
    results = pipeline.run(target_column='price')
    
    print("\n" + "=" * 60)
    print("Pipeline Complete!")
    print("=" * 60)
    print("\nYou can now use pipeline.predict() for new predictions")

if __name__ == "__main__":
    main()
```

---

## ğŸ“ Learning Checklist | Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ¹Ù„Ù…

- [ ] Day 1: Understand ML pipeline concept
- [ ] Day 2: Create data loader
- [ ] Day 3: Create preprocessor
- [ ] Day 4: Create feature engineer
- [ ] Day 5: Create model trainer
- [ ] Day 6: Create pipeline class
- [ ] Day 7: Test complete pipeline
- [ ] Day 8: Add visualizations
- [ ] Day 9: Improve error handling
- [ ] Day 10: Write documentation

---

## ğŸ’¡ Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©

1. **House Price Prediction** - Real estate valuation
2. **Customer Churn Prediction** - Identify customers likely to leave
3. **Sales Forecasting** - Predict future sales
4. **Spam Detection** - Classify emails
5. **Disease Diagnosis** - Medical predictions

---

**Good luck building your ML pipeline!** ğŸš€

