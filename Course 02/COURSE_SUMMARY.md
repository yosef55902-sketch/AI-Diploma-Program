# Course 02 - Course Summary
## ŸÖŸÑÿÆÿµ ÿßŸÑÿØŸàÿ±ÿ©

This document provides a comprehensive text summary of all course materials.
Ÿáÿ∞ÿß ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ ŸäŸàŸÅÿ± ŸÖŸÑÿÆÿµ ŸÜÿµŸä ÿ¥ÿßŸÖŸÑ ŸÑÿ¨ŸÖŸäÿπ ŸÖŸàÿßÿØ ÿßŸÑÿØŸàÿ±ÿ©.

---


## Pdfs



### 02

--- Page 1 ---

Python 4 AI
Introduction
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 2 ---

Textbook ‚ÄòReference ‚Äô
Russell & Norvig, AI: A Modern Approach, 4th Ed.

--- Page 3 ---

Course Mode
Instruction
‚Ä¢Grow knowledge
‚Ä¢Collaborate
‚Ä¢Work until success 
Assessment
‚Ä¢Measure knowledge
‚Ä¢Each student own ‚Äôs work
‚Ä¢Achieve success

--- Page 4 ---

Course Mode (2)
‚Ä¢I am here to help! 
‚Ä¢There are plenty of resources available for you 
‚Ä¢You can always talk to me if you are feeling stressed or missing 
something!
‚Ä¢Collaboration on homework is okay, but please cite 
collaborators

--- Page 5 ---

What is Artificial Intelligence (AI)?
‚Ä¢What is artificial intelligence (AI)? 
‚Ä¢What can AI do? 
‚Ä¢What is this course?

--- Page 6 ---

What is AI ( 2)?
Public View
Text assistants 
(ChatGPT, LLAMA, Claude, etc.)
Image Generation
(Stable Diffusion, Midjourney, etc.)

--- Page 7 ---

What is AI (3)?
Economical View
https://www.precedenceresearch.com/artificial -intelligence -market
638 billion US dollars ‚ò∫
Political View

--- Page 8 ---

What is AI (4)?
Legal View
Bloomberg Law, 2023
New York Times, 2024
The Economist, 2021 New York Times, 2023
 MarketWatch, 2023

--- Page 9 ---

What is AI ( 5)?
Science View
Nature, 2022
Wired, 2022

--- Page 10 ---

What is AI (6)?
Education View
Forbes, 2023
https://www.govtech.com/education/higher -
ed/study -30-of-college -students -have -used-
chatgpt -for-essays

--- Page 11 ---

What is AI ( 7)?
‚Ä¢Public imagination
‚Ä¢Economy 
‚Ä¢Politics 
‚Ä¢Law 
‚Ä¢Labor 
‚Ä¢Sciences 
‚Ä¢Education

--- Page 12 ---

What is AI ( 8)?

--- Page 13 ---

What is AI (9)?
The science of making machines that:
Think like people
Act like peopleThink rationally
Act rationally

--- Page 14 ---

Rational Decisions
We will use the term rational in a very specific, technical way:
‚ñ™Rational: Maximally achieving pre -defined goals
‚ñ™Rationality only concerns what decisions are made 
(not the thought process behind them)
‚ñ™Goals are expressed in terms of the utility of outcomes
‚ñ™Being rational means maximizing your expected utility
Computational Rationality

--- Page 15 ---

Maximize Your
Expected Utility

--- Page 16 ---

What About the Brain?
‚ñ™Brains (human minds) are very good at 
making rational decisions, but not perfect
‚ñ™Brains are not as modular as software , 
so hard to reverse engineer!
‚ñ™‚ÄúBrains are to intelligence as wings are to 
flight ‚Äù
‚ñ™Lessons learned from the brain: Memory 
and simulation are key to decision 
making

--- Page 17 ---

What About the Brain? ( 2)
‚ñ™We cannot yet build AI on the scale of the brain
‚ñ™~100T synapses in the human brain vs ~500B weights in artificial 
neural networks (ANNs)
‚ñ™Still, the brain can be a great inspiration for AI!

--- Page 18 ---

A (Short) History of AI
Thinking machines video 
https://www.youtube.com/watch?v=aygSMgK3BEM

--- Page 19 ---

A (Short) History of AI ( 2)
‚ñ™1940 -1950 : Early days: neural and computer science meet
‚ñ™1943 : McCulloch & Pitts: Boolean circuit model of brain
‚ñ™1950 : Turing's ‚ÄúComputing Machinery and Intelligence ‚Äù
‚ñ™1950 ‚Äî70: Excitement! Logic -driven
‚ñ™1950 s: Early AI programs, including Samuel's checkers program, Newell& 
Simon's Logic Theorist, Gelernter's Geometry Engine
‚ñ™1956 : Dartmouth meeting: ‚ÄúArtificial Intelligence ‚Äù adopted
‚ñ™1965 : Robinson's complete algorithm for logical reasoning
‚ñ™1970 ‚Äî90: Knowledge -based approaches
‚ñ™1969 ‚Äî79: Early development of knowledge -based systems
‚ñ™1980 ‚Äî88: Expert systems industry booms
‚ñ™1988 ‚Äî93: Expert systems industry busts: ‚ÄúAI Winter ‚Äù

--- Page 20 ---

A (Short) History of AI ( 2)
‚ñ™1990 ‚Äî: Statistical approaches
‚ñ™Resurgence of probability, focus on uncertainty
‚ñ™General increase in technical depth
‚ñ™Agents and learning systems ‚Ä¶ ‚ÄúAI Spring ‚Äù?
‚ñ™1996 : Kasparov defeats Deep Blue at chess
‚ñ™1997 : Deep Blue defeats Kasparov at chess
‚ñ™2000 ‚Äî: Where are we now?
‚ñ™Big data, big compute, neural networks
‚ñ™Some re -unification of sub -fields
‚ñ™AI used in many industries
‚ñ™Chess engines running on ordinary laptops can defeat the world ‚Äôs best chess 
players
‚ñ™What can AI do now?
‚ÄúI could feel ---I could smell ---a new kind of 
intelligence across the table.‚Äù ~Kasparov

--- Page 21 ---

What Can AI Do?
Quiz: Which of the following can be done at present?
‚ñ™Win against any human at chess? 
‚ñ™Win against the best humans at Go? 
‚ñ™Play a decent game of table tennis? 
‚ñ™Unload any dishwasher in any home? 
‚ñ™Drive safely along the highway? 
‚ñ™Drive safely along streets of Riyadh? 
‚ñ™Buy a week's worth of groceries on the web? 
‚ñ™Buy a week's worth of groceries at H? 
‚ñ™Discover and prove a new mathematical theorem? 
‚ñ™Perform a surgical operation? 
‚ñ™Translate spoken Chinese into spoken English in real time? 
‚ñ™Win an art competition? 
‚ñ™Write an intentionally funny story? 
‚ñ™Construct a building? 
‚ÄúI could feel ---I could smell ---a new kind of 
intelligence across the table.‚Äù ~Kasparov
Google ‚Äôs AlphaGo beats Lee Sedol at Go in 
2016 
But ‚Ä¶ a plot twist in 2023 !

--- Page 22 ---

What Can AI Do? ( 2)
Quiz: Which of the following can be done at present?
‚ñ™Win against any human at chess? 
‚ñ™Win against the best humans at Go? 
‚ñ™Play a decent game of table tennis? 
‚ñ™Unload any dishwasher in any home? 
‚ñ™Drive safely along the highway? 
‚ñ™Drive safely along streets of Riyadh? 
‚ñ™Buy a week's worth of groceries on the web? 
‚ñ™Buy a week's worth of groceries at Hyperpanda? 
‚ñ™Discover and prove a new mathematical theorem? 
‚ñ™Perform a surgical operation? 
‚ñ™Translate spoken Chinese into spoken English in real time? 
‚ñ™Win an art competition? 
‚ñ™Write an intentionally funny story? 
‚ñ™Construct a building?

--- Page 23 ---

‚ñ™Speech technologies (e.g. Siri)
‚ñ™Automatic speech recognition (ASR)
‚ñ™Text -to-speech synthesis (TTS)
‚ñ™Dialog systems
‚ñ™Language processing technologies
‚ñ™Question answering
‚ñ™Machine translation
‚ñ™Web search
‚ñ™Text classification, spam filtering, etc ‚Ä¶What Can AI Do? ( 3)

--- Page 24 ---

What Can AI Do? (4)
‚ñ™Object and face recognition
‚ñ™Scene segmentation
‚ñ™Image classification

--- Page 25 ---

What Can AI Do? ( 5)
‚ñ™Robotics
‚ñ™Part mech. eng.
‚ñ™Part AI
‚ñ™Reality much harder than
simulations!
‚ñ™Technologies
‚ñ™Vehicles
‚ñ™Rescue
‚ñ™Soccer!
‚ñ™Lots of automation

--- Page 26 ---

Designing Rational Agents
‚ñ™An agent is an entity that perceives and acts.
‚ñ™A rational agent selects actions that maximize its (expected) 
utility .
‚ñ™Characteristics of the percepts , environment , and action
space dictate techniques for selecting rational actions
‚ñ™General AI techniques for a variety of problem types
‚ñ™Learning to recognize when and how a new problem can be solved 
with an existing technique

--- Page 27 ---

Designing Rational Agents ( 2)

--- Page 28 ---

Designing Rational Agents ( 3)

--- Page 29 ---

Designing Rational Agents (4)
Core Components of Rational Agents

--- Page 30 ---

Designing Rational Agents ( 5)
Goal: Find the best plan for to solve a problem

--- Page 31 ---

Designing Rational Agents ( 6)
Goal: Learn to best act in the world

--- Page 32 ---

Designing Rational Agents ( 7)
Goal: Make sense of uncertainty in the world

--- Page 33 ---

Designing Rational Agents (8)
Goal: Learn a model of the world from data

--- Page 34 ---

Designing Rational Agents ( 9)

--- Page 35 ---

Course Topics 
‚Ä¢Part 1:Search Problems
‚Ä¢Depth -First Search
‚Ä¢Breadth -First Search
‚Ä¢Greedy Best -First Search
‚Ä¢A* Search. Minimax
‚Ä¢Alpha -Beta Pruning
‚Ä¢Part 2:Knowledge
‚Ä¢Propositional Logic
‚Ä¢Entailment
‚Ä¢Inference
‚Ä¢Model Checking‚Ä¢Resolution
‚Ä¢First Order Logic
‚Ä¢Part 3: Uncertainty
‚Ä¢Probability
‚Ä¢Conditional Probability
‚Ä¢Random Variables Independence
‚Ä¢Bayes‚Äô Rule
‚Ä¢Joint Probability
‚Ä¢Bayesian Networks
‚Ä¢Sampling
‚Ä¢Markov Models
‚Ä¢Hidden Markov Models

--- Page 36 ---

Course Topics ( 2) 
‚Ä¢Loss Functions
‚Ä¢Overfitting
‚Ä¢Regularization
‚Ä¢Reinforcement Learning
‚Ä¢Markov Decision Processes
‚Ä¢Q-Learning
‚Ä¢Unsupervised Learning
‚Ä¢K-means Clustering‚Ä¢Part 4: Optimization
‚Ä¢Local Search
‚Ä¢Hill Climbing
‚Ä¢Simulated Annealing
‚Ä¢Linear Programming
‚Ä¢Constraint Satisfaction
‚Ä¢Backtracking Search
‚Ä¢Part 5: Search Problems
‚Ä¢Supervised Learning
‚Ä¢Nearest -Neighbor Classification
‚Ä¢Perceptron Learning
‚Ä¢Support Vector Machines Regression

--- Page 37 ---

Course Topics ( 3) 
‚Ä¢Part 6:Artificial Neural 
Networks
‚Ä¢Artificial Neural Networks
‚Ä¢Activation Functions
‚Ä¢Gradient Descent
‚Ä¢Backpropagation
‚Ä¢Overfitting
‚Ä¢TensorFlow/Pytorch 
‚Ä¢Image Convolution
‚Ä¢Convolutional Neural Networks
‚Ä¢Recurrent Neural Networks‚Ä¢Part 7:NLP
‚Ä¢Syntax
‚Ä¢Semantics
‚Ä¢Context -Free Grammar
‚Ä¢Nltk
‚Ä¢N-grams
‚Ä¢Bag-of-Words Model
‚Ä¢Naive Bayes
‚Ä¢Word Representation
‚Ä¢Word 2vec Model
‚Ä¢Attention Mechanism
‚Ä¢Transformers

--- Page 38 ---

Course End 
‚Ä¢Build and understand math of rational, 
learning agents 
‚Ä¢Select and apply the right AI methods for wide 
range of problems 
‚Ä¢Recognize how these methods are used in 
modern AI systems 
‚Ä¢Be prepared to make decisions on how AI is 
used in society

--- Page 39 ---

Python 4 AI
Search Algorithms
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 40 ---

Search Problems
‚Ä¢Search problems involve an agent that is given an initial state
and a goal state .
‚Ä¢Then, the agent returns a solution of how to get from the 
initial state to the goal state . 
‚Ä¢Example: A navigator app uses a typical search process, where 
the agent (the thinking part of the program) receives as input 
your current location and your desired destination, and, based 
on a search algorithm, returns a suggested path.

--- Page 41 ---

Search Problems (2)
Initial state
Goal state
Initial state
 Goal state
Initial state
 Goal state

--- Page 42 ---

Search Problems (3)
‚Ä¢Agent: An entity that perceives its environment and 
acts upon that environment. In a navigator app, for 
example, the agent would be a representation of a 
car that needs to decide on which actions to take to 
arrive at the destination.
‚Ä¢State: A configuration of an agent in its 
environment. For example, in a 15 puzzle, a state is 
any one way that all the numbers are arranged on 
the board.
15-Puzzle

--- Page 43 ---

Search Problems (4)
‚Ä¢Initial State: The state from which the search algorithm starts. 
In a navigator app, that would be the current location.
‚Ä¢Actions: Choices that can be made in a state. More precisely, 
actions can be defined as a function. Upon receiving state s as 
input, Actions(s) returns as output the set of actions that can be 
executed in state s. For example, in a 15 puzzle, the actions of a 
given state are the ways you can slide squares in the current 
configuration (4 if the empty square is in the middle, 3 if next to 
a side, 2 if in the corner).

--- Page 44 ---

Search Problems (5)
‚Ä¢Transition Model: A description of what state results from 
performing any applicable action in any state. More precisely, 
the transition model can be defined as a function. Upon 
receiving state s and action a as input, Results(s, a) returns the 
state resulting from performing action a in state s. For example, 
given a certain configuration of a 15 puzzle (state s), moving a 
square in any direction (action a) will bring to a new 
configuration of the puzzle (the new state).

--- Page 45 ---

Search Problems (6)
‚Ä¢State Space: The set of all states reachable from the initial state 
by any sequence of actions. For example, in a 15 puzzle, the 
state space consists of all the 16!/2 configurations on the board 
that can be reached from any initial state. 
‚Ä¢The state space can be visualized as a directed graph with 
states, represented as nodes , and actions, represented as 
arrows between nodes .

--- Page 46 ---

Search Problems (7)
‚Ä¢Goal Test: The condition that determines whether a given state 
is a goal state. For example, in a navigator app, the goal test 
would be whether the current location of the agent (the 
representation of the car) is at the destination. If it is ‚Äîproblem 
solved. If it is not ‚Äîwe continue searching.
‚Ä¢Path Cost: A numerical cost associated with a given path. For 
example, a navigator app does not simply bring you to your 
goal; it does so while minimizing the path cost, finding the 
fastest way possible for you to get to your goal state.

--- Page 47 ---

Solving Search Problems
‚Ä¢Solution: A sequence of actions that leads from the initial state 
to the goal state.
‚Ä¢Optimal Solution: A solution that has the lowest path cost 
among all solutions.
‚Ä¢In a search process, data is often stored in a node , a data 
structure that contains the following data:
‚Ä¢A state
‚Ä¢Its parent node, through which the current node was generated
‚Ä¢The action that was applied to the state of the parent to get to the 
current node
‚Ä¢The path cost from the initial state to this node

--- Page 48 ---

Solving Search Problems (2)
‚Ä¢Important Note 1: Nodes contain information that makes them very 
useful for the purposes of search algorithms. They contain a state, 
which can be checked using the goal test to see if it is the final state .
‚Ä¢If it is, the node‚Äôs path cost can be compared to other nodes‚Äô path 
costs, which allows choosing the optimal solution. Once the node is 
chosen, by virtue of storing the parent node and the action that led 
from the parent to the current node, it is possible to trace back every 
step of the way from the initial state to this node, and this sequence 
of actions is the solution.

--- Page 49 ---

Solving Search Problems (3)
‚Ä¢Important Note 2: However, nodes are simply a data structure ‚Äî
they do not search, they hold information. To actually search, we use 
the frontier (orfringe ), the mechanism that ‚Äúmanages‚Äù the nodes.
‚Ä¢The frontier starts by containing an initial state and an empty set of 
explored items, and then repeats the following actions until a solution 
is reached:
‚Ä¢Repeat:
1. If the frontier is empty
‚ñ™Stop. There is no solution to the problem.
2. Remove a node from the frontier. This is the node that will be considered.
3. If the node contains the goal state,
‚ñ™Return the solution. Stop.

--- Page 50 ---

Solving Search Problems (4)
4. Else:
‚ñ™Expand the node (find all the new nodes that could be reached from this 
node), and add resulting nodes to the frontier.
‚ñ™Add the current node to the explored set.
State (k)
State (k+1) State (k+1)
State (k+2)
 State (k+2)State (k+2)
 State (k+2)Which way to go?

--- Page 51 ---

Solving Search Problems (5)
????

--- Page 52 ---

Solving Search Problems: Depth -First Search (DFS)
Note: In the previous description of the frontier, one thing went 
unmentioned: The node removal strategy ‚òπ
Question: Which node should be removed? 
Answer: This choice has implications on the quality of the solution and 
how fast it is achieved. There are multiple ways to go about the question 
of which nodes should be considered first, two of which can be 
represented by the data structures of stack (in depth -first search ) and 
queue (in breadth -first search ).

--- Page 53 ---

Solving Search Problems: DFS (2)

--- Page 54 ---

Solving Search Problems: DFS (3)
A depth -first search (DFS) algorithm exhausts each one direction before 
trying another direction. 
In these cases, the frontier is managed as a stack data structure (last-in 
first-out ‚ÄìLIFO ). 
After nodes are being added to the frontier, the first node to remove and 
consider is the last one to be added. 
Note: This results in a search algorithm that goes as deep as possible in the 
first direction that gets in its way while leaving all other directions for later.

--- Page 55 ---

Solving Search Problems: DFS (4)
‚Ä¢Pros:
‚ñ™At best, this algorithm is the fastest. 
‚ñ™If it ‚Äúlucks out‚Äù and always chooses the right path to the solution 
(by chance), then depth -first search takes the least possible time to 
get to a solution.
‚Ä¢Cons:
‚ñ™It is possible that the found solution is not optimal.
‚ñ™At worst, this algorithm will explore every possible path before 
finding the solution, thus taking the longest possible time before 
reaching the solution.

--- Page 56 ---

Solving Search Problems: DFS (5)
defremove(self):
ifself.empty():
raiseException( ‚ÄúEmpty frontier" )
else:
node = self.frontier[ -1]
self.frontier = self.frontier[: -1]
returnnodeLast node retrieved from frontier
Updated frontier

--- Page 57 ---

Solving Search Problems: Breadth -First Search (BFS)
A breadth -first search (BFS) algorithm will follow multiple directions at the 
same time, taking one step in each possible direction before taking the 
second step in each direction. 
In this case, the frontier is managed as a queue data structure (first-in 
first-out ‚ÄìFIFO ). 
In this case, all the new nodes add up in line, and nodes are being 
considered based on which one was added first (first come first served!). 
This results in a search algorithm that takes one step in each possible 
direction before taking a second step in any one direction.

--- Page 58 ---

Solving Search Problems: BFS (2)
‚Ä¢Pros:
‚ñ™This algorithm is guaranteed to find the optimal solution. 
‚Ä¢Cons:
‚ñ™This algorithm is almost guaranteed to take longer than the 
minimal time to run.
‚ñ™At worst, this algorithm takes the longest possible time to run.

--- Page 59 ---

Solving Search Problems: BFS (3)
defremove(self):
ifself.empty():
raiseException( "empty frontier" )
else:
node = self.frontier[ 0]
self.frontier = self.frontier[ 1:]
returnnode

--- Page 60 ---

Solving Search Problems: Practical Examples
‚Ä¢State space: 
‚ñ™Cities 
‚Ä¢Successor function: 
‚ñ™Roads: Go to adjacent city with cost = distance 
‚Ä¢Initial state: 
‚ñ™Arad ‚Ä¢Goal state: 
‚ñ™Bucharest
‚Ä¢Goal test: 
‚ñ™Is state == Bucharest? 
‚Ä¢Solution
‚ñ™????Initial state
Goal state

--- Page 61 ---

Solving Search Problems: Practical Examples (2)
‚Ä¢State space: 
‚ñ™States: {(x, y), dot Booleans} 
‚Ä¢Successor function: 
‚ñ™Update location and possibly a dot Boolean
‚Ä¢Initial state: 
‚ñ™Pacman start maze ‚Ä¢Goal state: 
‚ñ™All dots set to false
‚Ä¢Goal test: 
‚ñ™No more dots
‚Ä¢Solution
‚ñ™????Initial state Goal state

--- Page 62 ---

Solving Search Problems: Practical Examples (3)
‚Ä¢World (or environment) state:
‚ñ™Agent (Pacman) positions:
‚ñ™Food presence:
‚ñ™Ghost positions:
‚ñ™Agent (Pacman) actions: 
‚Ä¢Dimensions?
‚ñ™World states:
‚ñ™States for pathing: 
‚ñ™States for eat -all-dots?120 
30 Booleans
12 
‚Üë, ‚Üê, ‚Üí, ‚Üì
120x(230)x(122)x4
120 
120x(230) 
Note: 230= ?? 230= (210)3
210= 1024 ‚âà 1000 = 103ü°∫230‚âà 
(103)3
ü°∫230‚âà 1096.9 trillion states 
‚òπ‚òπ‚òπ

--- Page 63 ---

Solving Search Problems: Practical Examples (4)
Problem Statement: Eat all dots while keeping the ghosts perma -
scared
State Space: Agent position, dot Booleans, power pellet Booleans, 
remaining scared time

--- Page 64 ---

Solving Search Problems: Practical Examples (5)
‚Ä¢State space graph: A mathematical 
representation of a search problem 
‚ñ™Nodes are (abstracted) world configurations
‚ñ™Arcs represent successors (action results) 
‚ñ™The goal test is a set of goal nodes (maybe 
only one) 
‚Ä¢In a state space graph, each state occurs 
only once ! 
Note: We can rarely build this full graph in 
memory (it‚Äôs too big), but it is a useful idea
Tiny state space graph for a tiny search problem

--- Page 65 ---

Solving Search Problems: Practical Examples (6)
Current state
Future statesCostAction‚Ä¢A search tree:
‚ñ™A ‚Äúwhat if ‚Äù tree of plans and their outcomes 
‚ñ™The start state is the root node
‚ñ™Children correspond to successors
‚ñ™Nodes show states, but correspond to 
PLANS that achieve those states For most problems, we 
can never actually build 
the whole tree
Note: AlphaGo algorithm used Monte Carlo tree search (MCTS)!

--- Page 66 ---

Solving Search Problems: Practical Examples (7)
‚Ä¢Each node in in the search tree is an entire path in the state space 
graph.
‚Ä¢We construct both on demand ‚Äìand we construct as little as possible.
Lots of repeated structure in the search tree!

--- Page 67 ---

Solving Search Problems: Practical Examples (8)
Search Solution
1.Expand out potential plans (tree nodes) 
2.Maintain a frontier (or fringe) of partial 
plans under consideration
3.Try to expand as few tree nodes as 
possible

--- Page 68 ---

General Tree Search Solution
Important ideas:
1.Frontier (or fringe)
2.Expansion 
3.Exploration strategywhich fringe 
nodes to explore

--- Page 69 ---

General Tree Search Solution (2)
s 
s ‚Üí d
s ‚Üí e
s ‚Üí p
s ‚Üí d ‚Üí b
s ‚Üí d ‚Üí c
s ‚Üí d ‚Üí e
s ‚Üí d ‚Üí e ‚Üí h
s ‚Üí d ‚Üí e ‚Üí r
s ‚Üí d ‚Üí e ‚Üí r ‚Üí fs ‚Üí d ‚Üí e ‚Üí r ‚Üí f ‚Üí c
s ‚Üí d ‚Üí e ‚Üí r ‚Üí f ‚Üí G
G ????Got you ‚ò∫

--- Page 70 ---

‚Ä¢BFS and DFS algorithms are uninformed search algorithms . 
‚Ä¢These algorithms do not utilize any knowledge about the problem that 
they did not acquire through their own exploration. 
‚Ä¢However, most often is the case that some knowledge about the 
problem is, in fact, available. 
‚Ä¢Example: When a human maze -solver enters a junction, the human 
can see which way goes in the general direction of the solution and 
which way does not. Greedy Best -First Search

--- Page 71 ---

‚Ä¢AI can do the same. A type of algorithm that considers additional 
knowledge to try to improve its performance is called 
aninformed search algorithm.
‚Ä¢Greedy best -first search expands the node that is the closest to the 
goal, as determined by a heuristic function h(n). 
‚Ä¢As its name suggests, the function estimates how close to the goal the 
next node is, but it can be mistaken . Greedy Best -First Search (2)

--- Page 72 ---

‚Ä¢The efficiency of the greedy best -firstalgorithm depends on how good 
the heuristic function is. 
‚Ä¢Example: In a maze, an algorithm can use a heuristic function that 
relies on the Manhattan distance between the possible nodes and the 
end of the maze. 
‚Ä¢TheManhattan distance ignores walls and counts how many steps 
up, down, or to the sides it would take to get from one location to the 
goal location. This is an easy estimation that can be derived based on 
the (x, y) coordinates of the current location and the goal location.Greedy Best -First Search (3)

--- Page 73 ---

‚Ä¢However, it is important to emphasize that, as with any heuristic, it 
can go wrong and lead the algorithm down a slower path than it would 
have gone otherwise. 
‚Ä¢It is possible that an uninformed search algorithm will provide a 
better solution faster, but it is less likely to do so than an informed 
algorithm ‚òπGreedy Best -First Search (4)

--- Page 74 ---

Greedy Best -First Search (5)
Manhattan distance
Wall ‚òπ

--- Page 75 ---

Greedy Best -First Search (6)

--- Page 76 ---

Greedy Best -First Search (7)
Heuristic function h(n)

--- Page 77 ---

A*Search Algorithm
‚Ä¢A development of the greedy best -firstalgorithm, A* 
search considers not only h(n), the estimated cost from the current 
location to the goal , but also g(n), the cost that was accrued until 
the current location . 
‚Ä¢By combining both these values, the algorithm has a more accurate 
way of determining the cost of the solution and optimizing its choices 
on the go.

--- Page 78 ---

A*Search Algorithm (2)
‚Ä¢The algorithm keeps track of (cost of path until now +estimated 
cost to the goal) , and once it exceeds the estimated cost of some 
previous option, the algorithm will ditch the current path and go back 
to the previous option, thus preventing itself from going down a long, 
inefficient path that h(n) erroneously marked as best.
‚Ä¢Yet again, since this algorithm, too, relies on a heuristic, it is as good 
as the heuristic that it employs . 
‚Ä¢It is possible that in some situations it will be less efficient than 
greedy best -first search or even the uninformed algorithms ‚òπ

--- Page 79 ---

A*Search Algorithm (3)
‚Ä¢For A* search to be optimal, the heuristic function, h(n), should be:
‚ñ™Admissible (i.e., never overestimating the true cost)
‚ñ™Consistent which means that the estimated path cost to the goal of 
a new node in addition to the cost of transitioning to it from the 
previous node is greater or equal to the estimated path cost to the 
goal of the previous node. 
‚ñ™To put it in an equation form, h(n) is consistent if for every node n
and successor node n‚Ä≤with step cost c: h(n) ‚â§ h(n‚Ä≤) + c .

--- Page 80 ---

A*Search Algorithm (4)

--- Page 81 ---

Introduction to
Artificial Intelligence
with Python (Python 4 AI)
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti 2019 @gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 82 ---

Artificial Intelligence

--- Page 83 ---

SearchO
XX
OX

--- Page 84 ---

1234
5678
9101112
131415

--- Page 87 ---

Search Problems

--- Page 88 ---

Agent
Entity that perceives its environment
and acts upon that environment

--- Page 89 ---

State
A configuration of the agent and
its environment

--- Page 90 ---

57 42
111 38
1210 6 14
15139
42 9 12
314 78
101161
15 135
10 3 4 15
1112 1 13
14 7 59
2 86
State s1 State sk‚Ä¶.. ‚Ä¶..
State sk+1000

--- Page 91 ---

Initial (start) state
The state in which the agent begins

--- Page 92 ---

Initial state
57 42
111 38
1210 6 14
15139

--- Page 93 ---

Actions
Choices that can be made in a state

--- Page 94 ---

Actions
ACTIONS (s)returns the set of actions that can 
be executed in state s

--- Page 95 ---

Actions1 2
3 4

--- Page 96 ---

Actions1 2
3

--- Page 97 ---

Actions11

--- Page 98 ---

Actions1 2
3 44

--- Page 99 ---

Actions2
3 4 33

--- Page 100 ---

Transition model
A description of what state results from 
performing any applicable action in any state

--- Page 101 ---

Transition model
RESULT (s, a)returns the state resulting from 
performing action a in state s

--- Page 102 ---

RESULT (                             ,       ) 
57 42
111 38
12106 14
15139
57 42
111 38
12106 14
15 139
RESULT (                             ,     ) 
57 42
111 38
12106 14
15139
57 42
111 38
12106 14
15139

--- Page 103 ---

RESULT (                             ,     )  
57 42
111 38
12106 14
15139
57 42
111 38
12106 14
15139
57 42
111 38
12106 14
15139RESULT (                             ,       ) 
57 42
111 38
12106 14
15 139

--- Page 104 ---

State space
The set of all states reachable from the initial 
state by any sequence of actions

--- Page 105 ---

2457
83111
14 61012
91315
2457
83111
14 61012
913 15
2457
83111
14 610
12 91315
2457
83111
14 6
1012
913 15
2457
83111
14 61012
9 1315
2457
83111
14 6 10
12 91315
2457
831
11 14 610
12 91315

--- Page 107 ---

Goal test
A mechanism to determine whether a given 
state
is a goal state

--- Page 108 ---

Path cost
Numerical cost associated with a given path

--- Page 109 ---

C D
H
MIE
JKA
B
F G
L

--- Page 110 ---

C D
H
MIE
JKA
B
F G
L2
1
3 2
214
2
3 435
6
2 4Cost for moving from 
node C to node A

--- Page 111 ---

C D
H
MIE
JKA
B
F G
L1
1
1 1
111
1
1 111
1
1 1
Constant Costs ‚ò∫

--- Page 112 ---

Search Problems
‚Ä¢Initial (start) state
‚Ä¢Actions
‚Ä¢Transition model
‚Ä¢Goal test
‚Ä¢Path cost function

--- Page 113 ---

Solution
A sequence of actions that leads from the initial 
state to a goal state

--- Page 114 ---

Optimal solution
A solution that has the lowest path cost among 
all solutions

--- Page 115 ---

Node
A data structure that keeps track of:
-A state
-A parent (node that generated this node)
-An action (action applied to parent to get node)
-A path cost (from initial state to node)

--- Page 116 ---

Approach
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the frontier.

--- Page 117 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial 
state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the 
solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.A

--- Page 118 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.A

--- Page 119 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.B

--- Page 120 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.B

--- Page 121 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.C D

--- Page 122 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.C D

--- Page 123 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.D
E

--- Page 124 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.D E

--- Page 125 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.D

--- Page 126 ---

EA
B
C D
FFrontierFind a path from A to 
E.
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Expand node, add resulting nodes to the 
frontier.D

--- Page 127 ---

What could go wrong?

--- Page 128 ---

EA
B
C D
FFrontierFind a path from A to 
E. A

--- Page 129 ---

EA
B
C D
FFrontierFind a path from A to 
E.
A

--- Page 130 ---

EA
B
C D
FFrontierFind a path from A to 
E.
B

--- Page 131 ---

EA
B
C D
FFrontierFind a path from A to 
E.
B

--- Page 132 ---

EA
B
C D
FFrontierFind a path from A to 
E. A
C D

--- Page 133 ---

EA
B
C D
FFrontierFind a path from A to 
E.
A C D

--- Page 134 ---

EA
B
C D
FFrontierFind a path from A to 
E.
C D

--- Page 135 ---

Revised Approach
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Start with an empty explored set .
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Add the node to the explored set.
‚Ä¢Expand node, add resulting nodes to the frontier if they 
aren't already in the frontier or the explored set.

--- Page 136 ---

Revised Approach
‚Ä¢Start with a frontier that contains the initial state.
‚Ä¢Start with an empty explored set .
‚Ä¢Repeat:
‚Ä¢If the frontier is empty, then no solution.
‚Ä¢Remove a node from the frontier.
‚Ä¢If node contains goal state, return the solution.
‚Ä¢Add the node to the explored set.
‚Ä¢Expand node, add resulting nodes to the frontier if they 
aren't already in the frontier or the explored set.

--- Page 137 ---

Stack
Last-in first -out (LIFO) data type

--- Page 138 ---

EA
B
C D
FFrontierFind a path from A to 
E. A
Explored Set

--- Page 139 ---

EA
B
C D
FFrontierFind a path from A to 
E.
A
Explored Set

--- Page 140 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetB
A

--- Page 141 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetB
A

--- Page 142 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetC D
B A

--- Page 143 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetC D
B A

--- Page 144 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetC
FB A D

--- Page 145 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetC F
B A D

--- Page 146 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetC
B A D F

--- Page 147 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
EB A D F C

--- Page 148 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetE
B A D F C

--- Page 149 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
B A D F C

--- Page 150 ---

Depth -first search (DFS)
A search algorithm that always expands the 
deepest node in the frontier

--- Page 151 ---

Breadth -first search (BFS)
A search algorithm that always expands the 
shallowest node in the frontier

--- Page 152 ---

Queue
First-in first -out (FIFO) data type

--- Page 153 ---

EA
B
C D
FFrontierFind a path from A to 
E. A
Explored Set

--- Page 154 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored SetA

--- Page 155 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
AB

--- Page 156 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
AB

--- Page 157 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
A BC D

--- Page 158 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
A BC D

--- Page 159 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
A B CD
E

--- Page 160 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
A B CD E

--- Page 161 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
A B C DE
F

--- Page 162 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
A B C DE F

--- Page 163 ---

EA
B
C D
FFrontierFind a path from A to 
E.
Explored Set
A B C DF

--- Page 164 ---

B
ADepth -First 
Search

--- Page 165 ---

B
ADepth -First 
Search

--- Page 166 ---

B
ADepth -First 
Search

--- Page 167 ---

B
ADepth -First 
Search

--- Page 168 ---

B
ADepth -First 
Search

--- Page 169 ---

B
ADepth -First 
Search

--- Page 170 ---

B
ADepth -First 
Search

--- Page 171 ---

B
ADepth -First 
Search

--- Page 172 ---

B
ADepth -First 
Search

--- Page 173 ---

B
ADepth -First 
Search

--- Page 174 ---

B
ADepth -First 
Search

--- Page 175 ---

B
ADepth -First 
Search

--- Page 176 ---

B
ADepth -First 
Search

--- Page 177 ---

B
ADepth -First 
Search

--- Page 178 ---

B
ADepth -First 
Search

--- Page 179 ---

B
ADepth -First 
Search

--- Page 180 ---

B
ADepth -First 
Search

--- Page 181 ---

B
ADepth -First 
Search

--- Page 182 ---

B
ADepth -First 
Search

--- Page 183 ---

B
ADepth -First 
Search

--- Page 184 ---

B
ADepth -First 
Search

--- Page 185 ---

B
ADepth -First 
Search

--- Page 186 ---

B
ADepth -First 
Search

--- Page 187 ---

B
ADepth -First 
Search

--- Page 188 ---

B
ADepth -First 
Search

--- Page 189 ---

B
ADepth -First 
Search

--- Page 190 ---

B
ADepth -First 
Search

--- Page 191 ---

B
ADepth -First 
Search

--- Page 192 ---

B
ADepth -First 
Search

--- Page 193 ---

B
ADepth -First 
Search

--- Page 194 ---

B
ADepth -First 
Search

--- Page 195 ---

B
ADepth -First 
Search

--- Page 196 ---

B
ADepth -First 
Search

--- Page 197 ---

B
ADepth -First 
Search

--- Page 198 ---

B
ADepth -First 
Search

--- Page 199 ---

B
ADepth -First 
Search

--- Page 200 ---

B
ADepth -First 
Search

--- Page 201 ---

B
ADepth -First 
Search

--- Page 202 ---

B
ADepth -First 
Search

--- Page 203 ---

B
ADepth -First 
Search

--- Page 204 ---

B
ADepth -First 
Search

--- Page 205 ---

B
ADepth -First 
Search

--- Page 206 ---

B
ADepth -First 
Search

--- Page 207 ---

B
ADepth -First 
Search

--- Page 208 ---

B
ADepth -First 
Search

--- Page 209 ---

B
ADepth -First 
Search

--- Page 210 ---

B
ADepth -First 
Search

--- Page 211 ---

B
ADepth -First 
Search

--- Page 212 ---

B
ABreadth -First 
Search

--- Page 213 ---

B
ABreadth -First 
Search

--- Page 214 ---

B
ABreadth -First 
Search

--- Page 215 ---

B
ABreadth -First 
Search

--- Page 216 ---

B
ABreadth -First 
Search

--- Page 217 ---

B
ABreadth -First 
Search

--- Page 218 ---

B
ABreadth -First 
Search

--- Page 219 ---

B
ABreadth -First 
Search

--- Page 220 ---

B
ABreadth -First 
Search

--- Page 221 ---

B
ABreadth -First 
Search

--- Page 222 ---

B
ABreadth -First 
Search

--- Page 223 ---

B
ABreadth -First 
Search

--- Page 224 ---

B
ABreadth -First 
Search

--- Page 225 ---

B
ABreadth -First 
Search

--- Page 226 ---

B
ABreadth -First 
Search

--- Page 227 ---

B
ABreadth -First 
Search

--- Page 228 ---

B
ABreadth -First 
Search

--- Page 229 ---

B
ABreadth -First 
Search

--- Page 230 ---

B
ABreadth -First 
Search

--- Page 231 ---

B
ABreadth -First 
Search

--- Page 232 ---

B
ABreadth -First 
Search

--- Page 233 ---

B
ABreadth -First 
Search

--- Page 234 ---

B
ABreadth -First 
Search

--- Page 235 ---

B
ABreadth -First 
Search

--- Page 236 ---

B
ABreadth -First 
Search

--- Page 237 ---

B
ABreadth -First 
Search

--- Page 238 ---

B
ABreadth -First 
Search

--- Page 239 ---

B
ABreadth -First 
Search

--- Page 240 ---

B
ABreadth -First 
Search

--- Page 241 ---

B
ABreadth -First 
Search

--- Page 242 ---

B
ABreadth -First 
Search

--- Page 243 ---

B
ABreadth -First 
Search

--- Page 244 ---

B
ABreadth -First 
Search

--- Page 245 ---

B
ABreadth -First 
Search

--- Page 246 ---

B
ABreadth -First 
Search

--- Page 247 ---

B
ABreadth -First 
Search

--- Page 248 ---

B
ABreadth -First 
Search

--- Page 249 ---

B
ABreadth -First 
Search

--- Page 250 ---

B
ABreadth -First 
Search

--- Page 251 ---

B
ABreadth -First 
Search

--- Page 252 ---

B
ABreadth -First 
Search

--- Page 253 ---

B
ABreadth -First 
Search

--- Page 254 ---

B
ABreadth -First 
Search

--- Page 255 ---

B
ABreadth -First 
Search

--- Page 256 ---

B
ABreadth -First 
Search

--- Page 257 ---

B
ABreadth -First 
Search

--- Page 258 ---

B
ABreadth -First 
Search

--- Page 259 ---

B
ABreadth -First 
Search

--- Page 260 ---

B
ABreadth -First 
Search

--- Page 261 ---

B
ABreadth -First 
Search

--- Page 262 ---

B
ABreadth -First 
Search

--- Page 263 ---

B
ABreadth -First 
Search

--- Page 264 ---

B
ABreadth -First 
Search

--- Page 265 ---

B
ABreadth -First 
Search

--- Page 266 ---

B
ABreadth -First 
Search

--- Page 267 ---

B
ABreadth -First 
Search

--- Page 268 ---

B
ABreadth -First 
Search

--- Page 269 ---

B
ABreadth -First 
Search

--- Page 270 ---

B
ABreadth -First 
Search

--- Page 271 ---

uninformed search
search strategy that uses no problem -specific 
knowledge

--- Page 272 ---

Informed search
A search strategy that uses problem -specific 
knowledge to find solutions more efficiently

--- Page 273 ---

Greedy best -first search
A search algorithm that expands the node that is 
closest to the goal, as estimated by a heuristic 
function h(n)

--- Page 274 ---

B
AHeuristic 
function?

--- Page 275 ---

B
D
C
AHeuristic 
function?

--- Page 276 ---

B
D
C
AHeuristic 
function?Manhattan distance

--- Page 277 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 278 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 279 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 280 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 281 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 282 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 283 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 284 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 285 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 286 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 287 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 288 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 289 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 290 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 291 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 292 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 293 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 294 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 295 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 296 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 297 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 298 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 299 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 300 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 301 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 302 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 303 ---

11 9 7 3 2 B
12 10 8 7 6 4 1
13 12 11 9 7 6 5 2
13 10 8 6 3
14 13 12 11 9 7 6 5 4
13 10
A16 15 14 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 304 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 12 10 9 8 7 6 4
13 11 5
A16 15 14 12 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 305 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 12 10 9 8 7 6 4
13 11 5
A16 15 14 12 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 306 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 12 10 9 8 7 6 4
13 11 5
A16 15 14 12 11 10 9 8 7 6Greedy Best -First 
Search

--- Page 307 ---

A* search
A search algorithm that expands node with 
lowest value of g(n)+ h(n)
g(n)= Cost to reach node
h(n)= Estimated cost to goal

--- Page 308 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 12 10 9 8 7 6 4
13 11 5
A16 15 14 12 11 10 9 8 7 6A* Search

--- Page 309 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 12 10 9 8 7 6 4
13 11 5
A 1+16 15 14 12 11 10 9 8 7 6A* Search

--- Page 310 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 12 10 9 8 7 6 4
13 11 5
A 1+16 2+15 14 12 11 10 9 8 7 6A* Search

--- Page 311 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 12 10 9 8 7 6 4
13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 312 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 313 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 314 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 10 9 8 7 6 5 4 2
13 6+11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 315 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 9 8 7 6 5 4 2
13 6+11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 316 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 8 7 6 5 4 2
13 6+11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 317 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 7 6 5 4 2
13 6+11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 318 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 6 5 4 2
13 6+11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 319 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 11+6 5 4 2
13 6+11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 320 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 11+6 12+5 4 2
13 6+11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 321 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
13 6+11 5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 322 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
13 6+11 14+5 3
14 13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 323 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
13 6+11 14+5 3
14 6+13 5+12 10 9 8 7 6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 324 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
13 6+11 14+5 3
14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 325 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 326 ---

10 9 8 7 6 5 4 3 2 1B
11 1
12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 327 ---

10 9 8 7 6 5 4 3 2 1B
11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 328 ---

10 9 8 7 6 5 4 3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 329 ---

11+10 9 8 7 6 5 4 3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 330 ---

11+10 12+9 8 7 6 5 4 3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 331 ---

11+10 12+9 13+8 7 6 5 4 3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 332 ---

11+10 12+9 13+8 14+7 6 5 4 3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 333 ---

11+10 12+9 13+8 14+7 15+6 5 4 3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 334 ---

11+10 12+9 13+8 14+7 15+6 16+5 4 3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 335 ---

11+10 12+9 13+8 14+7 15+6 16+5 17+4 3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 336 ---

11+10 12+9 13+8 14+7 15+6 16+5 17+4 18+3 2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 337 ---

11+10 12+9 13+8 14+7 15+6 16+5 17+4 18+3 19+2 1B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 338 ---

11+10 12+9 13+8 14+7 15+6 16+5 17+4 18+3 19+2 20+1 B
10+11 1
9+12 7+10 8+9 9+8 10+7 11+6 12+5 13+4 2
8+13 6+11 14+5 3
7+14 6+13 5+12 10 9 8 7 15+6 4
4+13 11 5
A 1+16 2+15 3+14 12 11 10 9 8 7 6A* Search

--- Page 339 ---

A* search
Optimal if:
-h(n)is admissible (never overestimates the true 
cost)
-h(n)is consistent (for every node nand successor 
n' with step cost c, h(n) ‚â§ h(n') + c )

--- Page 340 ---

Adversarial Search

--- Page 343 ---

Minimax

--- Page 344 ---

OXX
OO
OXXXOX
OOX
XXOO X
XO
XOX
1-
10

--- Page 345 ---

Minimax
‚Ä¢MAX (X) aims to maximize score.
‚Ä¢MIN (O) aims to minimize score.

--- Page 346 ---

Game
‚Ä¢S0: initial state
‚Ä¢PLAYER (s): Returns which player to move in state s
‚Ä¢ACTIONS (s): Returns legal moves in state s
‚Ä¢RESULT (s, a): Returns state after action a taken in state s
‚Ä¢TERMINAL (s): Checks if state s is a terminal state
‚Ä¢UTILITY (s): Final numerical value for terminal state s

--- Page 347 ---

Initial State

--- Page 348 ---

PLAYER (s)
PLAYER ( ) =X
X PLAYER ( ) =O

--- Page 349 ---

ACTIONS (s)
XO
OXX
X OACTIONS ( ) ={       ,       }O
O

--- Page 350 ---

RESULT (s, a)
XO
OXX
X ORESULT ( ,         ) =O OXO
OXX
X O

--- Page 351 ---

TERMINAL (s)
O
OX
XOXTERMINAL ( ) =false
O X
OX
XOXTERMINAL ( ) =true

--- Page 352 ---

UTILITY (s)
O X
OX
XOXUTILITY ( ) =1
OXX
XO
OXOUTILITY ( ) =-1

--- Page 353 ---

VALUE : 
1OXO
OXX
XXO

--- Page 354 ---

XO
OXX
X OPLAYER (s) 
= O
OXO
OXX
X OXO
OXX
XOOMIN-VALUE :
0
OXO
OXX
XXOXXO
OXX
XOOVALUE :
0VALUE :
1MAX-VALUE :
1MAX-VALUE :
0

--- Page 355 ---

XO
OXX
X O
OXO
OXX
X OXO
OXX
XOOMIN-VALUE :
0
OXO
OXX
XXOXXO
OXX
XOOVALUE :
0VALUE :
1MAX-VALUE :
1MAX-VALUE :
0PLAYER (s) 
= O

--- Page 356 ---

PLAYER (s) 
= X
XO
OXX
X O
OXO
OXX
X OXO
OXX
XOOMIN-VALUE :
0
OXO
OXX
XXOXXO
OXX
XOOVALUE :
0VALUE :
1MAX-VALUE :
1MAX-VALUE :
0XO
OX
X O
XXO
OX
X O
XXO
OXO
X OXXO
OX
XOOMIN-VALUE :
-1
XXO
OXX
XOOVALUE :
0VALUE :
-1MAX-VALUE :
0XO
OX
XXOVALUE :
1MAX-VALUE :
1

--- Page 357 ---

9
5 3 9

--- Page 358 ---

9
5 3 9 2 888

--- Page 359 ---

Minimax
‚Ä¢Given a state s:
‚Ä¢MAX picks action ain ACTIONS (s)that produces highest 
value of MIN-VALUE (RESULT (s, a))
‚Ä¢MIN picks action ain ACTIONS (s)that produces smallest 
value of MAX-VALUE (RESULT (s, a))

--- Page 360 ---

Minimax
function MAX-VALUE (state ):
if TERMINAL (state ):
return UTILITY (state )
v= -‚àû
for action in ACTIONS (state ):
v= MAX(v, MIN-VALUE (RESULT (state , 
action )))
return v

--- Page 361 ---

Minimax
function MIN-VALUE (state ):
if TERMINAL (state ):
return UTILITY (state )
v= ‚àû
for action in ACTIONS (state ):
v= MIN(v, MAX-VALUE (RESULT (state , 
action )))
return v

--- Page 362 ---

Optimizations

--- Page 363 ---

4
5 2
2 6 43
9 7 34
4 5 8

--- Page 364 ---

4
5‚â§
2
2‚â§
3
9 34
4 5 8

--- Page 365 ---

Alpha -Beta Pruning

--- Page 366 ---

Total possible Tic -Tac-Toe games

--- Page 367 ---

288,000,000,000
Total possible chess games
after four moves each

--- Page 368 ---

Total possible chess games
(lower bound)29000

--- Page 369 ---

Depth -Limited Minimax

--- Page 370 ---

Evaluation function
Function that estimates the expected utility of 
the game from a given state

--- Page 372 ---

https://xkcd.com/832/

--- Page 373 ---

Linear Image Classifiers
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 374 ---

Linear Image Classifiers
Input Image
CIFAR 10Image Dataset:
Training: 50000 32x32x3 images
Testing: 10000 32x32x3 images
K: Number of classes = 10airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck Linear Image 
Classifier ( W, b)s1
s2
‚Ä¶
‚Ä¶
s10
si: Probability that input image belongs to class i
s1: Probability that input image belongs to ‚Ä≤airplane‚Ä≤ class
s2: Probability that input image belongs to ‚Ä≤automobile‚Ä≤ class
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
s10: Probability that input image belongs to ‚Ä≤truck‚Ä≤ classs = f(x, W, 
b)10x30723072x1
10x1
10x1
 10x30733073x1
s = f(x, W)

--- Page 375 ---

Linear Image Classifiers (2)
Linear Image Classifiers = f(x, W) = W . xs1
s2
‚Ä¶
‚Ä¶
s10Input Imagex
s = f(Œ±.x, W) = Œ±.W.x
Input ImageŒ±.x
Œ±.s1
Œ±. s2
‚Ä¶
‚Ä¶
Œ±. s10Œ±=0.5

--- Page 376 ---

Linear Image Classifiers (3)
Linear Image Classifiers = f(x, W) = W. xs1
s2
‚Ä¶
‚Ä¶
s10Input Imagex
3073x1
10x307310x1
W
xss1: Probability that x ‚àà‚Ä≤airplane‚Ä≤W1,:s1= <W1,:, x>Dot product!
s2= <W2,:, x>
s3= <W3,:, x>
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
s10= <W10,:, x>
airplane
 automobile
 bird
 cat
 deerdog                            frog                          horse                       ship                        truck
???

--- Page 377 ---

Linear Image Classifiers (4)
s1= <W1,:, x>
Equation of a hyperplane in the ‚Ñú3073space! 
w1,1
w1,2
‚Ä¶
‚Ä¶
w3073,1s2= <W2,:, x>
s3= <W3,:, x>
airplaneautomobile
bird
Hyperplanes carving up a high -dimensional space!

--- Page 378 ---

Linear Image Classifiers (5)
w2
w1
Class 2Class 2 Class 1
Class 1
w2
w1Class 1
Class 1Class 1
Class 1
w2
w1
Class 2Class 2Class 1
Class 1Class 2Class 1
Class 1
Linearly non -separable data!
XOR Problem
Perceptron Machine
 ‚Ä≤Perceptron‚Ä≤ Book

--- Page 379 ---

Linear Image Classifiers (6)
xAlgebraic Viewpoint Visual 
Viewpoint
f(x, W, b) = W.x + 
b
10 classes 
ü°∫10 
templatesGeometric Viewpoint
Hyperplanes cutting up 
high -dimensional space

--- Page 380 ---

Linear Image Classifiers (7)
w2
w1W2,:W1,:
W3,:
Question: How can we ‚Äú update ‚Äù the weights and biases, Wand b, to get the best ‚Äúdecision 
boundaries‚Äù to correctly discriminate the images pertaining to the different classes?
Answer: Adopt a ‚Äú loss function ‚Äù to measure the model ( Wand b) classification efficiency!

--- Page 381 ---

Linear Image Classifiers (7)
w2
w1W1,:W1,:W1,:W1,:

--- Page 382 ---

Linear Image Classifiers (8)
w2
w1W2,:
W2,:W2,:
W2,:

--- Page 383 ---

Linear Image Classifiers (9)
w2
w1
W3,:
W3,:W3,:
W3,:
W3,:

--- Page 384 ---

Linear Image Classifiers (9)
w2
w1W3,:
W2,:
W1,:
Note: After some updates, the separation hyperplanes will look as follows:

--- Page 385 ---

Loss Function
‚Ä¢A loss function measures the performance of the linear image classifier:
‚Ä¢Low loss ü°∫good classifier 
‚Ä¢High loss ü°∫bad classifier 
Note 1: The loss function is also known as the objective function and cost function . 
Note 2: When the loss function is  negative , it is known as the reward function , 
profit function , utility function , and fitness function .
Note 3: Likelihood and log-likelihood functions are also used to measure the 
performance of some machine learning models.
Note 4: The optimization task consists of ‚Äú minimizing ‚Äù the loss function or 
‚Äúmaximizing ‚Äù the reward function.

--- Page 386 ---

Loss Function (2)
Training data set: m images with defined labels (1, 2, ‚Ä¶, K):
ithinput imageLabel of ithinput image 
(integer taking values from [1‚Ä¶K]Number of training samples
Loss Per Sample: The loss function for each image sample (i = 1, 2, ‚Ä¶., m) is given 
by:
Classifier output
Loss Per Set: The loss function for all training images is the average over all 
individual losses:

--- Page 387 ---

Loss Function (3)
wx1 x2 y
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. +1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. -1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. +1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. +1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. -1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. -1Training 
Datax2
x1Decision BoundaryNot optimal 
‚òπ

--- Page 388 ---

Loss Function (4)
wx1 x2 y
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. +1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. -1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. +1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. +1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. -1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶. -1Training 
Datax2
x1Œ¥
Œ¥‚Ä≥Maximum -
Margin‚Ä≥
Decision Boundary
Œ¥: Maximum marginSupport vectorsSupport vectors

--- Page 389 ---

Loss Function (5)
wx2
x1x1 x2 yTest 
Data
+1
-1
+1
+1^
-1
+1
-1
‚Ä¶.     ‚Ä¶.      ‚Ä¶..
‚Ä¶.     ‚Ä¶.      ‚Ä¶..Classification Errors 
‚òπ
Misclassifications!
Classification Decision:
wT . x(t)‚â• 0 x(t)‚ààClass +1
wT . x(t)< 0 x(t)‚ààClass -1
Classification Decision ‚â° Dot Product!wT . x(t)= +1
wT . x(t)= -1wT . x(t)= 0

--- Page 390 ---

Loss Function (6)
wx2
x1Question: Shall we treat all these correct 
classifications equally?
Answer: No! 
The samples above the decision region of 
class +1 should be ‚Äúrewarded more‚Äù !
The samples below the decision region of 
class +1 should be ‚Äúrewarded less‚Äù !

--- Page 391 ---

Loss Function (7)
wx2
x1The samples below the decision region of 
class -1 should be ‚Äúrewarded more‚Äù !
The samples above the decision region of 
class -1 should be ‚Äúrewarded less‚Äù !

--- Page 392 ---

Loss Function (8)
Design Goal 1: Design a loss function that 
assigns 0 loss to the class +1 samples that 
are correctly classified above the wT . x(t)= 
+1 decision boundary
wx2
x
1wT . x(t)= +1
wT . x(t)= -1wT . x(t)= 0
Loss L= 0
Design Goal 2: Design a loss function that 
assigns an increasing loss to the class +1 
samples in proportion to their distance 
from the wT . x(t)= +1 decision boundaryIncreasing Loss L
wT . x(t)scoreLoss L
œÑ: ThresholdHinge Loss 
LUsually œÑ = 1

--- Page 393 ---

Loss Function (9)
Note: Same design goal applies to the 
class -1samples!
wx2
x
1wT . x(t)= +1
wT . x(t)= -1wT . x(t)= 0
Loss L= 0Increasing Loss L
wT . x(t)scoreLoss L
-1
Given the above design goals, the hinge 
lossfor the binary classifier is defined as 
follows:
œÑ = 1

--- Page 394 ---

Loss Function (10)
x1 x2 y y L
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ +1 2.3
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ -1 -1.5
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ +1 0.1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ +1 -1.1
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ -1 2.4
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ +1 0.5
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ -1 -0.02
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ -1 -3.2
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ +1 1.1Test 
Data^
L(1) = max(0, 1 -1*(2.3)) = max(0, -1.3) = 0 0
L(2) = max(0, 1 -(-1)*(-1.5)) = max(0, -0.5) = 0 0
L(3) = max(0, 1 -1*(0.1)) = max(0, 0.9) = 0.9 0.9
L(4) = max(0, 1 -1*(-1.1)) = max(0, 2.1) = 2.1 2.1
L(5) = max(0, 1 -(-1)*(2.4)) = max(0, 3.4) = 
3.43.4
L(6) = max(0, 1 -1*(0.5)) = max(0, 0.5) = 0.5 0.5
L(7) = max(0, 1 -1*(0.02)) = max(0, 0.98) = 0.98 0.98
L(8) = max(0, 1 -(-1)*(-3.2)) = max(0, -2.2) = 0 0
L(9) = max(0, 1 -1*(1.1)) = max(0, -0.1) = 0 0

--- Page 395 ---

Loss Function (11)
Question: How can we extend the hinge loss function to the multi -classification 
problem where we have K classes instead of 2?
Answer: Design the loss function to reward the classification decisions that assign 
higher scores to the correct class of the sample being classified
wT . x(t)scoresLoss L
1x1 x2 y y
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ [1‚Ä¶K] [p1, p2, ‚Ä¶, pK]^
True class of ithsample
Sum over all predictions except that of the true classPrediction scores of true class
Prediction scores of all other classespmaxHighest score among other classes
Margin

--- Page 396 ---

Loss Function (12)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
2.1
5.4
-3.5
-6.7p(x is a 
‚Ä≤cat‚Ä≤)p(x is a 
‚Ä≤dog‚Ä≤)p(x is a 
‚Ä≤car‚Ä≤)p(x is a 
‚Ä≤frog‚Ä≤)
L (1)= max(0, 1 ‚Äì(5.4‚Äì2.1)) + max(0, 1 ‚Äì(5.4‚Äì(-3.4)) + max(0, 1 ‚Äì(5.4 ‚Äì(-6.7))
= max(0, 1 -3.3) + max(0, 1 -8.8) + max(0, 1 -12.1) 
= max(0, -2.3) + max(0, -7.8) + max(0, -11.1) = 0 + 0 + 0 = 02.5
2.1
-3.2
-6.7
L (1)= max(0, 1 ‚Äì(2.1‚Äì2.5)) + max(0, 1 ‚Äì(2.1‚Äì(-3.4)) + max(0, 1 ‚Äì(2.1 ‚Äì(-6.7))
= max(0, 1 + 0.4) + max(0, 1 -5.5) + max(0, 1 -8.8) = 1.4 + 0 + 0 = 1.4
The margin between the predictions of the true and remaining classes ‚â• 1 ü°∫Loss = 
0!The margin between the predictions of the true and remaining classes < 1 ü°∫Loss > 
0!

--- Page 397 ---

Loss Function (13)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
2.1
5.4
-3.5
-6.7
importtorch
fromtorch importnn
loss = nn.MultiMarginLoss ()
x = torch.tensor([[ 2.1, 5.4, -3.5, -6.7]])
y = torch.tensor([ 1])
print('Hinge Loss = ' , loss(x, y))
x = torch.tensor([[ 2.5, 2.1, -3.2, -6.7]])
print('Hinge Loss = ' , loss(x, y))
Source: https://pytorch.org/docs/stable/generated/torch.nn.MultiMarginLoss.htmlDefault: p = 1
= 4Class weight

--- Page 398 ---

Loss Function (14)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
2.1
5.4
-3.5
-6.7
 Input Imagex(2)
2.5
2.1
-3.2
-6.7y(1)^ y(2)^
loss = nn.MultiMarginLoss()
x = torch.tensor([[ 2.1, 5.4, -3.5, -6.7],
[2.5, 2.1, -3.2, -6.7]])
y = torch.tensor([ 1, 1])
print('Hinge Loss = ' , loss(x, y))loss = nn.MultiMarginLoss( reduction = 'sum')
x = torch.tensor([[ 2.1, 5.4, -3.5, -6.7],
[2.5, 2.1, -3.2, -6.7]])
y = torch.tensor([ 1, 1])
print('Hinge Loss = ' , loss(x, y))
Default: reduction = ‚Ä≤mean‚Ä≤
Loss Per Set: The loss function for all training images is the average over all 
individual losses:
 nn.MultiMarginLossnn.MultiMarginLoss( reduction = 'sum')

--- Page 399 ---

Loss Function (15)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
2.1
5.4
-3.5
-6.7y(1)^Question: What is the effect of reduction the prediction score of the true class on the 
hinge loss?
Apply slight changes on this score
Answer: The hinge loss will remain unchanged until the difference between the 
prediction score of the true class and the highest prediction score of the other classes 
becomes smaller than one!
loss = nn.MultiMarginLoss()
x = torch.tensor([[ 2.1, 3.1, -3.5, -6.7]])
y = torch.tensor([ 1])
print('Hinge Loss = ' , loss(x, y))
loss = nn.MultiMarginLoss()
x = torch.tensor([[ 2.1, 3.099999 , -3.5, -6.7]])
y = torch.tensor([ 1])
print('Hinge Loss = ' , loss(x, y))

--- Page 400 ---

Loss Function (16)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
2.1
3.1
-3.5
-6.7y(1)^Question: What is the range of values of the hinge loss?
Change from +100 to -100
Answer: The hinge loss can take values from +‚àû (theoretically) to 0!

--- Page 401 ---

Loss Function (16)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
‚Ä¶.
‚Ä¶.
‚Ä¶.
‚Ä¶.y(1)^Question: What will be the hinge loss if all the prediction score are drawn from a 
distribution such as normal ( Œº = 0 and œÉ2= 0.1 ) or uniform [ -0.5:0.5 ]?
Small random scores!
Answer: The hinge loss will take a value close to K -1 where K is the number of 
classes!

--- Page 402 ---

Loss Function (17)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
‚Ä¶.
‚Ä¶.
‚Ä¶.
‚Ä¶.y(1)^Question: What will be the hinge loss if we include the prediction of the true class in 
our calculations?
Used in the loss calculation
Answer: The hinge loss will have a bias of 1!
Sum over all K predictions

--- Page 403 ---

Loss Function (18)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
‚Ä¶.
‚Ä¶.
‚Ä¶.
‚Ä¶.y(1)^Question: What will be the hinge loss if we take the mean of the loss scores?
Answer: The hinge loss scores will be scaled by a factor of 1/(K -1)
loss = nn.MultiMarginLoss( reduction = ‚Äòmean')
x = torch.tensor([[ 2.1, 5.4, -3.5, -6.7],
[2.5, 2.1, -3.2, -6.7]])
y = torch.tensor([ 1, 1])
print('Hinge Loss = ' , loss(x, y))Default: reduction = ‚Ä≤mean‚Ä≤

--- Page 404 ---

Loss Function (19)
Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
‚Ä¶.
‚Ä¶.
‚Ä¶.
‚Ä¶.y(1)^Question: What will be the hinge loss if we take the square of each term in the loss 
summation?
Answer: The new loss function is known as the squared hinge loss which is a 
smoother version of the hinge loss with nice mathematical properties (smoothness and 
continuity)!
loss = nn.MultiMarginLoss( p = 2)

--- Page 405 ---

Loss Function (20)
Question: Is the set of Wand bthat lead to a hinge loss of 0unique?
Answer: Using any scaled versions of Wand b(Œ±.Wand Œ±. B; Œ± > 0 ) will lead to the 
same hinge loss of 0!Linear Image Classifiers = f(x(1), W, b) = W . x(1)+ b
Input Imagex(1)
2.1
3.1
-3.5
-6.7y(1)^
4.2
6.2
-7.0
-13.4All prediction scores will change equally!

--- Page 406 ---

Loss Function and Regularization
Linear Image Classifier Input Imagex(i)
2.1
3.1
-3.5
-6.7y(i)^Note: So given a set of ‚Ä≥optimal‚Ä≥ Wand bweights that produce a zero hinge loss on a 
given dataset, we can choose any scaled version of these weights and get the same 
loss!
s = f(x(i), W, b) = W . x(i)+ b s = f(x(i), 2.W, 2.b) = 2.(W . x(i)+ b)4.2
6.2
-7.0
-13.4s = f(x(i), Œ±.W, Œ±.b) = Œ±.(W . x(i)+ b)Œ±.2.1
Œ±.3.1
-Œ±. 3.5
-Œ±. 6.7
Question: How can we choose a set of weights ( W, b) over another?
Answer: By adding a regularization term to the hinge loss function as follows:
Data loss: Classifier 
‚Ä≥optimized‚Ä≥ for training dataRegularization term to 
prevent model ‚Ä≥overfitting ‚Ä≥Lagrange multiplier
DOES NOT DEPEND ON DATA!

--- Page 407 ---

Loss Function and Regularization (2)
Hyperparameter
Regularization Choices:
w‚àà‚ÑúmxnL2regularization
L1regularization
Elastic net (L1+ L2)‚Ä¢Dropout : Randomly drops hidden activations
‚Ä¢Batch normalization: Normalizes the output of a 
previous activation layer by subtracting the batch mean 
and dividing by the batch standard deviation.
‚Ä¢Regional dropout : Erase random regions on the input
‚Ä¢Cutout: Randomly masks out square regions of input 
samples during training
‚Ä¢Stochastic Depth: Shrinks the depth of a network during 
training by randomly dropping entire ResBlocks (used 
mainly with ResNet models)
‚Ä¢Mixup : Constructs virtual samples using convex 
combinations of training samples (images and labels):

--- Page 408 ---

Loss Function and Regularization (3)
Note: In general, we use regularization to:
‚Ä¢Express preferences in among models beyond ‚Ä≥ minimize training error ‚Ä≥ 
‚Ä¢Avoid overfitting by preferring simple models that generalize better over complex 
ones (want models to work better on unseen 
‚Ä¢Improve optimization by adding curvature which ultimately helps the optimization 
process
1.0
1.0
1.0
1.0x(i)
Linear Classifier 1wT= [1, 0, 0, 0]
Linear Classifier 2wT= [1/4, 1/4, 1/4, 1/4]1.0y1(i)
1.0y2(i)
Equal loss
L2regularization likes to 
‚Ä≥spread out‚Ä≥ the weightsL2regularization encourages 
to use all available features!L1regularization encourages 
‚Ä≥sparse‚Ä≥ models!

--- Page 409 ---

xy
‚Ä¢The red model (average model) fits the data poorly
‚Ä¢The blue model fits the data with some training error
‚Ä¢The orange model fits the data better that the blue model
‚Ä¢The green model fits the training data perfectly Simple Model
Complex ModelLess Overfitting
More OverfittingLoss Function and Regularization (4)
Regularization pushes against fitting the data too well so the noise is not fitted in the data

--- Page 410 ---

xy
Loss Function and Regularization (5)
Simple Model
Complex ModelLess 
Variance
More 
VarianceMore Bias
Less Bias

--- Page 411 ---

Cross Entropy Loss
Linear Image Classifier Input Imagex(i)
2.1
3.1
-3.5
-6.7y(i)^
Question: How can we convert the raw prediction scores into probabilities?s = f(x(i), W, b) = W . x(i)+ 
b
Answer: Transform the raw prediction scores into values in the [0:1] range that add up 
to 1.0.2686
0.7303
0.0009
0.00005z(i)
Softmax 
Function
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..
Multinomial Logistic Regression
Numerically unstable 
‚òπ
Numerically stable 
‚ò∫

--- Page 412 ---

Cross Entropy Loss (2)
importnumpy asnp
defdef_softmax (x):
numerator = np.exp(x)
denominator = np. sum(numerator)
softmax = numerator/denominator
returnsoftmax
# "Easy" input :) 
x = np.array([ 1, 2, 3])
y = def_softmax(x)
print('Output with ""easy"" input = ' , y)
# "Hard" input :(
x = np.array([ 12345, 67890, 99999999 , 101818271634 ])
y = def_softmax(x)
print('Output with ""hard"" input = ' , y)
defstable_softmax (x):
z = x -max(x)
numerator = np.exp(z)
denominator = np. sum(numerator)
softmax = numerator/denominator
returnsoftmax
# "Easy" input :) 
x = np.array([ 1, 2, 3])
y = stable_softmax(x)
print('Output with ""easy"" input = ' , y)
# "Hard" input :(
x = np.array([ 12345, 67890, 99999999 , 101818271634 ])
y = stable_softmax(x)
print('Output with ""hard"" input = ' , y)

--- Page 413 ---

Cross Entropy Loss (3)
Linear Image Classifier Input Imagex(i)
2.1
3.1
-3.5
-6.7y(i)^
0.2686
0.7303
0.0009
0.00005z(i)Hinge Loss
Cross 
Entropy LossLogits: No interpretation 
‚òπ
Normalized Probabilities: 
Probabilistic interpretation 
‚ò∫Probability distribution over
the classes (labels)

--- Page 414 ---

-log(z(i))
z(i)Cross Entropy Loss (4)
2.1
3.1
-3.5
-6.7y(i)^
Unnormalized log -probabilities (or logits )Max
Function0
1
0
0z(i)
Softmax
Function0.2686
0.7303
0.0009
0.00005z(i)
Normalized probabilities
Softmax
FunctionMax
Function
Non-Differentiable 
‚òπNo gradient update 
‚òπ
Differentiable 
‚ò∫Gradient update 
‚ò∫Cross Entropy Loss:
Select the weights (w, b) to maximize 
the likelihood of the observed data

--- Page 415 ---

Cross Entropy Loss (5)
0.2686
0.7303
0.0009
0.00005z(i)
Normalized probabilities0
1
0
0t(i)
True 
labelQuestion: How can we compare 2 probability 
distributions?
Answer: There are 2 main ways to do so:
‚Ä¢Kolmogorov -Smirnov test
‚Ä¢Kullback -Leibler divergenceWorks with continuous distributions only (although there is a discrete version)
Non-differentiable 
‚òπ
Works with discrete distributions
Differentiable 
‚ò∫Non-symmetric 
‚òπ
D1 D2
Cross Entropy Entropy

--- Page 416 ---

Cross Entropy Loss (6)
Cross Entropy Loss:
-log(z(i))
z(i)
Min = 0 Max = ‚àû
Note: A loss of 0is achieved only with: 0.0
1.0
0.0
0.0z(i)Hinge Loss:
1Note: A loss of 0is achieved when the margin >= 1!
Question: What will be the score of the cross 
entropy loss if all z(i)are set to small random 
values? 
‚âà1/K
‚âà1/K
‚âà1/K
‚âà1/Kz(i)

--- Page 417 ---

Cross Entropy Loss (7)
importtorch
fromtorch importnn
loss = nn.CrossEntropyLoss ()
input = torch.tensor([[ 10, -2, -3, -1]], dtype = torch. float, requires_grad = True)
target = torch.tensor([ 0], dtype = torch. long)
print('Cross entropy loss = ' , (round(loss(input, target).item(), 6)))
10
-2
-3
-1y(i)^
0.999
6.14e -6
2.26e -6
1.67e -5z(i)

--- Page 418 ---

Cross Entropy Loss (8)
0.999
6.14e -6
2.26e -6
1.67e -5z(i)
10
-2
-3
-1y(i)^
10
9
9
-4y(i)^
10
-100
-100
-90y(i)^loss = nn.CrossEntropyLoss()
input = torch.tensor([[ 10, -2, -3, -1], [10, 9, 9, -4], [10, 1, 1, -5]], dtype = 
torch.float)
target = torch.tensor([ 0, 0, 0], dtype = torch. long)
output = loss(input, target)
print('Cross entropy loss = ' , loss(input, target).item())
0.576
0.2119
0.2119
4.79e -7z(i)
0.997
1.23e -4
1.23e -4
3.05e -7z(i)
loss = nn.CrossEntropyLoss(reduction = 'sum')Default: ‚Äòmean'

--- Page 419 ---

Cross Entropy Loss Vs. Hinge Loss
Cross Entropy -Based
Linear Image ClassifierInput Image
x(i)Hinge Loss -Based
Linear Image Classifier
4 Classes (K = 4)
10
-2
-3
-1y(i)^
10
9
9
-4y(i)^
10
-100
-100
-90y(i)^ Hinge Loss:
Cross Entropy Loss:
input = torch.tensor([[ 10, -2, -3, -1], [10, 9, 
9, -4], [10, -100, -100, -90]])
m = nn.Softmax()
output = m(input)
print(output)
Normalized probabilitiesloss = nn.CrossEntropyLoss()
target = torch.tensor([ 0, 0, 0], dtype = torch. long)
print('Cross entropy loss = ' , 
loss(input, target).item())
?

--- Page 420 ---

Cross Entropy Loss Vs. Hinge Loss (2)
Cross Entropy -Based
Linear Image ClassifierInput Image
x(i)Hinge Loss -Based
Linear Image Classifier
4 Classes (K = 4)
10
-2
-3
-1y(i)^
10
9
9
-4y(i)^
10
-100
-100
-90y(i)^ Hinge Loss:
Cross Entropy Loss:
5
1
1
-5y(i)^

--- Page 421 ---

Cross Entropy Loss Vs. Hinge Loss (3)
Cross Entropy -Based
Linear Image ClassifierInput Image
x(i)Hinge Loss -Based
Linear Image Classifier
4 Classes (K = 4)
10
-2
-3
-1y(i)^
10
9
9
-4y(i)^
10
-100
-100
-90y(i)^ Hinge Loss:
Cross Entropy Loss:
20
-2
-3
-1y(i)^
20
9
9
-4y(i)^
20
-100
-100
-90y(i)^
Performance tip 1: The cross entropy loss will always push the model weights to produce 
better probability estimates!
Performance tip 2: The hinge loss will produce a 0zero once the difference between the 
prediction of the true class and the highest prediction of the other classes exceeds 1!

--- Page 422 ---

Summary: Linear Image Classifiers‚Äô Views
Algebraic View:Visual 
View:
Geometric View:

--- Page 423 ---

Summary: Linear Image Classifiers‚Äô Score & Loss Functions
Training Data (Images+Labels)
Function Approximation: 
y(i)= f(x(i), w, b) = w.x(i)+ b ^ Hinge Loss:
Cross Entropy Loss:Regularization 
Term:
‚Ä≥Best‚Ä≥  
w‚Äôs and b‚Äôs?

--- Page 424 ---

Summary: Linear Image Classifiers‚Äô Score & Loss Functions 
(2)
Training Data (Images+Labels)
Hinge Loss:
Cross Entropy Loss:
Range: 0 to ‚àû
= 0 if (pi‚Äìmax(pj)) ‚â• 1
j
= (K -1) / m for random scoresRange: 0 to ‚àû
= 0 only if pi= 1 and pj‚â†i= 0
= -log(K) / m for random scores
Useful for debugging purposes 
‚ò∫

--- Page 425 ---

Artificial Neural Networks (ANNs): 
Essential Concepts
Dr. Lahouari Ghouti, PhD
Prince Sultan University. Riyadh, Saudi Arabia.
Email: lghouti@psu.edu.sa
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 426 ---

x1 x2 x3‚Ä¶‚Ä¶. xn y
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶..
Real -world dataFrom Mathematical Modeling to Artificial Neural Networks 
(ANNs)
Independent variables Dependent variable
f(x1, x2, ‚Ä¶, xn) = yMathematical Modeling: In this modeling, we try to 
‚Äúdiscover‚Äù the mathematical function that governs the 
relationship between the independent variables (or 
features), x1, x2, ‚Ä¶, xn, and the dependent variable y. In 
this way, it is assumed that there exists a multivariable 
function f: ÔÉÇnÔÇÆÔÉÇ such that:
Important Note: In most real -world scenarios, find f can 
be very challenging. On the other hand, artificial neural 
networks (ANNs) can be used instead as they represent 
universal approximators [1]. 
[1] K. Hornik , M. Stinchcombe , and H. White, "Multilayer Feedforward Networks are Universal Approximators ," Neural Networks, vol. 2, no. 5, pp. 359 ‚Äì366, 1989.

--- Page 427 ---

From Mathematical Modeling to ANNs (2)
Example: Let us assume that the underlying function, f, that relates x1and x2to y is given 
by:
Important Note: In most real -world scenarios, find f can 
be very challenging. On the other hand, artificial neural 
networks (ANNs) can be used instead as they represent 
universal approximators [1]. 
x1x2
Multilayer perceptron 
(2-3-4-1)fromsklearn.neural_network import MLPRegressor
func_approximator =MLPRegressor (hidden_layer_sizes =(3,4))
#Trysolver='adam‚Äô ---default: sgd

--- Page 428 ---

x y
x1 y1
x2 y2
x3 y3
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶..
xm ymANN Model
ÔÄ†ÔÅ≥(x,        )w0   x1
w0: Set to small random values in the range [ -0.5:0.5]y1^
Training datay1
ee = (y1‚Äìy1)2 ^Regression:
Classification:
e = ‚Äìy1log(y1)‚Äì(1‚Äìy1)log(1 ‚Äìy1)^ ^
w1= w0 ‚ÄìÔÅ®ÔÉëeww1x2^y2y2
w2= w1 ‚ÄìÔÅ®ÔÉëeww2x3^y3y3
w3= w2 ‚ÄìÔÅ®ÔÉëeww3
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
wm-1= wm-2 ‚ÄìÔÅ®ÔÉëewwm-1xm^ymym
wm= wm-1 ‚ÄìÔÅ®ÔÉëewwm
ÔÅ®: Learning rate ÔÉé]0.0, 1.0[
ÔÉëe: Gradient of error function with respect to wwBasic ANN Models

--- Page 429 ---

Gradient Descent
x y
3 10
3 10
3 10
3 10
3 10
‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶‚Ä¶..
3 10
Training dataw (y ‚Äìy)2 ^
1 49
e = (y ‚Äìy)2 ^ANN Model x = 3y = 10
ÔÄ†ÔÅ≥(x, w)y = w.x^
ÔÉëe = 2 (y ‚Äìy) (y)ÔÇ¢= 2(y ‚Äìy) xw^ ^
w^wnew= wold‚ÄìÔÅ®ÔÉëew1.042 47.251
1.083 45.566
1.112 43.940
1.163 42.372
8.49x10-15 3.333
ÔÅ®= 0.01ÔÅ®ÔÉé]0, 0.1[ 1000 iterations later!
loss = 1.262x10-29w = 3.333
ÔÅ®= 0.1
‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶..
Large ÔÅ®ÔÉ®Fast convergence!
Small ÔÅ®ÔÉ®Slow convergence!

--- Page 430 ---

Gradient Descent (2)
importmatplotlib.pyplot asplt
importnumpyasnp
x=3
y=10
w=1
eta =0.01
loss_small_eta =[]
forindinrange(0,1000):
yhat=w*x
e=yhat-y
loss_small_eta.append (e**2)
grad=2*(yhat-y)*x
w=w-0.1*eta*grad
#print('e =',e**2,'w=',w,'yhat=',yhat)
eta =0.1
w=1
large_loss_eta =[]
forindinrange(0,1000):
yhat=w*x
e=yhat-y
large_loss_eta.append (e**2)
grad=2*(yhat-y)*x
w=w-0.1*eta*grad
#print('e =',e**2,'w=',w,'yhat=',yhat)fig,ax=plt.subplots ()
ln = len(loss_small_eta )
ax.plot(np.arange (0,ln),loss_small_eta ,'r',label='$\eta=0.01$'
)
ax.plot(np.arange (0,ln),large_loss_eta ,'b',label='$\eta=0.1$')
legend=ax.legend (loc='upperright',fontsize ='x-large')
plt.xlabel ('Iteration index')
plt.ylabel ('loss=$(\hat{y}-y)^{2}$')
plt.title ('Gradient -basedweightupdate')
plt.show ()

--- Page 431 ---

Gradient Descent (3)
ANN Model x = 3
ÔÄ†ÔÅ≥(x, w)y = w.x^w.xTraining data: constants!
Variable!x2 -2.x.y -y2
ÔÉëe = 2 (y ‚Äìy) (y)ÔÇ¢= 2(y ‚Äìy) xw^ ^
w^
w0w1= w0 ‚ÄìÔÅ®ÔÉëew
w1w ÔÇª3.333loss ÔÇª-200

--- Page 432 ---

Gradient Descent (4)
Question: How will the loss function loom like if we have more than one weight 
coefficient (say w1and w2)?
Answer: In this case, the loss function will be a multivariable function that depends on 
many variables (w1, w2, ‚Ä¶., wn)
loss = f(w1and w2)
Partial derivative w.r.t w1
Partial derivative w.r.t w2ÔÉëwloss
The gradient vector ‚Äúpoints‚Äù to the 
direction of the maximum increase in 
the function!
w wk+1= wk‚ÄìÔÅ®ÔÉëe
Go opposite to the gradient direction to get the minimum ÔÅä

--- Page 433 ---

Gradient Descent (5)
Note: This is the reason behind the algorithm name: gradient descent or hill descent ÔÅä
Hint: Finding the minimum of a multivariable function is like descending from the top of 
a hill following the direction of the steepest descent (opposite direction of the gradient 
vector)!
loss = f(w1and w2)
Likelihood = f(w1and w2)
w1w2
w wk+1= wk+ ÔÅ®ÔÉëe
Follow the gradient direction to get the maximum ÔÅäGradient ascent!

--- Page 434 ---

loss = ‚Äìylog(y) ‚Äì(1‚Äìy)log(1 ‚Äìy)^ ^x y
x1 1
x2 0
x3 1
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶..
xm 1
Training dataloss = ‚Äìylog(y) ‚Äì(1‚Äìy)log(1 ‚Äìy)^ ^Cross Entropy Loss
Note: In binary classification problems, the target variable, y, is binary and usually takes 0 
and 1 values.
Case 1: y = 1Cross entropy loss
Case 1: y = 0 loss = ‚Äìylog(y) ‚Äìlog(1 ‚Äìy)^ ^loss = ‚Äì

--- Page 435 ---

x1
x2
x3
xn-1
xnh1[1]
h2[1]b1[1]
b2[1]
b3[1]
bK[1]h3[1]
hK-1[1]
hK[1]v3[1]v2[1]v1[1]
vK-1[1]
vK[1]b1[2]
b2[2]
bL[2]h1[2]v1[2]
h2[2]v2[2]
hL-1[2]vL-1[2]
hL[2]vL[2]b1[3]
b2[3]
bM[3]h1[3]v1[3]
h1[3]v2[3]
h1[3]v3[3]
h1[3]v4[3]z1b1[4]
b2[4]
b3[4]z2
z3p1
p2
p3Multi -Layer ANNs
Input 
LayerHidden
Layer 1Hidden
Layer 2Hidden
Layer 3Hidden
Layer 4Softmax
Layer

--- Page 436 ---

Multi -Layer ANNs (2)
Input 
LayerHidden
Layer 1Hidden
Layer 2Hidden
Layer 3Hidden
Layer 4Output
Layerx1
x2
x3
xn-1
xnh1[1]
h2[1]b1[1]
b2[1]
b3[1]
bK[1]h3[1]
hK-1[1]
hK[1]v3[1]v2[1]v1[1]
vK-1[1]
vK[1]b1[2]
b2[2]
bL[2]h1[2]v1[2]
h2[2]v2[2]
hL-1[2]vL-1[2]
hL[2]vL[2]b1[3]
b2[3]
bM[3]h1[3]v1[3]
h1[3]v2[3]
h1[3]v3[3]
h1[3]v4[3]z1b1[4]
b2[4]
b3[4]z2
z3bp2
y^

--- Page 437 ---

z1 p1
z2 p2
z3 p3
Softmax
Layer
z1 p1
z2 p2
zK pK
Softmax
Layer
Softmax Layer
defstandard_softmax (z):
numerator =np.exp (z)
denominator =np.sum(numerator)
softmax =numerator/denominator
returnsoftmax
#Inputvectortosoftmax layer
inp_softmax =np.array ([1,2,3,4,5])
#Outputvectorfromsoftmax layer
out_softmax =standard_softmax (inp_softmax )
print('Output fromSoftmax layer=',out_softmax )
inp_softmax =np.array ([12345 ,67890 ,99999999 ])
out_softmax =standard_softmax (inp_softmax )
print('Output fromSoftmax layer=',out_softmax )

--- Page 438 ---

Softmax Layer (2)
defstable_softmax (z):
z=z-max(z)
numerator =np.exp(z)
denominator =np.sum(numerator)
softmax =numerator/denominator
returnsoftmax#Inputvectortosoftmax layer
inp_softmax =np.array ([1,2,3,4,5])
#Outputvectorfromsoftmax layer
out_softmax =stable_softmax (inp_softmax )
print('Output fromSoftmax layer=',out_softmax )
inp_softmax =np.array ([12345 ,67890 ,99999999 ])
out_softmax =stable_softmax (inp_softmax )
print('Output fromSoftmax layer=',out_softmax )

--- Page 439 ---

x y0 y1 y2
x1 1 0 0
x2 0 0 1
x3 1 0 0
x4 0 1 0
x5 0 0 1
‚Ä¶‚Ä¶‚Ä¶‚Ä¶. ‚Ä¶.‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶.‚Ä¶‚Ä¶..
xm 1 0 0
Training dataCategorical Cross Entropy Loss
Note: In multi -classification problems, the target variable, y, is a one-hot encoded vector .
Class 0
Class 2
Class 1Categorical cross entropy loss
loss = ‚Äìyklog(yk)  ÔÉ•^
k=13
Number of classes
y0 y1 y2
0 0 1
0 1 0
0 1 0
1 0 0
0 0 1Target labels
y0 y1 y2
0.11 0.26 0.63
0.2 0.55 0.25
0.21 0.07 0.72
0.81 0.1 0.09
0.2 0.15 0.65Predicted labels
^ ^ ^
0 0 1Target labels
y0 y1 y2
0.001 0.001 0.998Predicted labels
^ ^ ^
loss = 0.002
y0 y1 y2
0.499 0.5 0.001Predicted labels
^ ^ ^
loss = 6.907 !!!ykyk
yk
yk

--- Page 440 ---

x y0 y1 y2
x1 0 0 1
x2 0 1 0
x3 0 1 0
x4 1 0 0
x5 0 0 1
‚Ä¶‚Ä¶‚Ä¶‚Ä¶. ‚Ä¶.‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶.‚Ä¶‚Ä¶..
xm 1 0 0
Training dataCategorical Cross Entropy Loss (2)
y0 y1 y2
0.11 0.26 0.63
0.2 0.55 0.25
0.21 0.07 0.72
0.81 0.1 0.09
0.2 0.15 0.65
‚Ä¶.‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶.‚Ä¶‚Ä¶..
0.65 0.15 0.2^ ^ ^
Predicted labelsTotal categorical cross entropy loss
loss = ‚Äì log(y)  ÔÉ•
i=1m1
m___ ^
Number of training samples

--- Page 441 ---

Regression Output Layer
z1
z2y
zL Output
Layer^ÔÉéÔÉÇ
x y
x1 y1
x2 y2
x3 y3
‚Ä¶‚Ä¶‚Ä¶.. ‚Ä¶‚Ä¶..
xm ym
Training dataÔÉéÔÉÇ+
Important Note: Sometimes, the target variable, y, may be restricted to the positive range. 
For example, when y represents a house price or profit or any naturally -positive quantity. 
In this case, we can use a restricting activation function at the output such as the rectified 
linear unit ( ReLU ).ÔÉéÔÉÇ+
^y
ReLU Block

--- Page 442 ---

Any
Layerx1
x2
x3
xn-1
xnhb
v = ÔÅ≥(h)
Any
Nodew1
w2
w3
wn-1
wnComputation Nodes in ANNs
h= x1. w1+ x2. w2+ x3. w3+ ‚Ä¶. + xn-1. wn-1+ xn. wn+ bDot ProductBias term (kind of w0with x0= 1)!
h= <x,  w1>  + b
v= ÔÅ≥(h)
tanh = 2 logistic -1

--- Page 443 ---

Computation Nodes in ANNs (2)
x1
x2
x3
x4hb
v = ÔÅ≥(h)w1
w2
w3
w4
#Inputvectorx
x=np.array ([1,2,3,4])#Weightvectorw
w=np.array ([0.5,-0.5,1.2,-2.5])#biasb
b=0.5
h= x1. w1+ x2. w2+ x3. w3+ ‚Ä¶. + xn-1. wn-1+ xn. wn+ b
v= ÔÅ≥(h)#Outputnodeh
h=np.dot(w, x)+b
print('h=',h)
deflogistic (z):
numerator =1
denominator =1+np.exp(-z)
logit=numerator/denominator
returnlogit#Outputnodev
v=logistic(h)
print('v=',v)
#Outputnodev
v=relu(h)
print('v=',v)defrelu(z):
returnnp.maximum (0,z)
deftanh(z):
numerator =np.exp(z)-np.exp(-z)
denominator =np.exp(z)+np.exp(-z)
logit=numerator/denominator
returnlogit#Outputnodev
v=tanh(h)
print('v=',v)

--- Page 444 ---

Computation Nodes in ANNs (3)
Any
Layerx1
x2
x3
xn-1
xnh1b1
v1= ÔÅ≥(h1)
w3nw12
w13
w1n-1
w1nh2b2
v2= ÔÅ≥(h2)w21
w22
w23
w2n-1
w2n b3
h3 v3= ÔÅ≥(h3)w32 w33 w3n-1w31w11
w11w12   w13‚Ä¶‚Ä¶.  w1n-1w1n   
w21w22   w23‚Ä¶‚Ä¶.  w2n-1w2n   
w31w32   w33‚Ä¶‚Ä¶.  w3n-1w3n   Option 1:
w11
w21   
w31
wn-1,1
wn,1‚Ä¶.‚Ä¶...w13
w23   
w33
wn-1,3
wn,3   ‚Ä¶‚Ä¶‚Ä¶.w12
w22   
w32
wn-1,2
wn,2   ‚Ä¶‚Ä¶‚Ä¶.Option 2:
x
nx1W
3xnb
3x1h
3x1v = ÔÅ≥(h)

--- Page 445 ---

Computation Nodes in ANNs (4)
x
nx1W
3xnb
3x1h
3x1v = ÔÅ≥(h)
#Inputvectorx
x=np.array ([1,2,3,4])
print('xvector=',x)#Weightmatrixw
w=np.array ([[1,1,1,1],
[2,2,2,2],
[3,3,3,3]])
print('weight matrix=',w)#biasvectorb
b=np.array ([-1,-2,-3])
print('biasvector=',b)
#Outputnodeh
xdotw=np.dot(w, x)
print('xdotw=',xdotw)
1
2
3
4<w1, x> = 101
1
1
1
1
2
3
4<w1, x> = 202
2
2
21
2
3
4<w1, x> = 303
3
3
3
h=xdotw+b
print('h=',h)
 #Outputnodev
v=logistic(h)
print('v=',v)

--- Page 446 ---

Computation Nodes in ANNs (5)
loss = ‚Äìylog(y) ‚Äì(1-y)log(1 -y)^ ^h = w1.x1+ w2.x2+ bw1
x2b x1
w2y = ÔÅ≥(h)ÔÅ≥^y 2
3= 0.2
= 0.3= 0.1
h = 1.4 y = 0.8^= 0
= 1.62
=0 =1
= 3= 0.8(1 -0.8) = 0.16
= 1/(1 -0.8) = 5
= 2
= 1

--- Page 447 ---

Computation Nodes in ANNs (6)
= 5 = 0.16 = 3
Computed once only ÔÅä
 = 5 = 0.16 = 2= 1.6= 5 = 0.16 = 1= 0.8= 2.4
= 2

--- Page 448 ---

Computation Nodes in ANNs (7)
= 0.3 = -0.1 .2.4= 0.06
= 0.2 = -0.1 .1.6= 0.039
= 0.1 = -0.1 .0.8= 0.019

--- Page 449 ---

Computation Nodes in ANNs (8)
loss = ‚Äìylog(y) ‚Äì(1-y)log(1 -y)^ ^h = w1.x1+ w2.x2+ bw1
x2b x1
w2y = ÔÅ≥(h)ÔÅ≥^y 2
3= 0.039
= 0.06= 0.019
h = 0.277 y = 0.0568^= 0
= 0.0584
Old loss = 1.62Decrease in loss after update!

--- Page 450 ---

Computation Nodes in ANNs (9)
x1‚Ä¶.. xn y
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
Training datah = w1.x1+ w2.x2+ bw1
xnb x1
w2y = ÔÅ≥(h)ÔÅ≥^yForward pass 1
Loss 
gradient Gradient backpropagation + Weight update 1Forward pass 2
Loss 
gradient Gradient backpropagation + Weight update 2Forward pass 3
Loss 
gradient Gradient backpropagation + Weight update 3Forward pass m
Loss 
gradient Gradient backpropagation + Weight update m
Algorithm: Stochastic Gradient Descent (SGD)

--- Page 451 ---

Computation Nodes in ANNs (10)
x1‚Ä¶.. xn y
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
Training datah = w1.x1+ w2.x2+ bw1
xnb x1
w2y = ÔÅ≥(h)ÔÅ≥^yForward pass 1
Loss 
gradient Gradient backpropagation + Weight update 1
Algorithm: Mini -Batch Gradient DescentK samples K samplesForward pass 2
Loss 
gradient Gradient backpropagation + Weight update 2K samplesForward pass 3
Gradient backpropagation + Weight update 3Loss 
gradientK samplesForward pass L
Loss 
gradient Gradient backpropagation + Weight update L
Average loss for K training samples!

--- Page 452 ---

Computation Nodes in ANNs (11)
x1‚Ä¶.. xn y
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.‚Ä¶‚Ä¶.
Training datah = w1.x1+ w2.x2+ bw1
xnb x1
w2y = ÔÅ≥(h)ÔÅ≥^yForward pass 1
Gradient backpropagation + Weight update 1
Algorithm: Batch Gradient Descentm samples
Loss 
gradient
1 epoch: Average loss for m training samples!m samplesForward pass 2
Loss 
gradient Gradient backpropagation + Weight update 2

--- Page 453 ---

x1
x2
x3b1[1]
b2[1]
b4[3]
b4[1]b1[2]
b2[2]exp
expz1
z2ez1
ez2+ÔÅ°//ez1/ÔÅ°
ez2/ÔÅ°y^y
LCCE

--- Page 454 ---

Softmax Layer: Gradient Computation
Softmax layer
z vector p vector
ÔÉéÔÉÇKÔÉéÔÉÇK

--- Page 455 ---

Jacobian Matrix
z vector p vector
ÔÉéÔÉÇKÔÉéÔÉÇK‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..
Jacobian Matrix
log(   )
Note: Thanks to the introduction of the log, the required partial derivative is simpler now!Required term Simple term!

--- Page 456 ---

Jacobian Matrix (2)
zi
Let us take the partial derivative of log(pi) with respect to zj:
We can write the above using the indicator function 1{¬∑} . The indicator function takes on 
a value of 1 if its argument is true, and 0 otherwise. So, we will write 1{i=j}
= 1 if i= j 
= 0 if iÔÇπj u 1/u
u /    zj

--- Page 457 ---

Jacobian Matrix (3)
= ezjConstants with respect to zj
= 1
= 1
= 1
= 1= 0 = 0 = 0
pj

--- Page 458 ---

Softmax Layer, Jacobian Matrix and Gradient
Sample belongs to Class 3
The categorical cross -entropy loss function takes two K -dimensional vectors, y and y, that 
represent a probability distribution and the one -hot encoded target, respectively.^
K denotes the number of different classes and the subscript ùëñdenotes ùëñ-thelement of the 
vector.

--- Page 459 ---

Softmax Layer, Jacobian Matrix and Gradient (2)
We need to find the derivative of the loss function w.r.t to the weighted input z of the 
output layer:
 Linear 
operator
The indicator function 1{¬∑} takes on a value of 1 for ùëñ= ùëóand 0 everywhere else. 
Therefore, we get:
Independent of i= 1

--- Page 460 ---

Softmax Layer, Jacobian Matrix and Gradient (3)
LCCEh = w1.x1+ w2.x2+ bw1
x2b x1
w2y = ÔÅ≥(h)ÔÅ≥^y

--- Page 461 ---

x1
x2
x3b1[1]
b2[1]
b4[3]
b4[1][[ 0.15232633 0.42112296 -0.32685409 0.12062509 ]
[-0.39337332 -0.31456204 -0.25989355 -0.0194304 ]
[-0.35759404 0.35853488 -0.0902096 0.02739133 ]]Wh=

--- Page 462 ---

x1
x2w1
w2y^y
e = (y -y)2 ^

--- Page 463 ---

x1
x2y
Input Layer
Hidden Layer 1
Hidden Layer 2Output Layer

--- Page 464 ---

Convolutional Neural Networks: 
Some Insights
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 465 ---

Function Representation
f(x)=4x+3
x Computer 
Code 1MULT OP
ADD OP
Intel 8086 Processor: ÔÇ≤MadeÔÇ≤for ADD & MULT OPS!
f(x)= -0.2x2+x+0.8
4*x + 3 Computer 
Code 2-1*0.2*x*x + x + 0.8
ADD OPMULT OP

--- Page 466 ---

Function Representation (2)
x Computer 
Code ???f(x)
f(x)=sin(x)
f(x)=cos(x)
f(x)=log(x)
f(x)=ex
 Internal Architecture of Computer ProcessorADD & MULT OPS!
SUB OP ÔÇªADD OP Using 2‚Äôs Complement!
DIV OP ÔÇªMULT OP Using Reverse Bit Shifting!Brook Taylor (1685 -1731)
Approximation of f(x) around point x = a
Taylor Series + Some Lookup Tables (Intel)
Note: Shift -and-add algorithms use only additions , multiplications by a 
power of 2 (i.e., bit shifts only) and comparisons

--- Page 467 ---

Function Representation (3)
Gradient of f evaluated at point a Hessian of f evaluated at point a2nd-order Taylor polynomial approximation of multi -variable function

--- Page 468 ---

Function Representation (4)
f(x): Periodic FunctionFourier series of f(x)
Fourier Coefficients
Jean -Baptiste Joseph Fourier (1768 -1830)
a0= an= 0
n = 1
 n = 2
 n = 4
 n = 3
Fourier Transform Pair
Forward Transform: Time ÔÉ†Frequency Backward Transform: Frequency ÔÉ†TimeNon-Periodic Function!
x(t)
 X(f)

--- Page 469 ---

Function Representation (5)
x(t)
X(f)
16000 samples / secHighest Frequency = 8000 Hz
||.||
FFT
ÔÅ≠x(t)
Cos(2ÔÅ∞(800)t)
 cos(2ÔÅ∞(8000)t)

--- Page 470 ---

Function Representation (6)
I(x, y): Grayscale image (HxWx1)
 U(f1, f2): 2D Fourier Representation: Magnitude (Spectrum)x
y(0, 0)
f1f2
(0, 0)
I(x, y) = U(0, 0) x                 + U(1, 0) x                + U(0, 1) x                 + ‚Ä¶. + U(3, 5) x                  + U(20, -10) x                +‚Ä¶.

--- Page 471 ---

Function Representation (7)
U(f1, f2): 2D Fourier Representation: Magnitude (Spectrum)f1f2
(0, 0)(5, 5)
2D IFFT
I(x, y)
I(x, y)(30, 30)

--- Page 472 ---

Function Representation (8)
Note 1: Fourier series and transforms yield generally complex -valued representations. 
Therefore, such representations are not recommended for feature extraction as they are 
expansive (images with n2pixels will be represented with n2complex -valued coefficients 
(i.e., 2 n2real-valued coefficients!)
Note 2: For feature extraction, real-valued representations are preferred!
‚Ä¢Discrete cosine transform (DCT)
8x8 block
DCT2 bases
w1.+ w2.
+ w3. + ‚Ä¶.  +w63.
+ w64.
DCT2 Zigzag Scan
 Scan Indexw‚Äòs variance00Basic filtering/compression 
process!JPEG Image Compression Standard!Decay similar to eigenvalues!

--- Page 473 ---

Function Representation (9)
8x8 block
DCT2 coefficients: w‚Äôs
8x8 image block
32x32 block32x32 image block
DCT2 coefficients: w‚Äôs
Scan Indexw‚Äòs variance

--- Page 474 ---

Function Representation (10)
dct_inv =np.zeros ([8,8])
dct_inv[7,7]=1#64thDCT2basisfunction!
im_blk_dct_basis =idct2(dct_inv)
plt.figure ()
plt.imshow (im_blk_dct_basis ,cmap= 'gray')
DCT2 bases

--- Page 475 ---

Image Filtering: 101 Basics
0 0
0 00
0
0Highpass -filtered (HPF) image
Lowpass -filtered (LPF) image

--- Page 476 ---

Image Filtering: 101 Basics (2)
Note: In digital image processing (DIP) and computer vision, there are many famous 
‚Äúhand -engineered ‚Äù filters (or kernels)
Mean filter
 Gaussian filter Laplacian filter
Sobel -X filter
 Sobel -Y filter
Symmetric FiltersNon-Symmetric Filters

--- Page 477 ---

Image Filtering: 101 Basics (3)
Note: To apply a filter on an image, we need to apply the convolution operator .
Definition: Given an image I(x, y) and a filter h(x, y), the convolution operator is defined 
as:
Convolution OperatorFilter reversal!
Important Note 1: The convolution looks similar to the correlation but it is drastically 
different!
NO filter reversal!
Important Note 2: The convolution is identical to the correlation when applying 
symmetric filters !

--- Page 478 ---

Image Filtering: 101 Basics (4)
Note: The convolution can be implemented in either the spatial or the frequency domain
Spatial -Domain Convolution: Given an image I(x, y) and a filter h(x, y), the spatial -
domain convolution is performed as follows:
I(x, y)
h(x, y)
h(-x, -y)
1     2     1
0            0
-1    -2    -1
If(x, y)

--- Page 479 ---

Image Filtering: 101 Basics (5)
Frequency -Domain (Fourier -Domain) Convolution: Given an image I(x, y), a filter h(x, 
y) and their Fourier representations, F(u, v) and H(u, v), the frequency -domain 
convolution is performed as follows:
I(x, y)
h(x, y)2D FFT
F(u, v)
H(u, v)2D IFFTElement -wise 
Multiplication
Laplacian -Filtered Image

--- Page 480 ---

Image Filtering: 101 Basics (6)
Old Good Days
Deep Learning Days
Celebrated Complexity:
 Core -Based Computation:
Nvidia Titan Z: 5760 cores / 12 GB of memory

--- Page 481 ---

Function Representation (10)
Input imageLPF 
(rows+columns )
ÔÄ†ÔÇØ2
LPF (rows) + HPF 
(columns)ÔÄ†ÔÇØ2
HPF (rows) + LPF 
(columns)ÔÄ†ÔÇØ2
HPF 
(rows+columns )ÔÄ†ÔÇØ2
Half -size ‚Äúapproximation‚Äú image
Half -size ‚Äúhorizontal edges‚Äù image
Half -size ‚Äúvertical edges‚Äù image
Half -size ‚Äúdiagonal edges‚Äù image
Note 1: The LPF and HPF filters are ‚Äú hand -engineered ‚Äù kernels!
Note 2: Down -sampling by 2 ( ÔÄ†ÔÇØ2) is possible because half of the image frequencies have 
been removed ! This is very similar to average/max pooling in CNNs!

--- Page 482 ---

224x224x3
3x3 
Kernel (filter)
Receptive 
field
Image
PaddingNote 1: For a kernel size of KxK , the 
padding size is floor(K/2)
Note 2: Convolution is performed as 
an element -wise multiplication 
between the pixels of the receptive 
field and the KxK kernel coefficients
-1     2     1.5
-0.5  0.2   1.2
1      0     -2124 204   98
145  230  124
95   220  145 
124x( -1) + 204x2 + 98x1.5 + 145x( -0.5) + 230x0.2 + 124x1.4 + 95x1 + 220x0 + 145x( -2) = 383.1  
Feature Map 
CoefficientNote 3: The number of output features in each dimension of 
the resulting feature map can be found using:
Padding size Stride sizeKernels (Filters) in Convolutional Neural Networks (CNNs)

--- Page 483 ---

Note 1: When a KxK kernel is applied to a color image (RGB or any other color space), 
the kernel size is KxKx3 or 3xKxKKernels (Filters) in CNNs (2)
KxKx3
Kernel (filter)Feature Map ( LxL)
fp

--- Page 484 ---

Note 1: VGG -19is a variant of VGG model which consists of 19 layers (16 
convolution layers, 3 Fully connected layer, 5 pool layers (max or average) and 
one softmax layer) . 
Note 2: Other variants of VGG -19are VGG -11and VGG -16. 
Note 3: The VGG -19 model has 19.6 billion FLOPs.VGG*-19 Architecture
224x224x3
VGG -191000x1Used for ImageNet
* Visual Geometry Group (VGG) at Oxford University (UK)

--- Page 485 ---

The VGG -19architecture is as follows:VGG -19 Architecture (2)
‚Ä¢ Convolution layer 1: 64 3x3 kernels (or filters) with stride = 1
‚Ä¢ Convolution layer 2: 64 3x3 kernels
‚Ä¢ Downsampling layer 1: 2x2 MaxPool with stride = 2 
‚Ä¢ Convolution layer 3: 128 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 4: 128 3x3 kernels with stride = 1
‚Ä¢ Downsampling layer 2: 2x2 MaxPool with stride = 2
‚Ä¢ Convolution layer 5: 256 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 6: 256 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 7: 256 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 8: 256 3x3 kernels with stride = 1 
‚Ä¢ Downsampling layer 3: 2x2 MaxPool with stride = 2‚Ä¢ Convolution layer 9: 512 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 10: 512 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 11: 512 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 12: 512 3x3 kernels with stride = 1
‚Ä¢ Downsampling layer 3: 2x2 MaxPool with stride = 2
‚Ä¢ Convolution layer 13: 512 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 14: 512 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 15: 512 3x3 kernels with stride = 1
‚Ä¢ Convolution layer 16: 512 3x3 kernels with stride = 1
‚Ä¢ Downsampling layer 3: 2x2 MaxPool with stride = 2
‚Ä¢ Fully -connected layer 1: 4096x1
‚Ä¢ Fully -connected layer 2: 4096x1
‚Ä¢ Fully -connected layer 3: 1000x1
‚Ä¢ Softmax layer 1: 1000x1 ( Imagenet 1000 classes)
VGG -19has 19layers (convolutional and fully -connected)!

--- Page 486 ---

p11   p22     p13   p14   p15   p16     p17
p21   p22     p23   p24   p25   p26     p27
p31   p32     p33   p34   p35   p36     p37
p41   p42     p43   p44   p45   p46     p47
p51   p52     p53   p54   p55   p56     p57
p61   p62     p63   p64   p65   p66     p67
p71   p72     p73   p74   p75   p76     p77Receptive Field3x3 vs. 5x5 Kernels 
3x3Feature Map (3x3)
fp34 fp33
fp43
fp53fp54fp35
fp45
fp55fp44
3x3
Note 1: The coefficient fp44of the feature map consists of the following coefficients in the receptive field: 
p33, p34, p35, p43, p44, p45, p53, p54and p55.
Note 2: Similarly, the coefficients fp33to fp55 of the feature map consist of the coefficients in the receptive 
field ranging from p22to p66.
Note 3: Applying a second 3x3 kernel on the feature map with coefficient fp44as center will consist of the 
coefficients in the receptive field ranging from p22to p66.
Note 4: This has the same effect as applying a 5x5 kernel on the receptive field with coefficient p44as 
center!3x3ÔÇª
5x53x3x

--- Page 487 ---

p11   p22     p13   p14   p15   p16     p17
p21   p22     p23   p24   p25   p26     p27
p31   p32     p33   p34   p35   p36     p37
p41   p42     p43   p44   p45   p46     p47
p51   p52     p53   p54   p55   p56     p57
p61   p62     p63   p64   p65   p66     p67
p71   p72     p73   p74   p75   p76     p77Receptive Field3x3 vs. 7x7 Kernels 
3x3Feature Map (3x3)
fp34 fp33
fp43
fp53fp54fp35
fp45
fp55fp44
3x3Feature Map (3x3)
fp24 fp22
fp42
fp62fp64fp26
fp46
fp66fp44
3x3Feature Map (3x3)
fp14 fp11
fp41
fp72fp74fp17
fp47
fp77fp44
3x3ÔÇª
7x73x3x
3x3x
Note 1: The VGG -19model implements repeated 3x3 kernels as a replacement of 
larger kernels which dramatically reduces the computational complexity!49 coefficients 
1 bias term
3x9 + 3= 27 coefficients + 3 bias terms
Note 2: Using 3 consecutive layers enables the use of 3 non -linear activations 
(such as ReLUs ) which makes the decision function more discriminative !

--- Page 488 ---

1x1 Convolution
Note: A 1x1 convolution kernel consists of a single scalar coefficient!
0.51x1 Kernel
224x224x3
224x224x3
224x224x3
1.61x1 Kernel
Note: The effect of a 1x1 convolution kernel on the feature map is a mere scaling of 
the coefficients!

--- Page 489 ---

1x1 Convolution (2)
Question: Is the use of 1x1 convolution useless? 
Answer: No! It is very useful whe n used astutely!
WxHxC
1x1xC kernelHint: 1x1 convolution could be taught of as a cross -channel convolution !
Feature Map ( WxH )
fijCx1 Layer
fijw1
w2
w3
w4
wC
Note: The layer weights, w‚Äôs, are 
provided by the 1x1xC kernel 
coefficients!

--- Page 490 ---

1x1 Convolution (3)
WxHxC1x1xC kernel
Feature Map ( WxH )Cx1 Layer
f11w1
w2
w3
w4
wC1x1xC kernel
f11Cx1 Layer
f12w1
w2
w3
w4
wCf121x1xC kernelCx1 Layer
fWHw1
w2
w3
w4
wC
fWH
Note: 1x1 convolution can be taught of a single hidden 
layer network inside a bigger network!
2014

--- Page 491 ---

1x1 Convolution (3)
WxHxCHint: We can use L 1x1xC kernels where L can be either smaller than C in case we 
want to ‚Äúcompress‚Äù the feature maps or bigger than C for ‚Äúexpanding‚Äù the 
dimension of the feature map
L 1x1xC 
kernels
Feature Map ( WxHxL )

--- Page 492 ---

1x1 Convolution (4)
Example: Convert a 7x7x512 tensor into a 4096x1 tensor 
7x7x5124096x1
classifier =nn.Sequential (
nn.Linear (512*7*7,4096),
nn.ReLU(True),
nn.Dropout (p = dropout),
nn.Linear (4096,4096),
nn.ReLU(True),
nn.Dropout (p = dropout),
nn.Linear (4096,num_classes ))importtorch
importtorch.nn asnn
classMyModel(nn.Module):
def__init__(self):
super(MyModel, self).__init__()
self.flatten = nn.Flatten ()
self.fc1 = nn.Linear (in_features = 4096, out_features = 4096)
self.relu= nn.ReLU()
defforward(self, x):
x = self.flatten (x)
x = self.fc1(x)
x = self.relu(x)
returnx
model = MyModel()
input_tensor = torch.randn (1, 3, 224, 224)
output = model( input_tensor )

--- Page 493 ---

VGG -19 Architecture
224x224x3
Spatial padding: 1
64 3x3 filters
224x224x64
64 3x3 filters
224x224x64
2x2 Pooling
112x112x64
128 3x3 filters
112x112x128
128 3x3 filters
112x112x128
2x2 Pooling
56x56x128
256 3x3 filters
56x56x256
256 3x3 filters
56x56x256
256 3x3 filters
56x56x256
256 3x3 filters
56x56x256
2x2 Pooling
26x26x256512 3x3 filters
28x26x512
512 3x3 filters
28x26x512
512 3x3 filters
28x26x512
512 3x3 filters
28x26x512
2x2 Pooling
14x14x512
512 3x3 filters
14x14x512
512 3x3 filters
14x14x512
512 3x3 filters
14x14x512
512 3x3 filters
14x14x512
2x2 Pooling
7x7x512
Fully -connected 
4096x1
Fully -connected 
4096x1
1000x14096x1
Softmax Fully -connected

--- Page 494 ---

Resnet*Architecture
* K. He, X. Zhang, S. Ren, and J. Sun, "Deep Residual Learning for Image Recognition," arXiv , 2015. https://doi.org/10.48550/arxiv.1512.03385

--- Page 495 ---

Resnet Architecture (2)
224x224x3
64 7x7 filters
Batch Norm
ReLU112x112x64
112x112x64
112x112x64
3x3 Max Pooling
56x56x64Stride =2& Padding =1
Block 1Skip connections
Stride =1& Padding =1
56x56x64
Block 2

--- Page 496 ---

Resnet Architecture
56x56x64
Block 2
Skip connection with tensors of different dimensions (dotted lines)!56x56x64
Skip connection with tensors of same dimensions (solid lines)!
Stride = 2 ÔÉ®dimension reduction!28x128x128

--- Page 497 ---

Image Processing / Computer Vision : 
A Primer
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 498 ---

Image Channels
r=img1.copy()
#Setgreenandbluechannels to0
r[:,:,1]=0
r[:,:,2]=0
g=img1.copy()
#Setredandbluechannels to0
g[:,:,0]=0
g[:,:,2]=0
b=img1.copy()
#Setredandgreenchannels to0
b[:,:,0]=0
b[:,:,1]=0importcv2ascv
importnumpyasnp
frommatplotlib importpyplotasplt
frommatplotlib.pyplot importfigure
img1=cv.imread ('peppers.jpg' )
#Convert BGRtoRGB!
img1=cv.cvtColor (img1, cv.COLOR_BGR2RGB)
figure(figsize =(15,15),dpi=80)
plt.subplot (131),plt.imshow (r),plt.title ('RedChannel' )
plt.subplot (132),plt.imshow (g),plt.title ('GreenChannel' )
plt.subplot (133),plt.imshow (b),plt.title ('BlueChannel' )
plt.show ()

--- Page 499 ---

Image Padding
importcv2ascv
importnumpyasnp
frommatplotlib importpyplotasplt
frommatplotlib.pyplot importfigureimg1=cv.imread ('peppers.jpg' )#Convert BGRtoRGB!
img1=cv.cvtColor (img1, cv.COLOR_BGR2RGB )
replicate =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_REPLICATE )
figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img1), plt.title ('Original' )
plt.subplot (122),plt.imshow (replicate), plt.title ('Replicate (aaaaaa|abcdefgh|hhhhhhh )')
plt.show ()

--- Page 500 ---

Image Padding ( 2)
GREEN=[0,255,0]
constant_green =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_CONSTANT ,value =GREEN)
RED=[255,0,0]
constant_red =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_CONSTANT ,value =RED)
BLUE=[0,0,255]
constant_blue =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_CONSTANT ,value =BLUE)R     G B

--- Page 501 ---

Image Padding (3)
reflect =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_REFLECT )

--- Page 502 ---

Image Padding (4)
reflect101=cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_REFLECT_ 101)
cv.BORDER_DEFAULT

--- Page 503 ---

Image Padding (5)
wrap=cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_WRAP )
Periodic Image Replication!

--- Page 504 ---

Image Frequency Content
importcv2
importnumpyasnp
frommatplotlib importpyplotasplt
#Reading ingrayscale mode
img=cv2.imread( 'peppers.jpg' ,0)
f=np.fft.fft2( img)
fshift =np.fft.fftshift (f)
magnitude_spectrum =20*np.log( np.abs(fshift ))figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img,cmap='gray')
plt.title ('Spatial Domain'),plt.xticks ([]),plt.yticks ([])
plt.subplot (122),plt.imshow (magnitude_spectrum ,cmap='gray')
plt.title ('Fourier Domain'),plt.xticks ([]),plt.yticks ([])
plt.show ()
f1f2
(0,0)
0 fmax 0 -fmax
0
fmax
0-fmaxScale Fourier coefficients for display purposes

--- Page 505 ---

Image Frequency Content (2)
dft=cv2.dft(np.float32( img), flags =cv2.DFT_COMPLEX_OUTPUT )
dft_shift =np.fft.fftshift (dft)
magnitude_spectrum =20*np.log(cv2.magnitude( dft_shift [:,:,0],dft_shift [:,:,1]))
f1f2
(0,0)

--- Page 506 ---

Image Frequency Content (3)
f=np.fft.fft2( img)
fshift=np.fft.fftshift (f)
magnitude_spectrum =np.abs(fshift )
figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img,cmap='gray')
plt.title ('Spatial Domain'),plt.xticks ([]),plt.yticks ([])
plt.subplot (122),plt.imshow (magnitude_spectrum ,cmap='gray')
plt.title ('Fourier Domain'),plt.xticks ([]),plt.yticks ([])
plt.show ()No scaling!

--- Page 507 ---

Image Filtering
rows,cols=img.shape
crow,ccol=rows//2,cols//2
fshift [crow -30:crow+ 30,ccol -30:ccol+ 30]=0#Filtering the low frequency content!
f_ishift =np.fft.ifftshift (fshift )
img_back =np.fft.ifft2( f_ishift )
img_back =np.abs(img_back )figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img,cmap='gray')
plt.title ('Original' ),plt.xticks ([]),plt.yticks ([])
plt.subplot (122),plt.imshow (img_back ,cmap='gray')
plt.title ('Filtered image'),plt.xticks ([]),plt.yticks ([])
plt.show ()

--- Page 508 ---

Image Filtering (2)
dft=cv2.dft(np.float 32(img),flags =cv2.DFT_COMPLEX_OUTPUT)
dft_shift =np.fft.fftshift (dft)
#Createamaskfirst,centersquareis1,remaining allzeros
mask=np.zeros ((rows,cols ,2),np.uint8)
mask[crow -30:crow+ 30,ccol -30:ccol+ 30]=1
#Applymaskandinverse DFT
fshift =dft_shift *mask
f_ishift =np.fft.ifftshift (fshift)
img_back =cv2.idft(f_ishift )
img_back =cv2.magnitude( img_back [:, :,0],img_back [:, :,1])figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img,cmap='gray')
plt.title ('Original' ),plt.xticks ([]),plt.yticks ([])
plt.subplot (122),plt.imshow (img_back ,cmap='gray')
plt.title ('Filtered' ),plt.xticks ([]),plt.yticks ([])
plt.show ()

--- Page 509 ---

Image Filtering (3)
mask=np.zeros ((rows,cols ,2),np.uint8)
mask[crow -15:crow+15,ccol-15:ccol+15]=1

--- Page 510 ---

Convolution in 1D

--- Page 511 ---

End of Part 1

--- Page 512 ---

Backpropagation ?
Chain RuleAffine
Operatorx(i) h=wTx(i)Activation
Functionp=ÔÅ≥(h)Loss
FunctionL(w)
Note: For w0, the update is based on:

--- Page 513 ---

Backpropagation ?
Note: For each data sample, ( x(i), y(i)), derivatives are evaluated for some values of 
w. 
Hint: Since we have an expression for our network (see previous) slide, we can 
build a function that takes as inputs x(i), y(i)and w, and returns the derivatives that 
we can use in the gradient update. 
Note: This approach works well but does not generalize . For example, if the 
network is changed, a new function for the derivative evaluation is needed!
w1
x(i) h=f(w1, w2, w3, x(i))
w2w3
A new function for 
derivative evaluation is needed!w1
x(i)
w2h=f(w1, w2, w3, w4, w5, x(i))w5w3
w4 A new function for 
derivative evaluation is needed!

--- Page 514 ---

Backpropagation ?
Hint: We need to find a formalism to calculate the derivatives of the loss with 
respect to the model weights such that:
1.It is flexible enough that adding a node or a layer or changing something in the 
network will not require re -deriving the functional form from scratch. 
2.It is exact. 
3.It is computationally efficient.
Note: For each data sample, ( x(i), y(i)), we need to evaluate the derivatives based on 
only x(i), y(i)and the current values of wusing the chain rule of derivatives!

--- Page 515 ---

Backpropagation ?
Variables Derivatives Variable Value Derivative ValueExample: Let x(i)=3, y(i)= 1 and w= 3
ÔÅÜ1= -wTxÔÅ§ÔÅÜ1
ÔÅ§w= -x -9 -3ÔÅ§ÔÅÜi
ÔÅ§w
-3
ÔÅÜ2= eÔÅÜ1= e-wTxÔÅ§ÔÅÜ2
ÔÅ§ÔÅÜ1= eÔÅÜ1 e-9 e-9-3 . e-9
ÔÅÜ3= 1 + ÔÅÜ2 = 1+ e-wTxÔÅ§ÔÅÜ3
ÔÅ§ÔÅÜ2= 1 1 + e-9 1 -3 . e-9 .1
1
ÔÅÜ3ÔÅÜ4=           =1
1+e-wTxÔÅ§ÔÅÜ4
ÔÅ§ÔÅÜ3=-1
ÔÅÜ2
31
1+e-9-1
(1+e-9)2-3 . e-9 .1.-1
(1+e-9)2
ÔÅÜ5= log(ÔÅÜ4)ÔÅ§ÔÅÜ5
ÔÅ§ÔÅÜ4=1
ÔÅÜ41
1+e-9log(             ) 1+e-93 . e-9 .1.1
(1+e-9)2(1+e-9) 3 . e-9 .1.1
(1+e-9)
L(w) = -y . ÔÅÜ5
ÔÅ§L(w)
ÔÅ§ÔÅÜ5=-y1
1+e-9-log(             ) -1 -3 . e-9 .1.1
(1+e-9)
Forward computations: Store all values!

--- Page 516 ---

Backpropagation ?
Variables Derivatives Function Code Derivative Code
ÔÅÜ1= -wTxÔÅ§ÔÅÜ1
ÔÅ§w= -x
ÔÅÜ2= eÔÅÜ1= e-wTxÔÅ§ÔÅÜ2
ÔÅ§ÔÅÜ1= eÔÅÜ1
ÔÅÜ3= 1 + ÔÅÜ2 = 1+ e-wTxÔÅ§ÔÅÜ3
ÔÅ§ÔÅÜ2= 1
1
ÔÅÜ3ÔÅÜ4=           =1
1+e-wTxÔÅ§ÔÅÜ4
ÔÅ§ÔÅÜ3=-1
ÔÅÜ2
3
ÔÅÜ5= log(ÔÅÜ4)ÔÅ§ÔÅÜ5
ÔÅ§ÔÅÜ4=1
ÔÅÜ4
L(w) = -y . ÔÅÜ5ÔÅ§L(w)
ÔÅ§ÔÅÜ5=-yÔÅÜ0= xÔÅ§ÔÅÜ1
ÔÅ§x= 1defx0(x):
return(x)defderx0():
return 1
defx1(w, x):
return(-w*x)defderx1(w, x):
return -w
defx2(x):
return np.exp(x)defderx2(x):
return np.exp(x)
defx3(x):
return 1 + xdefderx3(x):
return 1
defx4(x):
return 1/xdefderx4(x):
return -(1/x)**2
defx5(x):
return np.log(x)defderx5():
return 1/x
deffunc(x, y):
return -y*xdefderfunc(y):
return -y
Note: The partial derivatives involve very basic mathematical and code functions!

--- Page 517 ---

Backpropagation: Simple Framework!
Step 1: Specify the network structure
w1
x(i)
w2h=f(w1, w2, w3, w4, w5, x(i))w5w3
w4
Step 2: Follow the computational graph

--- Page 518 ---

Backpropagation: Computational Graphs
+ÔÇ∏log
log
exp1-
*y*
x w+*--

--- Page 519 ---

Step 3: Go either forward or backward depending on the situation. In general, 
forward is easier to implement and to understand. Backpropagation: Computational Graphs (2)
Step 1: Specify the network structure
w1
x(i)
w2h=f(w1, w2, w3, w4, w5, x(i))w5w3
w4
Step 2: Envision the computational graph (only needed for the reverse mode ). At 
each node of the graph we build two functions: the evaluation of the variable and its 
partial derivative with respect to the previous variable (see details in previous 
table).

--- Page 520 ---

Nice demo at: https://autoed.herokuapp.comAutomatic Differentiation Demo

--- Page 521 ---

Gradient Descent and Stochastic Gradient Descent (SGD)

--- Page 522 ---

Gradient Descent and SGD: Requirements
‚Ä¢We still need to calculate the derivatives. 
‚Ä¢We need to know what is the learning rate or how to set it. 
‚Ä¢Local versus global minima. 
‚Ä¢The full likelihood function includes summing up all individual ‚Äò errors ‚Äô. 
Sometimes this includes hundreds of thousands of examples!

--- Page 523 ---

Derivatives‚Äô Calculations: Example 
Example: Derivative calculations in logistic regression
Basic Concepts: Chain rule in derivatives‚Äô calculations
Scalar variable
Vector variable
Note: For longer chains, we can write:

--- Page 524 ---

Derivatives‚Äô Calculations: Example (2) 
Note: For the logistic regression, the negative log -likelihood function is given by:
li[A]
li[B]
Using the above notation, the derivative of the negative log -likelihood is defined as 
follows:

--- Page 525 ---

Derivatives‚Äô Calculations: Example (3) 
Variable Partial Derivative Partial Derivative

--- Page 526 ---

Derivatives‚Äô Calculations: Example (4) 
Variable Partial Derivative Partial Derivative

--- Page 527 ---

Gradient Descent and SGD: Requirements (2)
‚Ä¢We still need to calculate the derivatives. 
‚Ä¢We need to know what is the learning rate or how to set it. 
‚Ä¢Local versus global minima. 
‚Ä¢The full likelihood function includes summing up all individual ‚Äò errors ‚Äô. 
Sometimes this includes hundreds of thousands of examples!

--- Page 528 ---

Learning Rate ( ÔÅ®)
Note: Our choice of the learning rate has a significant impact on the performance of 
gradient descent. 
When ÔÅ®is too small , the 
algorithm makes very little 
progress . When ÔÅ®is too large , the 
algorithm may overshoot the 
minimum and oscillates too 
much!When ÔÅ®is appropriate (???) , 
the algorithm will find the 
minimum at convergence !

--- Page 529 ---

Learning Rate ( ÔÅ®) (2)
Question: How can we tell when gradient descent is converging? 
Answer: We can visualize the loss function at each step of gradient descent. This is 
called the trace plot . 
While the loss is decreasing throughout 
training, it does not look like descent hit 
the bottomLoss is mostly oscillating between 
values rather than converging
The loss has decreased significantly 
during training. Towards the end, 
the loss stabilizes and it cannot 
decrease further.

--- Page 530 ---

Learning Rate ( ÔÅ®) (3)
Note: There are many alternative methods which address how to set or adjust the 
learning rate, using the derivative or second derivatives (Hessian )and or the 
momentum . We will see more details later!

--- Page 531 ---

Gradient Descent and SGD: Requirements (3)
‚Ä¢We still need to calculate the derivatives. 
‚Ä¢We need to know what is the learning rate or how to set it. 
‚Ä¢Local versus global minima. 
‚Ä¢The full likelihood function includes summing up all individual ‚Äò errors ‚Äô. 
Sometimes this includes hundreds of thousands of examples!

--- Page 532 ---

Local Versus Global Minima
Note: If we choose ÔÅ®correctly, then gradient descent will converge to a stationary 
point. 
Question: Will this point be a global minimum ?
Note: If the loss function is convex then the stationary point will be a global 
minimum .
The MSE of the linear regression is convex !
Check the lecture slides on the MSE convexity!
The second -order derivative (Hessian) is positive semi -definite everywhere!
Every stationary point of the gradient is a 
global minimum Neural networks with different weights can correspond to the same 
function
The loss functions of regression neural 
networks are not convex !Most stationary points are local minima but 
not global optima!

--- Page 533 ---

Local Versus Global Minima (2)
Note: There is no guarantee to get the global minimum!
Question: What would be a good strategy in this case?
Answer:
1.Random restarts
2.Add noise to the loss function

--- Page 534 ---

Gradient Descent and SGD: Requirements (4)
‚Ä¢We still need to calculate the derivatives. 
‚Ä¢We need to know what is the learning rate or how to set it. 
‚Ä¢Local versus global minima. 
‚Ä¢The full likelihood function includes summing up all individual ‚Äòerrors‚Äô. 
Sometimes this includes hundreds of thousands of examples!

--- Page 535 ---

Batch and Stochastic Gradient Descent
Note: Instead of using all the examples for every step, use a subset of them (batch).
For each iteration k, use the following loss function to derive the derivatives: 
Important Note: is an approximation of          !

--- Page 536 ---

Batch and Stochastic Gradient Descent
Complete data processed ÔÉ®1 epoch ! Reshuffle data and repeat (more and more epochs ÔÅå)

--- Page 537 ---

Views on Local Minima
Ideally, we would like to arrive to 
the global minimum ÔÉ®Sometimes
it might not be possible!
This local minima performs poorly 
ÔÉ®Should be avoided!This local minima performs nearly as well 
as the global one  ÔÉ®Acceptable as a halting point !

--- Page 538 ---

Views on Local Minima (2)
Old view: Local minima is major problem in neural network training 
Recent view:
‚Ä¢For sufficiently large neural networks, most local minima incur low cost! 
‚Ä¢Not important to find true global minimum

--- Page 539 ---

Probability & Statistics Concepts 4 AI
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti 2019 @gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 540 ---

Data Sampling
Reminder: It is very straightforward to generate random data by sampling from a given 
data distribution ‚ò∫

--- Page 541 ---

Data Sampling ( 2)
samples = np.random.uniform( 0, 1, 1000)Uniform Distribution
a b# samples
a = -20
b = 35
c = (b -a)
n = 1000
samples = c * np.random.uniform( 0, 1, n) + aa b1
b-ap(x)
xNormal (Gaussian) Distribution
p(x)
xŒº = 0
œÉ2= 1
samples = np.random.normal( 0, 1, 1000)ŒºœÉ# samples
mu = 15
std = 5
samples = mu + 
(std * np.random.normal( 0, 1, 1000))

--- Page 542 ---

Data Sampling ( 3)
num_samples = 30000
image_shape = ( 100, 100, 3) 
num_repeats = 100
filenames = []
for_ inrange(num_repeats):
samples = np.random.randint( 0, 256, num_samples)
r = samples[ 0:10000].reshape(( 100, 100))
g = samples[ 10000:20000].reshape(( 100, 100))
b = samples[ 20000:30000].reshape(( 100, 100))
rgb_image = np.stack((r, g, b), axis = -1).astype(np.uint 8)
plt.imshow(rgb_image)
plt.axis( 'off') # Hide axes
filename = f'color_image_ {_}.png'
plt.savefig(filename, bbox_inches = 'tight', pad_inches = 0)
plt.close()
filenames.append(filename)
withimageio.get_writer( '/content/color_images.gif' , mode = 'I', duration = 0.5) aswriter:
forfilename infilenames:
image = imageio.imread(filename)
writer.append_data(image)Discrete version of Uniform( 0, 255)
Random color images ‚òπ

--- Page 543 ---

Data Sampling ( 4)
importnumpy asnp
fromIPython.display importAudio, display
duration = 5
sampling_rate = 4000
audio_samples = np.random.uniform( 0, 1, int(sampling_rate * duration))
display(Audio(audio_samples, rate = sampling_rate, autoplay = False))

--- Page 544 ---

Data Sampling ( 5)
mu = 15
std = 5
samples = mu + std * np.random.normal( 0, 1, 1000)
plt.figure(figsize = ( 10, 6))
plt.hist(samples, bins = 30, density = True, alpha = 0.7, color = 'blue', edgecolor = 'black')
plt.title( 'Histogram of $ \mathcal{N}( \mu = 15 , \sigma = 5) $')
plt.xlabel( 'x')
plt.ylabel( '$\mathbf{p(x)} $' )
plt.grid(axis = 'y', alpha = 0.75)
plt.show()

--- Page 545 ---

Data Sampling ( 6)
Mean vector ‚àà‚Ñúk
Covariance matrix ‚àà‚Ñúkxk
Determinant of ‚àëmu_vec = np.random.uniform( -10, 10, 100)
diag_vals = np.random.uniform( 1, 5, 100)
cov = np.random.uniform( 0.0, 1.0, (100, 100))
cov = (cov + cov.T) / 2
np.fill_diagonal(cov, diag_vals)
vals = np.random.multivariate_normal(mu_vec, cov, 1000)
Œº ‚àë # samples????

--- Page 546 ---

‚Ä¢Probability density function (PDF): p(x)
‚Ä¢Expected Value (Population Mean): Œº = E(x)
‚Ä¢Population Variance: œÉ2 = Var(x) = E[(x ‚àíŒº)2]Important Probability Concepts
Data Population
œÉ2 = Var(x) = 
E[(x‚àíŒº)2]= E[(x2‚àí2Œºx+Œº2]= E[(x2)] ‚àí 2ŒºE[x] + E[Œº2]Linear operator ‚ò∫ Œº
= E[(x2)] ‚àí (E[x])2 = E[(x2)] ‚àí 2ŒºŒº + Œº2-Œº2= -(E[x])2
Discrete Data
Start valueEnd value
Var(x)= E[(x2)] ‚àí (E[x])2

--- Page 547 ---

Data PopulationData SampleImportant Probability Concepts ( 2)
m data points only ‚òπInfinite data points only 
‚ò∫
Mias Dataset
 VoxCeleb Dataset
MNIST Dataset
CIFAR 
Dataset
Imagenet Dataset
COCO Dataset

--- Page 548 ---

Data PopulationData Sample
Important Probability Concepts ( 3)

--- Page 549 ---

Important Probability Concepts ( 4)
p(x)q(x)
How close is p(x) 
to q(x)?
How close is q(x) 
to p(x)? True 
PDFApproximate PDF
Kullaback -Leibler Divergence

--- Page 550 ---

Important Note 1:DKL(P ‚à•Q)measures how much information is lost when a model
pdf q(x) is used to approximate the true (or theoretical) pdf p(x). Important Probability Concepts ( 5)
Important Note 2:DKL(P ‚à•Q) ‚â†DKL(Q ‚à•P)
Important Note 3:The KLD is also defined for discrete distributions as follows :
kld = np. sum(np.where(pdf 1 > 0, pdf1 * np.log(pdf 1 / np.maximum(pdf 2, 1e-10)), 0))Avoids log( 0) Avoids division by 0KLD is not 
symmetric ‚òπKLD is not a 
distance ‚òπ

--- Page 551 ---

The KLD between 2 multivariate PDFs is defined as follows :Important Probability Concepts ( 6)
Given 2 Gaussian PDFs p(x) and q(x), ‚Ñµ(Œº1, œÉ2
1) and ‚Ñµ(Œº2, œÉ2
2), their KLD is given by :
Fixed/Target/Given Approximation/Learnable
Œº1= 0, œÉ12= 1= 1
Standard Gaussian

--- Page 552 ---

Important Probability Concepts ( 7)
p(x)
 q(x)
Softmax layer
q(x)
Cross -Entropy is 
KLD in disguise ‚ò∫
Using 
training

--- Page 553 ---

xz
x^
p(x)
p(z)p(x, z)
p(x): Marginal Probability p(z): Prior Probabilityp(x, z): Joint Probability
Bayes Theorem:
p(x, z) = p( z) .p(x| z)
p(x, z) = p( x) .p(z| x)p(x| z): Likelihood
p(z| x): Posterior Probabilityx‚àà‚ÑúKz‚àà‚Ñúh
h <<< K
Integrate over z ü°∫Get rid of z ‚ò∫Numerically intractable ‚òπ
Thomas Bayes
(1701 -1761 )Important Probability Concepts ( 8)

--- Page 554 ---

Modality (Image) Statistics
p(x) = ?
p(x) = e-||L||||L||

--- Page 555 ---

Modality (Image) Statistics ( 2)
DCT representation
 DCT representation
p(x) = e-||C||
CC

--- Page 556 ---

Image Processing / Computer Vision : 
A Primer
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti 2019 @gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 557 ---

Image Channels
r=img1.copy()
#Setgreenandbluechannels to0
r[:,:,1]=0
r[:,:,2]=0
g=img1.copy()
#Setredandbluechannels to0
g[:,:,0]=0
g[:,:,2]=0
b=img1.copy()
#Setredandgreenchannels to0
b[:,:,0]=0
b[:,:,1]=0importcv2ascv
importnumpyasnp
frommatplotlib importpyplotasplt
frommatplotlib.pyplot importfigure
img1=cv.imread ('peppers.jpg' )
#Convert BGRtoRGB!
img1=cv.cvtColor (img 1,cv.COLOR_BGR 2RGB)
figure(figsize =(15,15),dpi=80)
plt.subplot (131),plt.imshow (r),plt.title ('RedChannel' )
plt.subplot (132),plt.imshow (g),plt.title ('GreenChannel' )
plt.subplot (133),plt.imshow (b),plt.title ('BlueChannel' )
plt.show ()

--- Page 558 ---

Image Padding
importcv2ascv
importnumpyasnp
frommatplotlib importpyplotasplt
frommatplotlib.pyplot importfigureimg1=cv.imread ('peppers.jpg' )#Convert BGRtoRGB!
img1=cv.cvtColor (img1,cv.COLOR_BGR 2RGB)
replicate =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_REPLICATE )
figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img1), plt.title ('Original' )
plt.subplot (122),plt.imshow (replicate), plt.title ('Replicate (aaaaaa|abcdefgh|hhhhhhh )')
plt.show ()

--- Page 559 ---

Image Padding (2)
GREEN=[0,255,0]
constant_green =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_CONSTANT ,value =GREEN)
RED=[255,0,0]
constant_red =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_CONSTANT ,value =RED)
BLUE=[0,0,255]
constant_blue =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_CONSTANT ,value =BLUE)R     G B

--- Page 560 ---

Image Padding ( 3)
reflect =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_REFLECT )

--- Page 561 ---

Image Padding (4)
reflect101 =cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_REFLECT_101 )
cv.BORDER_DEFAULT

--- Page 562 ---

Image Padding ( 5)
wrap=cv.copyMakeBorder (img1,50,50,50,50,cv.BORDER_WRAP )
Periodic Image Replication!

--- Page 563 ---

Image Frequency Content
importcv2
importnumpyasnp
frommatplotlib importpyplotasplt
#Reading ingrayscale mode
img=cv2.imread( 'peppers.jpg' ,0)
f=np.fft.fft 2(img)
fshift =np.fft.fftshift (f)
magnitude_spectrum =20*np.log( np.abs(fshift ))figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img,cmap='gray')
plt.title ('Spatial Domain'),plt.xticks ([]),plt.yticks ([])
plt.subplot (122),plt.imshow (magnitude_spectrum ,cmap='gray')
plt.title ('Fourier Domain'),plt.xticks ([]),plt.yticks ([])
plt.show ()
f1f2
(0,0)
0 fmax 0 -fmax
0
fmax
0-fmaxScale Fourier coefficients for display purposes

--- Page 564 ---

Image Frequency Content ( 2)
dft=cv2.dft(np.float 32(img), flags =cv2.DFT_COMPLEX_OUTPUT )
dft_shift =np.fft.fftshift (dft)
magnitude_spectrum =20*np.log(cv 2.magnitude( dft_shift [:,:,0],dft_shift [:,:,1]))
f1f2
(0,0)

--- Page 565 ---

Image Frequency Content (3)
f=np.fft.fft 2(img)
fshift=np.fft.fftshift (f)
magnitude_spectrum =np.abs(fshift )
figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img,cmap='gray')
plt.title ('Spatial Domain'),plt.xticks ([]),plt.yticks ([])
plt.subplot (122),plt.imshow (magnitude_spectrum ,cmap='gray')
plt.title ('Fourier Domain'),plt.xticks ([]),plt.yticks ([])
plt.show ()No scaling!

--- Page 566 ---

Image Filtering
rows,cols=img.shape
crow,ccol=rows//2,cols//2
fshift [crow -30:crow+ 30,ccol -30:ccol+ 30]=0#Filtering the low frequency content!
f_ishift =np.fft.ifftshift (fshift )
img_back =np.fft.ifft 2(f_ishift )
img_back =np.abs(img_back )figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img,cmap='gray')
plt.title ('Original' ),plt.xticks ([]),plt.yticks ([])
plt.subplot (122),plt.imshow (img_back ,cmap='gray')
plt.title ('Filtered image'),plt.xticks ([]),plt.yticks ([])
plt.show ()

--- Page 567 ---

Image Filtering (2)
dft=cv2.dft(np.float32( img),flags =cv2.DFT_COMPLEX_OUTPUT)
dft_shift =np.fft.fftshift (dft)
#Createamaskfirst,centersquareis1,remaining allzeros
mask=np.zeros ((rows,cols ,2),np.uint8)
mask[crow -30:crow+ 30,ccol -30:ccol+ 30]=1
#Applymaskandinverse DFT
fshift =dft_shift *mask
f_ishift =np.fft.ifftshift (fshift)
img_back =cv2.idft( f_ishift )
img_back =cv2.magnitude( img_back [:, :,0],img_back [:, :,1])figure(figsize =(15,15),dpi=80)
plt.subplot (121),plt.imshow (img,cmap='gray')
plt.title ('Original' ),plt.xticks ([]),plt.yticks ([])
plt.subplot (122),plt.imshow (img_back ,cmap='gray')
plt.title ('Filtered' ),plt.xticks ([]),plt.yticks ([])
plt.show ()

--- Page 568 ---

Image Filtering ( 3)
mask=np.zeros ((rows,cols ,2),np.uint8)
mask[crow -15:crow+15,ccol-15:ccol+15]=1

--- Page 569 ---

Convolution in 1D

--- Page 570 ---

End of Part 1

--- Page 571 ---

Backpropagation ?
Chain RuleAffine
Operatorx(i) h=wTx(i)Activation
Functionp=ÔÅ≥(h)Loss
FunctionL(w)
Note: For w0, the update is based on:

--- Page 572 ---

Backpropagation ?
Note: For each data sample, ( x(i), y(i)), derivatives are evaluated for some values of 
w. 
Hint: Since we have an expression for our network (see previous) slide, we can 
build a function that takes as inputs x(i), y(i)and w, and returns the derivatives that 
we can use in the gradient update. 
Note: This approach works well but does not generalize . For example, if the 
network is changed, a new function for the derivative evaluation is needed!
w1
x(i) h=f(w1, w2, w3, x(i))
w2w3
A new function for 
derivative evaluation is needed!w1
x(i)
w2h=f(w1, w2, w3, w4, w5, x(i))w5w3
w4 A new function for 
derivative evaluation is needed!

--- Page 573 ---

Backpropagation ?
Hint: We need to find a formalism to calculate the derivatives of the loss with 
respect to the model weights such that:
1.It is flexible enough that adding a node or a layer or changing something in the 
network will not require re -deriving the functional form from scratch. 
2.It is exact. 
3.It is computationally efficient.
Note: For each data sample, ( x(i), y(i)), we need to evaluate the derivatives based on 
only x(i), y(i)and the current values of wusing the chain rule of derivatives!

--- Page 574 ---

Backpropagation ?
Variables Derivatives Variable Value Derivative ValueExample: Let x(i)=3, y(i)= 1 and w= 3
ÔÅÜ1= -wTxÔÅ§ÔÅÜ1
ÔÅ§w= -x -9 -3ÔÅ§ÔÅÜi
ÔÅ§w
-3
ÔÅÜ2= eÔÅÜ1= e-wTxÔÅ§ÔÅÜ2
ÔÅ§ÔÅÜ1= eÔÅÜ1 e-9 e-9-3 . e-9
ÔÅÜ3= 1 + ÔÅÜ2 = 1+ e-wTxÔÅ§ÔÅÜ3
ÔÅ§ÔÅÜ2= 1 1 + e-9 1 -3 . e-9 .1
1
ÔÅÜ3ÔÅÜ4=           =1
1+e-wTxÔÅ§ÔÅÜ4
ÔÅ§ÔÅÜ3=-1
ÔÅÜ2
31
1+e-9-1
(1+e-9)2-3 . e-9 .1.-1
(1+e-9)2
ÔÅÜ5= log(ÔÅÜ4)ÔÅ§ÔÅÜ5
ÔÅ§ÔÅÜ4=1
ÔÅÜ41
1+e-9log(             ) 1+e-93 . e-9 .1.1
(1+e-9)2(1+e-9) 3 . e-9 .1.1
(1+e-9)
L(w) = -y . ÔÅÜ5
ÔÅ§L(w)
ÔÅ§ÔÅÜ5=-y1
1+e-9-log(             ) -1 -3 . e-9 .1.1
(1+e-9)
Forward computations: Store all values!

--- Page 575 ---

Backpropagation ?
Variables Derivatives Function Code Derivative Code
ÔÅÜ1= -wTxÔÅ§ÔÅÜ1
ÔÅ§w= -x
ÔÅÜ2= eÔÅÜ1= e-wTxÔÅ§ÔÅÜ2
ÔÅ§ÔÅÜ1= eÔÅÜ1
ÔÅÜ3= 1 + ÔÅÜ2 = 1+ e-wTxÔÅ§ÔÅÜ3
ÔÅ§ÔÅÜ2= 1
1
ÔÅÜ3ÔÅÜ4=           =1
1+e-wTxÔÅ§ÔÅÜ4
ÔÅ§ÔÅÜ3=-1
ÔÅÜ2
3
ÔÅÜ5= log(ÔÅÜ4)ÔÅ§ÔÅÜ5
ÔÅ§ÔÅÜ4=1
ÔÅÜ4
L(w) = -y . ÔÅÜ5ÔÅ§L(w)
ÔÅ§ÔÅÜ5=-yÔÅÜ0= xÔÅ§ÔÅÜ1
ÔÅ§x= 1defx0(x):
return(x)defderx0():
return 1
defx1(w, x):
return(-w*x)defderx1(w, x):
return -w
defx2(x):
return np.exp(x)defderx2(x):
return np.exp(x)
defx3(x):
return 1 + xdefderx3(x):
return 1
defx4(x):
return 1/xdefderx4(x):
return -(1/x)**2
defx5(x):
return np.log(x)defderx5():
return 1/x
deffunc(x, y):
return -y*xdefderfunc(y):
return -y
Note: The partial derivatives involve very basic mathematical and code functions!

--- Page 576 ---

Backpropagation: Simple Framework!
Step 1: Specify the network structure
w1
x(i)
w2h=f(w1, w2, w3, w4, w5, x(i))w5w3
w4
Step 2: Follow the computational graph

--- Page 577 ---

Backpropagation: Computational Graphs
+ÔÇ∏log
log
exp1-
*y*
x w+*--

--- Page 578 ---

Step 3: Go either forward or backward depending on the situation. In general, 
forward is easier to implement and to understand. Backpropagation: Computational Graphs (2)
Step 1: Specify the network structure
w1
x(i)
w2h=f(w1, w2, w3, w4, w5, x(i))w5w3
w4
Step 2: Envision the computational graph (only needed for the reverse mode ). At 
each node of the graph we build two functions: the evaluation of the variable and its 
partial derivative with respect to the previous variable (see details in previous 
table).

--- Page 579 ---

Nice demo at: https://autoed.herokuapp.comAutomatic Differentiation Demo

--- Page 580 ---

Gradient Descent and Stochastic Gradient Descent (SGD)

--- Page 581 ---

Gradient Descent and SGD: Requirements
‚Ä¢We still need to calculate the derivatives. 
‚Ä¢We need to know what is the learning rate or how to set it. 
‚Ä¢Local versus global minima. 
‚Ä¢The full likelihood function includes summing up all individual ‚Äò errors ‚Äô. 
Sometimes this includes hundreds of thousands of examples!

--- Page 582 ---

Derivatives‚Äô Calculations: Example 
Example: Derivative calculations in logistic regression
Basic Concepts: Chain rule in derivatives‚Äô calculations
Scalar variable
Vector variable
Note: For longer chains, we can write:

--- Page 583 ---

Derivatives‚Äô Calculations: Example (2) 
Note: For the logistic regression, the negative log -likelihood function is given by:
li[A]
li[B]
Using the above notation, the derivative of the negative log -likelihood is defined as 
follows:

--- Page 584 ---

Derivatives‚Äô Calculations: Example (3) 
Variable Partial Derivative Partial Derivative

--- Page 585 ---

Derivatives‚Äô Calculations: Example (4) 
Variable Partial Derivative Partial Derivative

--- Page 586 ---

Gradient Descent and SGD: Requirements (2)
‚Ä¢We still need to calculate the derivatives. 
‚Ä¢We need to know what is the learning rate or how to set it. 
‚Ä¢Local versus global minima. 
‚Ä¢The full likelihood function includes summing up all individual ‚Äò errors ‚Äô. 
Sometimes this includes hundreds of thousands of examples!

--- Page 587 ---

Learning Rate ( ÔÅ®)
Note: Our choice of the learning rate has a significant impact on the performance of 
gradient descent. 
When ÔÅ®is too small , the 
algorithm makes very little 
progress . When ÔÅ®is too large , the 
algorithm may overshoot the 
minimum and oscillates too 
much!When ÔÅ®is appropriate (???) , 
the algorithm will find the 
minimum at convergence !

--- Page 588 ---

Learning Rate ( ÔÅ®) (2)
Question: How can we tell when gradient descent is converging? 
Answer: We can visualize the loss function at each step of gradient descent. This is 
called the trace plot . 
While the loss is decreasing throughout 
training, it does not look like descent hit 
the bottomLoss is mostly oscillating between 
values rather than converging
The loss has decreased significantly 
during training. Towards the end, 
the loss stabilizes and it cannot 
decrease further.

--- Page 589 ---

Learning Rate ( ÔÅ®) (3)
Note: There are many alternative methods which address how to set or adjust the 
learning rate, using the derivative or second derivatives (Hessian )and or the 
momentum . We will see more details later!

--- Page 590 ---

Gradient Descent and SGD: Requirements (3)
‚Ä¢We still need to calculate the derivatives. 
‚Ä¢We need to know what is the learning rate or how to set it. 
‚Ä¢Local versus global minima. 
‚Ä¢The full likelihood function includes summing up all individual ‚Äò errors ‚Äô. 
Sometimes this includes hundreds of thousands of examples!

--- Page 591 ---

Local Versus Global Minima
Note: If we choose ÔÅ®correctly, then gradient descent will converge to a stationary 
point. 
Question: Will this point be a global minimum ?
Note: If the loss function is convex then the stationary point will be a global 
minimum .
The MSE of the linear regression is convex !
Check the lecture slides on the MSE convexity!
The second -order derivative (Hessian) is positive semi -definite everywhere!
Every stationary point of the gradient is a 
global minimum Neural networks with different weights can correspond to the same 
function
The loss functions of regression neural 
networks are not convex !Most stationary points are local minima but 
not global optima!

--- Page 592 ---

Local Versus Global Minima (2)
Note: There is no guarantee to get the global minimum!
Question: What would be a good strategy in this case?
Answer:
1.Random restarts
2.Add noise to the loss function

--- Page 593 ---

Gradient Descent and SGD: Requirements (4)
‚Ä¢We still need to calculate the derivatives. 
‚Ä¢We need to know what is the learning rate or how to set it. 
‚Ä¢Local versus global minima. 
‚Ä¢The full likelihood function includes summing up all individual ‚Äòerrors‚Äô. 
Sometimes this includes hundreds of thousands of examples!

--- Page 594 ---

Batch and Stochastic Gradient Descent
Note: Instead of using all the examples for every step, use a subset of them (batch).
For each iteration k, use the following loss function to derive the derivatives: 
Important Note: is an approximation of          !

--- Page 595 ---

Batch and Stochastic Gradient Descent
Complete data processed ÔÉ®1 epoch ! Reshuffle data and repeat (more and more epochs ÔÅå)

--- Page 596 ---

Views on Local Minima
Ideally, we would like to arrive to 
the global minimum ÔÉ®Sometimes
it might not be possible!
This local minima performs poorly 
ÔÉ®Should be avoided!This local minima performs nearly as well 
as the global one  ÔÉ®Acceptable as a halting point !

--- Page 597 ---

Views on Local Minima (2)
Old view: Local minima is major problem in neural network training 
Recent view:
‚Ä¢For sufficiently large neural networks, most local minima incur low cost! 
‚Ä¢Not important to find true global minimum

--- Page 598 ---

Poor Conditioning 
Poorly conditioned Hessian matrix : High curvature
ÔÉ®Small steps leads to huge increase
ÔÉ®Learning is slow despite strong gradients
Oscillations slow down progress!

--- Page 599 ---

Loss Functions with No Critical Points!
Note: Some cost functions do not have critical points. In particular classification.
importnumpyasnp
importmatplotlib.pyplot asplt
#Sigmoid function
defsigmoid(z):
return1.0/(1.0+np.exp(-z))
#yHatrepresents theprobability valuecalculated asoutputof
thesigmoid function
#yrepresents theactuallabel
defcross_entropy_loss (yHat,y):
ify==1:
return-np.log(yHat)
else:
return-np.log(1-yHat)z=np.arange (-10,10,0.1)
h_z=sigmoid(z)
cost_1=cross_entropy_loss (h_z,1)
cost_0=cross_entropy_loss (h_z,0)
fig,ax=plt.subplots (figsize=(8,6))
plt.plot (h_z,cost_1, label='$\mathcal{L}(\mathbf{w})$ify=1')
plt.plot (h_z,cost_0, label='$\mathcal{L}(\mathbf{w})$ify=0')
plt.xlabel ('$1/(1+e^{ \mathbf{w}^{T}\mathbf{x}})$')
plt.ylabel ('$\mathcal{L}(\mathbf{w})$')
plt.legend (loc='best')
plt.tight_layout ()
plt.show ()

--- Page 600 ---

Exploding and Vanishing Gradients 
Issue 1: Exploding gradients lead to cliffs
Solution 1: Exploding gradients can be mitigated using gradient clipping
Issue 2: Vanishing gradients prevent learning!
Solution 2: Various techniques can be applied. More to come in next slides!

--- Page 601 ---

Momentum
Note: Oscillations because updates do not exploit curvature information
w1w2

--- Page 602 ---

Momentum (2)
Hint: Develop an algorithm that will lead us to the minimum faster.
w1w2

--- Page 603 ---

Momentum (3)
Note: Near point B, the gradient will almost not 
move (a plateau!).Start point
High -magnitude gradients
ÔÉ®Fast descent!
Low -magnitude gradients
ÔÉ®Slow move!
Global minima!
Question: How can we benefit from the descent momentum accumulated before 
reaching Point B?

--- Page 604 ---

Momentum (4)
Use of ‚Äúaccumulated momentum‚Äù

--- Page 605 ---

Momentum (5)

--- Page 606 ---

Momentum (6)
Hint: Compute an exponentially -weighted average of the past gradients and use 
this average in the gradient update rule!
The momentum term Œ≥is usually set to 0.9 or a similar value as such values will 
accommodate more gradients from the past.

--- Page 607 ---

Momentum (7)
Larger ÔÅ®

--- Page 608 ---

Momentum (8)
w1w2
Large stepsSmall steps
Average gradient

--- Page 609 ---

Adaptive Learning Rate

--- Page 610 ---

AdaGrad Optimizer
Note: Greater progress along gently -sloped directions.
Hint: We are making the learning rate variable and inversely proportional to the 
gradient magnitude!ÔÅ®k
Avoids division by 0Accumulated gradients!
Inversely proportional to all cumulated gradients

--- Page 611 ---

Root Mean Squared Propagation ( RMSProp ) Optimizer
Solution: To mitigate the rapid decay of the learning rate using the exponential 
moving averages of squared past gradients, we can limit the update to only the 
past few gradients .Issue: The AdaGrad optimizer works well for sparse settings . However, its 
performance deteriorates in settings where the loss functions are nonconvex and 
gradients are dense due to the rapid decay of the learning rate in these settings 
since it uses all past gradients in the update .

--- Page 612 ---

Adam Optimizer
Hint: Let us combine the momentum and RMPProp ptimizers ÔÉ®adam optimizer !
Step 1: Estimate the exponentially -weighted average (EWA) of the first moment :
Step 2: Estimate the EWA of the second moment :
Step 3: Perform the weight update using the adam step:
Note: A bias correction is applied on the EWA of first and second moments

--- Page 613 ---

Bias Correction in Adam Optimizer

--- Page 614 ---

LeNet5 in 5 Minutes ‚ò∫
Dr. Lahouari Ghouti, PhD
Prince Sultan University. Riyadh, Saudi Arabia
Email: lghouti@psu.edu.sa
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 615 ---

LeNet 5 Model

--- Page 616 ---

# Defining theconvolutional neuralnetwork
classLeNet5(nn.Module):
def__init__ (self,num_classes ):
super(LeNet5, self).__init__ ()
self.layer1 =nn.Sequential(
nn.Conv2d( 1,6,kernel_size = 5,stride = 1,padding = 0),
nn.BatchNorm2d( 6),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))
self.layer2 =nn.Sequential(
nn.Conv2d( 6,16,kernel_size = 5,stride = 1,padding = 0),
nn.BatchNorm2d( 16),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))
self.fc=nn.Linear( 400,120)
self.relu=nn.ReLU()
self.fc1=nn.Linear( 120,84)
self.relu1=nn.ReLU()
self.fc2=nn.Linear( 84,num_classes)Pytorch Implementation of LeNet5 Model

--- Page 617 ---

defforward(self,x):
out=self.layer1(x)
out=self.layer2(out)
out=out.reshape(out.size( 0),-1)
out=self.fc(out)
out=self.relu(out)
out=self.fc1(out)
out=self.relu1(out)
out=self.fc2(out)
returnoutPytorch Implementation of LeNet5 Model (2)
Input tensor
x
layer1 layer 2out outflattenoutoutReLUout outReLUout out

--- Page 618 ---

self.layer1 =nn.Sequential(
nn.Conv2d( 1,6,kernel_size = 5,stride = 1,padding = 0),
nn.BatchNorm2d( 6),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))
self.layer2 =nn.Sequential(
nn.Conv2d( 6,16,kernel_size = 5,stride = 1,padding = 0),
nn.BatchNorm2d( 16),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))
self.fc=nn.Linear( 400,120)
self.relu=nn.ReLU()
self.fc1=nn.Linear( 120,84)
self.relu1=nn.ReLU()
self.fc2=nn.Linear( 84,num_classes)Pytorch Implementation of LeNet5 Model (3)

--- Page 619 ---

self.layer1 =nn.Sequential(
nn.Conv2d( 1,6,kernel_size = 5,stride = 1,padding = 0),
nn.BatchNorm2d( 6),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))Pytorch Implementation of LeNet5 Model (4)
BN 
Layerout out ReLU 
Layerout outPool 
Layer
outout
6 28x28 feature maps
x
32x326 5x5 kernels
6 28x28 feature maps
 6 28x28 feature maps
6 28x28 feature mapsout
6 14x14 feature maps

--- Page 620 ---

16 5x5 
kernels
self.layer2 =nn.Sequential(
nn.Conv2d( 6,16,kernel_size = 5,stride = 1,padding = 0),
nn.BatchNorm2d( 16),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))Pytorch Implementation of LeNet5 Model (5)
6 14x14 
feature maps
 16 10x10 
feature mapsBN 
Layerout out
16 10x10 
feature mapsReLU 
Layerout out
16 10x10 
feature mapsPool 
Layerout out
16 5x5 
feature maps

--- Page 621 ---

Hrf x Wrf
Receptive Field
nn.Conv2d( 1,6,kernel_size = 5,stride = 1,padding = 0)
Kernel 
size
F x F
kernelPadding
Stride
stride = 1 Wfm= 28
Hfm= 28Image Convolution Using FxF Kernels (Filters)
Feature Map

--- Page 622 ---

Image Convolution Using FxF Kernels (Filters) (2)
tensor([[[[ 0.2302, -0.0125, 0.3277], 
[-0.0384, -0.0639, 0.2555], 
[-0.2883, 0.0586, 0.2867]]]])3x3
Kernel

--- Page 623 ---

Image Convolution Using FxF Kernels (Filters) (3)
tensor([[[[ 0.1452, -0.1186, 0.0369, -0.0524, 0.1772], 
[-0.1608, 0.1622, 0.1856, -0.0953, 0.0269], 
[ 0.1805, 0.1490, -0.1163, -0.0849, 0.0775], 
[-0.1633, 0.1674, 0.1797, -0.1629, 0.0872], 
[ 0.0753, -0.0892, 0.1328, -0.1449, 0.1925]]]])5x5
Kernel

--- Page 624 ---

Image Convolution Using FxF Kernels (Filters) (4)
tensor([[[[ -4.8637e-02, -3.6348e-02, 5.8196e -02, 3.1129e -02, 2.8892e -03, 4.7901e -02, -4.5319e-02, -1.7510e-02, 1.6188e -03, 4.274 9e-02, 3.4327e -02, -4.8034e-02, -5.2156e-02, -7.7014e-03, -2.8199e-02, 
-4.1171e-02], [ 1.1094e -02, 4.4938e -02, -1.4577e-02, 5.7923e -02, 2.2493e -02, 1.7187e -02, 4.9367e -02, 2.5299e -02, 5.6092e -02, 3.8450e-02, -8.9726e-03, -4.6109e-02, 5.4561e -02, 4.8675e -02, 4.2675e -02, 
-4.7146e-02], [-2.5647e-04, -3.5408e-02, -3.6866e-02, 4.9970e -02, -1.6128e-02, 5.2877e -02, -6.0837e-02, 1.9619e -02, 5.9778e -03, -1.0375e-03, 2.0638e -02, 2.4966e -02, -4.3769e-02, -8.1069e-03, 
1.2155e-02, -5.1751e-02], [ 4.6026e -02, 3.7067e -02, -3.7290e-02, -3.2774e-02, 4.9995e -02, 5.5436e -02, 5.0742e -02, 1.9087e -02, 4.9147e-02, -1.6310e-02, -2.7633e-02, -5.1010e-03, 4.2811e -02, -1.6855e-
02, 5.4953e -02, 3.6755e -02], [-5.8602e-02, 4.6460e -02, -3.7226e-02, 3.1313e -02, 4.0956e -02, -3.9051e-02, 5.5208e -02, -4.8029e-02, -1.7915e-02, -5.1606e-02, -6.1225e-02, 3.6331e -02, -2.0103e-02, -
6.1505e-02, -1.1195e-03, 3.8312e -02], [ 5.4286e -02, -3.3655e-03, -2.7274e-02, -1.4354e-02, -5.4846e-02, -3.7321e-02, -4.8424e-02, -5.3648e-02, 3.1602e -02, -1.2157e-02, -5.1523e-02, 1.1342e -02, -
3.5496e-02, 4.8565e -02, -3.1632e-02, -5.1151e-02], [-6.2163e-02, -5.8495e-02, -1.9743e-02, 3.7434e -02, 5.5533e -02, -5.8216e-02, 3.4085e-02, 5.8557e -02, 3.3111e -02, 4.6880e -02, 4.0356e -03, -4.3999e-
02, 9.8029e -03, 5.5880e -02, -2.9073e-03, -4.6396e-02], [ 1.1954e -02, -1.4724e-02, -2.4598e-02, -3.9056e-03, -1.2897e-02, 1.7851e -02, 5.7795e -02, 2.1817e -02, -1.2957e-02, 4.0333e -02, -2.2852e-02, 
4.9428e-03, -5.7856e-02, 3.8822e -02, 2.3773e -02, 4.6632e -02], [-1.9137e-02, -1.6837e-02, -3.5776e-02, -1.9032e-02, 2.4146e -02, -8.0244e-03, -7.3309e-03, 3.9448e -02, -3.8923e-02, -1.2072e-02, -
2.7292e-02, -2.7695e-02, -9.7499e-03, 6.0403e -02, 1.4079e -02, -5.8788e-02], [-5.3789e-03, -2.4109e-02, 4.0122e -02, 1.9008e -02, -3.8390e-02, -3.9399e-02, 2.4875e -02, -1.8354e-02, 2.8671e -03, 4.5585e -
02, 1.1902e -02, -4.8626e-02, -5.4750e-02, -4.2731e-02, 3.2204e -02, -1.9501e-02], [ 1.1989e -02, 1.4599e -02, -4.0592e-02, 1.2783e -02, -2.4330e-02, -5.3702e-02, -1.9679e-02, -5.6711e-02, 1.8407e -02, 
2.8302e-02, 3.8085e -02, -3.3848e-02, -6.1110e-02, 2.4325e -03, 5.8630e -02, 1.5033e -02], [ 5.2405e -03, -6.1083e-02, -2.7112e-02, -2.8636e-02, 4.4161e -02, 3.4356e -02, 4.7493e -02, -2.9973e-02, 3.0376e -
02, 2.5776e -02, -4.8128e-02, 9.8885e -04, 1.7498e -02, 3.1136e -02, 1.4549e -02, -4.1624e-02], [-4.5530e-02, -3.8363e-02, 5.6501e -02, 5.6844e -02, -5.6603e-02, -1.4318e-02, -8.9273e-03, -6.0003e-02, 
2.0866e-02, 3.1362e -02, 3.7527e -02, -5.3649e-02, 1.2359e -02, -5.2679e-02, -3.8416e-02, -9.1586e-03], [ 5.1646e -02, -4.8935e-02, -7.0542e-03, 9.5618e -04, -4.2151e-02, -2.7146e-02, -2.2532e-03, 
4.4622e-02, 3.3859e -02, -7.4185e-05, -1.1687e-02, -1.9793e-02, -5.6242e-02, 1.7936e -02, 1.0139e -03, -4.5829e-02], [-5.7047e-02, 4.7625e-02, -1.3917e-02, 1.8305e -02, -3.4003e-02, 2.0040e -02, -
3.5256e-02, -4.3960e-02, -4.0603e-02, -2.0620e-02, 5.2334e -02, 3.5702e -03, -6.0244e-03, 5.6837e -02, 4.0207e -02, 4.2603e -02], [ 3 .3670e-02, 3.5093e -02, 5.0031e -02, 5.9858e -02, -1.5740e-02, 4.2103e -
02, -4.4771e-02, -6.9506e-03, -6.2488e-02, -3.1298e-02, -5.9653e-02, -3.3814e-02, 3.1718e -02, 2.6800e -02, -7.9067e-03, -3.8451e-02]]]])16x16
Kernel

--- Page 625 ---

Image Convolution Using FxF Kernels (Filters) (5)
#Unsqueeze theimagetomakeit4Dtensor
img=img.unsqueeze( 0)
conv1=torch.nn.Conv 2d(1,1,kernel_size =5)
conv2=torch.nn.Conv 2d(1,1,kernel_size =5)
#apply2convolution operations onimage
img=conv1(img)
img=conv2(img)

--- Page 626 ---

Image Convolution Using FxF Kernels (Filters) (6)
conv1=torch.nn.Conv2d( 1,6,kernel_size =5,stride=1)

--- Page 627 ---

Image Convolution Using FxF Kernels (Filters) (6)
conv1=torch.nn.Conv2d( 1,6,kernel_size =5,stride=2)

--- Page 628 ---

Image Convolution Using FxF Kernels (Filters) (2)
nn.Conv2d( 1,6,kernel_size = 5,stride = 1,padding = 0)
6 5x5 kernels
6 28x28 
feature maps32 x 32
Receptive Field
tensor([[[[ 1.6722e -01, -6.5779e-02, -1.2306e-01, 1.8213e -01, 1.4136e -01], [-3.7962e-02, -1.8554e-01, -4.3781e-02, -4.0234e-02, -6.1235e-02], [-8.8460e-02, 6.1839e -02, 1.1887e -01, -1.9571e-01, 1.9496e -01], [ 6.0267e -02, 2.7076e -02, -2.3233e-02, -1.0925e-01, 
8.9112e-02], [-1.0524e-01, 1.0442e -01, -1.8168e-02, -8.9021e-02, 1.6712e -01]]], 
[[[ 8.2493e -06, 6.0229e -02, -9.3913e-02, -1.4296e-02, 1.4104e -03], [ 1.1369e -01, 1.0282e -02, -2.4073e-02, -1.4225e-01, 4.7108e -02], [ 3.1344e -02, -1.6887e-01, -3.7249e-02, -1.8126e-01, 4.2075e -02], [-1.5305e-01, 4.1452e -02, 2.7711e -02, -1.7410e-01, 2.4814e -
02], [-9.3857e-02, 1.8481e -01, -5.0826e-02, 7.2319e -02, -9.4868e-02]]], 
[[[-9.7757e-02, 1.8233e -01, 6.9163e -02, 3.2251e -02, -1.9380e-01], [ 2.9689e -02, 3.3050e -02, 1.3524e -01, -1.0839e-01, -1.5550e-01], [ 6.4101e -02, -1.4855e-01, 1.2839e -01, 1.7996e -01, 1.1009e -01], [ 1.0321e -01, -1.8760e-01, -7.4709e-02, 3.3579e -02, 9.7284e -02], 
[-1.7286e-01, 1.1381e -01, -4.8324e-02, -4.0892e-02, -1.9633e-01]]], 
[[[ 1.4378e -01, 1.7954e -01, -1.7388e-01, -1.6627e-01, -5.5098e-02], [-1.2988e-01, -1.2070e-02, -1.0712e-01, 8.3625e -02, -6.7383e-02], [-1.6427e-01, -4.6448e-02, -1.8334e-01, 9.7070e -02, -1.9433e-01], [-9.5481e-02, 1.7702e -01, 1.7788e -01, 1.5771e -01, -6.9236e-
02], [-1.5401e-01, 1.8203e -01, 1.2515e -01, 1.4476e -01, -1.8584e-01]]], 
[[[ 1.9776e -01, -1.5572e-01, -1.2810e-01, -1.6803e-01, -1.4367e-01], [ 1.0085e -02, 1.6527e -01, -1.0044e-01, -1.1564e-01, -8.7988e-02], [-9.5599e-02, -7.1464e-02, -1.2628e-01, -1.0336e-02, 6.4427e -02], [-3.0428e-02, 7.5229e -02, 1.7940e -01, 1.9090e -01, -1.2995e-
01], [-8.5112e-02, -1.1639e-01, 3.8665e -02, 7.3398e -02, -1.3758e-01]]], 
[[[-1.3190e-01, 2.8339e -02, 1.3287e -01, 1.2268e -01, -1.3469e-01], [ 6.4200e -02, 1.4850e -02, 1.7823e -01, 2.9814e -02, 1.6471e -01],[ 3.8958e -02, 1.1708e -01, 8.0646e -02, -2.6612e-02, -1.4931e-01], [-1.4202e-01, -1.2527e-02, -1.6211e-01, -4.1332e-02, -1.2739e-01], 
[ 9.8423e -02, 8.0368e -02, 2.6417e -02, 1.1033e -01, 7.6816e -02]]]])

--- Page 629 ---

rec_field =np.array([[ 0.9882,0.7529,0.4353,0.0784,0.0000],
[0.9922,0.1608,0.0000,0.0000,0.0000],
[0.5098,0.0000,0.0000,0.0000,0.0000],
[0.1569,0.0000,0.0000,0.0000,0.0000],
[0.0000,0.0000,0.0000,0.0000,0.0000]])
kernel=np.random.randn( 5,5)
print('%.2f'%np.sum(np.multiply(rec_field, kernel)))Image Convolution Using FxF Kernels (Filters) (3)
32 x 32
Receptive Field[[0.9882, 0.7529, 0.4353, 0.0784, 0.0000], 
[0.9922, 0.1608, 0.0000, 0.0000, 0.0000], 
[0.5098, 0.0000, 0.0000, 0.0000, 0.0000], 
[0.1569, 0.0000, 0.0000, 0.0000, 0.0000], 
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]
[[-1.43 1.25 1.36 -0.9 -2.57] 
[0.83 -0.41 0.44 1.44 0.39 ] 
[0.88 -0.27 0.1 1.8 -0.56] 
[-0.97 -0.84 0.49 -1.05 -0.77] 
[-0.85 -0.2 -0.82 -0.27 -0.02]]
= 1.15 x 5
Receptive Field

--- Page 630 ---

self.layer1 =nn.Sequential(
nn.Conv2d( 1,6,kernel_size = 5,stride = 1,padding = 0),
nn.BatchNorm2d( 6),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))Pytorch Implementation of LeNet5 Model (3)
x
6 5x5 kernels
6 28x28 feature mapsBN 
Layerout out ReLU 
Layer
6 28x28 feature mapsout out
6 28x28 feature mapsReLU 
Layerout out

--- Page 631 ---

Pytorch Implementation of LeNet5 Model (2)
model_parameters =filter(lambdap:p.requires_grad ,model.parameters())
params=sum([np.prod(p.size()) forpinmodel_parameters])
print('Number oftrainable parameters =',params)#Model loading in GPU device
model=LeNet5(num_classes).to(device)
#Defining costandoptimizer
cost=nn.CrossEntropyLoss()
optimizer =torch.optim. Adam(model.parameters(), lr = learning_rate)
6 14x14 feature maps

--- Page 632 ---

Pytorch Implementation of LeNet5 Model (2)
# Downloading thedataset andpreprocessing
torchvision.datasets.MNIST(root ='./data‚Äô, train=True, 
transform =transforms.Compose([transforms.Resize(( 32,32)), transforms.ToTensor(), 
transforms.Normalize(mean =(0.1307,),std=(0.3081,))]), download =True)
torchvision.datasets.MNIST(root ='./data‚Äô, train=False, 
transform =transforms.Compose([transforms.Resize(( 32,32)), transforms.ToTensor(), 
transforms.Normalize(mean =(0.1325,),std=(0.3105,))]))

--- Page 633 ---

Pytorch Implementation of LeNet5 Model (2)
#Creating training/testing subset
train_dataset =torchvision.datasets.MNIST(root ='./data‚Äô , train =True,
transform =transforms.Compose([
transforms.Resize(( 32,32)),
transforms.ToTensor(),
transforms.Normalize(mean =(0.1307,),std=(0.3081,))]),
download =False)
test_dataset =torchvision.datasets.MNIST(root ='./data‚Äô , train =False,
transform =transforms.Compose([
transforms.Resize(( 32,32)),
transforms.ToTensor(),
transforms.Normalize(mean =(0.1325,),std=(0.3105,))]))

--- Page 634 ---

Pytorch Implementation of LeNet5 Model (4)
#Model training
total_step =len(train_loader)
forepochinrange(num_epochs):
fori,(images, labels) inenumerate (train_loader):
images=images.to(device)
labels=labels.to(device)
#Forward pass
outputs =model(images)
loss=cost(outputs, labels)
#Backward andoptimize
optimizer.zero_grad()
loss.backward()
optimizer.step()
if(i+1)%400==0:
print('Epoch[{}/{}], Step[{}/{}], Loss:{:.4f}'
.format(epoch+1,num_epochs, i+1,total_step, loss.item()))

--- Page 635 ---

Pytorch Implementation of LeNet5 Model (5)
#Testthemodel
#Intestphase,wedon'tneedtocompute gradients (formemoryefficiency)
withtorch.no_grad():
correct =0
total=0
forimages, labelsintest_loader:
images=images.to(device)
labels=labels.to(device)
outputs =model(images)
_,predicted =torch.max(outputs.data, 1)
total+=labels.size( 0)
correct +=(predicted ==labels). sum().item()
print('Accuracy ofthenetwork onthe10000testimages: {}%'.format(100*correct /total))

--- Page 636 ---

#Trainthemodel
total_step =len(train_loader)
forepochinrange(num_epochs):
fori,(images, labels) inenumerate (train_loader):
images=images.to(device)
labels=labels.to(device)
#Forward pass
outputs =model(images)
loss=cost(outputs, labels)
#Backward andoptimize
optimizer.zero_grad()
loss.backward()
optimizer.step()
if(i+1)%400==0:
print('Epoch[{}/{}], Step[{}/{}], Loss:{:.4f}'
.format(epoch+1,num_epochs, i+1,
total_step, loss.item()))#Testthemodel
#Intestphase,wedon'tneedtocompute gradients (formem
oryefficiency)
withtorch.no_grad():
correct =0
total=0
forimages, labelsintest_loader:
images=images.to(device)
labels=labels.to(device)
outputs =model(images)
_,predicted =torch.max(outputs.data, 1)
total+=labels.size( 0)
correct +=(predicted ==labels). sum().item()
print('Accuracy ofthenetwork onthe10000testimages:
{}%'.format(100*correct /total))

--- Page 637 ---

StandardTransform Transform: Compose( 
Resize(size=(32, 32) , 
interpolation=bilinear ) ToTensor() 
Normalize(mean=(0.1325,), std=(0.3105,)) )

--- Page 638 ---

classLeNet5(nn.Module):
def__init__ (self,num_classes ):
super(LeNet5, self).__init__ ()
self.layer1 =nn.Sequential(
nn.Conv2d( 1,6,kernel_size = 5,stride = 1,padding = 0)
6 5x5 
Kernels (filters)

--- Page 639 ---

6 5x5 
Kernels (filters)
https://adamharley.com/nn_vis/cnn/2d.html

--- Page 640 ---

classLeNet5(nn.Module):
def__init__ (self,num_classes ):
super(LeNet5, self).__init__ ()
self.layer1 =nn.Sequential(
nn.Conv2d( 1,6,kernel_size = 5,stride = 1,padding = 0),
nn.BatchNorm2d( 6),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))
self.layer2 =nn.Sequential(
nn.Conv2d( 6,16,kernel_size= 5,stride=1,padding= 0),
nn.BatchNorm2d( 16),
nn.ReLU(),
nn.MaxPool2d(kernel_size =2,stride=2))
self.fc=nn.Linear( 400,120)
self.relu=nn.ReLU()
self.fc1=nn.Linear( 120,84)
self.relu1=nn.ReLU()
self.fc2=nn.Linear( 84,num_classes)

--- Page 642 ---

Perceptron Model
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 643 ---

Perceptron Model
Definition: A Perceptron is a supervised learning algorithm for 
binary classification . 
Based on input features, { x(i)‚àà‚Ñún}, i = 1, 2,‚Ä¶, m, a binary classifier selects a 
specific class from 2 possible ones.
In modern language, a perceptron is a single -layer neural network that
consists of:
‚Ä¢Input data (i.e., features) (n -dimensional vectors ‚àà‚Ñún)
‚Ä¢Synaptic weights and a bias (could be ‚Äúabsorbed‚Äù in the synaptic weights)
‚Ä¢A network sum
‚Ä¢Anactivation function ( signum )x(i)
1
x(i)
2
x(i)
3
x(i)
n‚àëb
net(i) y(i)(x(i))=sign( wT.x(i))^w1
w2
w3
wn
b

--- Page 644 ---

Perceptron Model (2)
Inputs: {x(i), y(i)= -1 or 1}, i = 1, 2,‚Ä¶, m
Prediction: {y(i)= -1 or 1}, i = 1, 2,‚Ä¶, m^
Case 1: y(i) = +1, wT.x(i)‚â• 0 ü°∫y(i) = +1 ü°∫Correct classification^
Case 2: y(i) = -1, wT.x(i)< 0 ü°∫y(i) = -1 ü°∫Correct classification^
Case 3: y(i) = +1, wT.x(i)< 0 ü°∫y(i) = -1ü°∫Wrong classification (misclassification)^
Case 4: y(i) = -1, wT.x(i)‚â• 0 ü°∫y(i) = +1ü°∫Wrong classification (misclassification)^Could be >
Could be  ‚â§
Correct classification: y(i) .(wT.x(i)) ‚â• 0 Misclassification: y(i) .(wT.x(i)) < 0

--- Page 645 ---

Perceptron Model: Example
Inputs: {x(i)‚àà‚Ñú2y(i)= -1 or 1}, i = 1, 2,‚Ä¶, m
x1x2
wT.x(i) = 
0wInputs: {x(i)‚àà‚Ñú2y(i)= -1 or 1}, i = 1, 2,‚Ä¶, m
x2
x1x3w
Linearly separable data!
Linearly separable data!wT.x(i) < 
0wT.x(i) > 
0

--- Page 646 ---

Perceptron Model: Example (2)
Example 1: {x(i)‚àà‚Ñú2y(i)= -1 or 1}, i = 1, 2,‚Ä¶, m
x1 x2 y
0 0 -1
1 0 1
0 1 1
1 1 1OR Data
x1x2x1 x2 y
0 0 -1
1 0 -1
0 1 1-
1 1 1AND Data
x1x2x1 x2 y
0 0 -1
1 0 1
0 1 1
1 1 -1XOR Data
x1x2
Linearly separable data!Linearly non -separable data!wT.x(i) = 
0wT.x(i) = 
0
Linearly separable data!Non-Linearly separable data!

--- Page 647 ---

Perceptron Model: Example (2)
Example 2: {x(i)‚àà‚Ñú y(i)= -1 or 1}, i = 1, 2,‚Ä¶, m
Linearly non -separable 1D data!
No line that passes by the original can correctly classify the data!x
0
x1x2
Linearly separable 2D data!wT.x(i) = 
0
x1x2
Linearly non -separable 2D data!
No line that passes by the original can correctly classify the data!x2
x1x3
wT.x(i) = 
0
Linearly separable 3D data!

--- Page 648 ---

Perceptron Model: Example (2)
Example 2: {x(i)‚àà‚Ñú3y(i)= -1 or 1}, i = 1, 2,‚Ä¶, m
x1 x2 x1x2 y
0 0 0 -1
1 0 0 1
0 1 0 1
1 1 1 -1XOR Data
Linearly separable data!x1x2
x1x2New dimension: Features‚Äô interaction

--- Page 649 ---

Perceptron Algorithm
Initialization: w= 0‚àà‚Ñún+1
while( true)
cnt = 0 
for x(i), y(i), i = 1, 2, ‚Ä¶, m
if y(i) (wT.x(i)) < 0
wü°∫w + y(i).x(i)
cnt++
if cnt = 0
terminate
Terminate loopw ü°®w ‚Äìx(i) if y(i)= -1 and y(i)= 1^
w ü°®w + x(i) if y(i)= 1 and y(i)= -1^# Counter of misclassifications
# Loop over all data points
# Check misclassification
# Update the synaptic weights
# Update the number of misclassifications
# Stop if all samples have been correctly classified(w(i))T.x(i) = 0w(i)
x(i)w(i+1)=w(i)-x(i)
Misclassifications
(w(i+1))T.x(i) = 0-x(i)
w(i+1)=w(i)-x(i)

--- Page 650 ---

Perceptron Algorithm: Gradient Descent Update
Given { x(i), y(i)}, i = 1, 2, ‚Ä¶, m, the error term is given by:
For a fixed data sample, we can write the error as:
The gradient update for each synaptic weight, wi, i = 1, 2, ‚Ä¶, n+1 is given by:
In the above relation, we used the following fact:

--- Page 651 ---

Perceptron Algorithm: Gradient Descent Update (2)
Initialization: w0= 0‚àà‚Ñún+1, 0 < Œ∑ < 1
while(|| wk+1‚Äìwk||2
2> Œµ & k < K)
for wj, j = 1, 2, ‚Ä¶, n+1Inputs: { x(i)‚àà‚Ñún, y(i)= 0 or 1
y(i)= wkT.x(i) ^
wjü°∫wj-Œ∑ (y(i) -
y(i)).xj(i)^
sign( wT.x(i))

--- Page 652 ---

Perceptron Algorithm: History
AI ‚ÄúWinter‚Äù (1969)
Frank Rosenblatt 
(1928 -1971)

--- Page 653 ---

Value Iteration Algorithm
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 654 ---

Markov Decision Process (MDP)
‚Ä¢Definition: A Markov decision process (MDP ) is a discrete -time stochastic control 
process . 
‚Ä¢The MDP formalism provides a mathematical framework for modeling a decision
making in situations where outcomes are partly random and partly under the control of 
a decision maker . 
‚Ä¢MDPs are useful for studying optimization problems solved via dynamic programming . 
‚Ä¢MDPs were known at least as early as the 1950s as evidenced by Ronald Howard's 
book: ‚Ä≥Dynamic Programming and Markov Processes ‚Ä≥.
‚Ä¢MDPs are used in many disciplines, including robotics ,automatic
control ,economics andmanufacturing and deep reinforcement learning and more 
recently large language models (LLMs)!

--- Page 655 ---

Markovian Property
‚Ä¢Definition: In a nutshell, the Markov property states the future is independent of the 
past given the present!
‚Ä¢Applications: Natural language/text processing
The quick brown fox jumps over the lazy 
dogtomorrowtoday
yesterdaybeginning
Conditional probability

--- Page 656 ---

Markovian Property (2)
‚Ä¢Applications: Speech processing
tomorrowtoday
yesterdaybeginning

--- Page 657 ---

Markovian Property (3)
‚Ä¢Applications: Optical character recognition
tomorrow today
yesterday
beginning

--- Page 658 ---

Markovian Property (3)
‚Ä¢Applications: Digital image processing / computer vision
tomorrow
todayyesterday beginning
px100px99px98px0

--- Page 659 ---

Markovian Property (4)
Markovian Model

--- Page 660 ---

MDP (2)
‚Ä¢MDPs are an extension of Markov chains .
‚Ä¢MDPs are named after the Russian mathematician Andrey Markov .
‚Ä¢At each time step, the process is in some state s, and the decision maker may choose 
any action athat is available in state s. 
‚Ä¢The process responds at the next time step by randomly moving into a new state s‚Ä≤, and 
giving the decision maker a corresponding reward r(s, a, s') .
‚Ä¢The probability that the process moves into its new state s'is influenced by the chosen 
action. Specifically, it is given by the state transition function T(s, a, s') . Thus, the next 
state s' depends on the current state sand the decision maker's action a. 
Andrey Andreyevich Markov
(1856 -1922)
2-states (A and E) Markov chain

--- Page 661 ---

MDP (3)
‚Ä¢But given sand a, it is conditionally independent of all previous states and actions: In 
other words, the state transitions of an MDP satisfy the Markov property .
‚Ä¢Although MDPs are an extension of Markov chains , they differ mainly in the 
presence actions to allow choices for the agent and rewards to boost the agent‚Äôs 
motivation ! 
‚Ä¢Therefore, an MDP with a single state ( s = ‚Ä≤wait‚Ä≤ ) and a single reward ( r = 0 ) reduces 
to a Markov chain !
‚Ä¢For a Markov state s and successor state s‚Ä≤, the state transition probability is defined 
by:

--- Page 662 ---

MDP vs Markov Chain
2-states (A and E) Markov chain
‚Ä¢When the model is in state A , there is a 
40% chance for the model to move to state 
Eand a 60% chance for remaining in state 
A. 
‚Ä¢When in state E , there is a 70% chance of 
moving to state A and a 30% chance of 
staying in state E . 
‚Ä¢The Markov chain is denoted by a sequence 
of states: s0, s1, s2, s3, ‚Ä¶, snafter n 
transitions where s0is the initial state 
(either state A or state E ).‚Ä¢When the model is in state s0and takes action 
a0there is a 50% chance for the model to move 
to state s2and a 50% chance for remaining in 
state s0with collecting any reward. 
‚Ä¢When in state s1and taking action a0, there is a 
70% chance of moving to state s0and 
collecting a reward of +5. There is a 10%
chance of staying in state s1 and 20% chance 
for the model to move to state s2without any 
reward.
‚Ä¢T(s2, a1, s0, -1) = 0.3, T(s2, a1, s2, 0) = 0.4
s0s1
s2
3-states, 2 -actions, 2 -rewards MDP

--- Page 663 ---

Student Markov Decision Process Model
Student Markov Model SleepSocial 
media
Course
1
SportCourse
3Course
2Success
Fail
0.40.30.30.30.70.9
0.1
0.80.2
0.70.2
0.8
0.40.6 0.10.21
Sample episodes for Student Markov Model:
[Starting state: Course 1 (C1)

--- Page 664 ---

Student Markov Decision Process Model (2)
Sample episodes (trajectories ) for Student 
Markov Model:
[Starting state : Course 1 (C1)]
{C1, C2, C3, Success, Success, Sleep }
{C1, S.M., S.M., C1, C2, C3, Fail, C3, 
Success, Sleep }
{C1, C2, C3, Sport, C3, Success, 
Success, Success, Sleep }
{C1, S.M., C1, C2,C3, Sport, C3, Fail, 
Fail, C3, Sport, C3, Success, Success, 
Success, Success, Success, Sleep }
{‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶., Sleep }

--- Page 665 ---

Student Markov Reward Model (MRP)
R = -1R = 0
R = +100
R = +2 R = +5R = +10
R = +4R = -100Markov Reward Process 
(MRP)

--- Page 666 ---

Student MRP (2)
Question: What is the total discounted reward starting from time step t?
Time step t

--- Page 667 ---

MDP(4)
Environment
RL 
Agent
Left Right
DownUp
Agent Actions
0.1 0.10.8
Action Noise

--- Page 668 ---

MDP (5)
1         2         3        4         5        6         7         8    
8
7
6
5
4
3
2
1
Environment‚Ä¢MDP States: 
State s1= <1, 1>
State s2= <1, 2>
State s8= <1, 8>
State s9= <2, 1>
State s10= <2, 2>
State s16= <2, 8>
State s64= <8, 8>‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶Absorbing (terminal) states!

--- Page 669 ---

MDP (6)
1         2         3        4         5        6         7         8    
8
7
6
5
4
3
2
1
Environment‚Ä¢State Actions: The set of possible actions an 
RL agent can take ate any state
Left Right
DownUp
Agent Actions

--- Page 670 ---

MDP (5)
1         2         3        4         5        6         7         8    
8
7
6
5
4
3
2
1
Environment‚Ä¢MDP States: 
State s1= <1, 1>
State s2= <1, 2>
State s8= <1, 8>
State s9= <2, 1>
State s10= <2, 2>
State s16= <2, 8>
State s64= <8, 8>‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶Absorbing (terminal) states!

--- Page 671 ---

MDP (7)
Environment
‚Ä¢Initial State: s0
‚Ä¢The agent can start at any valid state!
‚Ä¢For this we need to assume an initial 
state distribution :
s1         s2          s3         s4          s5          s6‚Ä¢Example: Initial state distribution for 6 -
states RL problem:

--- Page 672 ---

MDP (8)
‚Ä¢Transition probabilities: T(s, a, s‚Ä≤, r)
‚Ä¢The agent may or may not collect rewards by 
taking action awhile in state sand 
transitioning to state s‚Ä≤
Environment
1
-3
63

--- Page 673 ---

MDP (10)
Main Goal: Find an optimal policy œÄ* for the agent:
Environment
Agent policyExpected accumulated 
rewardsHorizon: Time span
Discount factor
Reward collected by taking action at at
state stand moving to state st+1
Agent

--- Page 674 ---

MDP (11)
Definition: A policy œÄ gives an action for each state for each time 
step.
t = 0
t = 1t = 2t = 3t = H
Important Note: If the environment is deterministic, the optimal policy would consist of 
a sequence of actions from the start to goal states.

--- Page 675 ---

Value Iteration Algorithm
Important Note: For illustration purposes, we will consider only small , discrete
state -action spaces andenvironments as they are simpler to explain the main concepts of 
reinforcement learning!
Expected value of sum of discounted 
rewards when starting from state s and 
acting optimally afterwards
Settings: Deterministic actions, Œ≥ = 1 , H = 100V*(<4, 3>) = 
 +1
V*(<3, 3>) = +1
V*(<2, 3>) = +1
V*(<1, 3>) = +1
V*(<4, 2>) = -1
V*(<1, 1>) = +1

--- Page 676 ---

Value Iteration Algorithm (2)
Settings: Deterministic actions, Œ≥ = 0.9 , H = 100V*(<4, 3>) = 
 +1
V*(<3, 3>) = 0.9x(+1) = 0.9
V*(<2, 3>) = 0.9x0.9x(+1) = 0.81
V*(<1, 3>) = (0.9)3x(+1) = 0.729
V*(<4, 2>) = -1
V*(<1, 1>) = (0.9)5x(+1) = 0.5904

--- Page 677 ---

Value Iteration Algorithm (3)
Settings: Stochastic actions, Œ≥ = 0.9 , H = 100V*(<4, 3>) = 
 +1
V*(<3, 3>) = 0.72
0.1 0.10.8
Action NoiseV*(<3, 3>) = 0.8x0.9xV*(<4, 3>) + 0.1x0.9xV*(<3, 3>) + 0.1x0.9xV*(<3, 2>) = 0.72
The value V*(<3, 3>) is updated ‚Ä≥recursively‚Ä≥!

--- Page 678 ---

Value Iteration Algorithm (4)
Settings: Stochastic actions, Œ≥ = 0.9 , H = 100V*(<4, 3>) = +1
V*(<3, 3>) = 0.72
V*(<2, 3>) = 0.8x0.9xV*(<3, 3>) + 0.1x0.9xV*(<2, 3>) + 0.1x0.9xV*(<2, 3>)
The value V*(<2, 3>) is updated ‚Ä≥recursively‚Ä≥! V*(<2, 3>) = 0.8x0.9x0.72 + 0.1x0.9x0 + 0.1x0.9x0 0.5184

--- Page 679 ---

Settings: Stochastic actions, Œ≥ = 0.9 , H = 100V*(<4, 3>) = +1
V*(<3, 3>) = 0.72
V*(<2, 3>) = 0.5184Value Iteration Algorithm (5)
V*(<1, 3>) = 0.8x0.9xV*(<2, 3>) + 0.1x0.9xV*(<1, 3>) + 0.1x0.9xV*(<1, 2>)
The value V*(<1, 3>) is updated ‚Ä≥recursively‚Ä≥! V*(<1, 3>) = 0.8x0.9x0.5184 + 0.1x0.9x0 + 0.1x0.9x0 0.3732

--- Page 680 ---

Settings: Stochastic actions, Œ≥ = 0.9 , H = 100V*(<4, 3>) = +1
V*(<3, 3>) = 0.72
V*(<2, 3>) = 0.5184
V*(<1, 3>) = 0.3732Value Iteration Algorithm (6)
V*(<1, 2>) = 0.8x0.9xV*(<1, 3>) + 0.1x0.9xV*(<1, 2>) + 0.1x0.9xV*(<1, 1>)
The value V*(<1, 2>) is updated ‚Ä≥recursively‚Ä≥! V*(<1, 2>) = 0.8x0.9x0.3732 + 0.1x0.9x0 + 0.1x0.9x0 0.2687

--- Page 681 ---

Settings: Stochastic actions, Œ≥ = 0.9 , H = 100V*(<4, 3>) = +1
V*(<3, 3>) = 0.72
V*(<2, 3>) = 0.5184
V*(<1, 3>) = 0.3732
V*(<1, 2>) = 0.2687Value Iteration Algorithm (7)
V*(<1, 1>) = 0.8x0.9xV*(<1, 2>) + 0.1x0.9xV*(<2, 1>) + 0.1x0.9xV*(<1, 1>)
The value V*(<1, 1>) is updated ‚Ä≥recursively‚Ä≥! V*(<1, 1>) = 0.8x0.9x0.2687 + 0.1x0.9x0 + 0.1x0.9x0 0.1934

--- Page 682 ---

Value Iteration Algorithm (8)
Initialization: Values of all states are set to 0!
0    0      0
0            0
0     0     0‚Ä¢V*
0(s)= Optimal value for state s when H = 0 
(no time step left!):
‚Ä¢V*
1(s)= Optimal value for state s when H = 1 (1 
time step remaining):
‚Ä¢V*
2(s)= Optimal value for state s when H = 2 (2 time steps remaining):
‚Ä¢V*
k(s)= Optimal value for state s when H = k (k time steps remaining):

--- Page 683 ---

Value Iteration Algorithm (9)
s = <1, 2>Value for state s with 
k time steps left
Maximum accumulated reward 
across all actions available to state sTransition probability from 
state s to state s‚Ä≤ taking action a
Value for state s‚Ä≤ with 
k-1 steps leftDiscount factor:
0 < Œ≥ < 1
Reward received taking action a at 
state s and transitioning to state s‚Ä≤

--- Page 684 ---

Value Iteration Algorithm (9)
Algorithm Summary:
Step 1: For all states s, setV0*(s) = 0
Step 2: Looping over time steps till horizon limit H (k = 1, 2, ‚Ä¶, H):
For all states s in S:
Extract best policy (action leading maximum V!)
Value update / Bellman update / Bellman back=up

--- Page 685 ---

Value Iteration Algorithm (10)
Step 1: For all states s, setV0*(s) = 0
Settings: Stochastic actions ( 0.8, 0.1, 0.1), Œ≥ = 0.9
Iteration k  = 0
Iteration k  = 1
Iteration k  = 2
Iteration k  = 3
Iteration k  = 100

--- Page 686 ---

Settings: Stochastic actions ( 0.8, 0.1, 0.1), Œ≥ = 0.9
Iteration k  = 2
Value Iteration Algorithm (11)

--- Page 687 ---

Settings: Stochastic actions ( 0.8, 0.1, 0.1), Œ≥ = 0.9
Iteration k  = 2
Value Iteration Algorithm (12)

--- Page 688 ---

Settings: Stochastic actions ( 0.8, 0.1, 0.1), Œ≥ = 0.9
Iteration k  = 2
Value Iteration Algorithm (13)

--- Page 689 ---

Settings: Stochastic actions ( 0.8, 0.1, 0.1), Œ≥ = 0.9
Iteration k  = 2
Value Iteration Algorithm (14)

--- Page 690 ---

Settings: Stochastic actions ( 0.8, 0.1, 0.1), Œ≥ = 0.9
Iteration k  = 2Value Iteration Algorithm (15)
Iteration k  = 3

--- Page 691 ---

Iteration k  = 3
Value Iteration Algorithm (16)
Iteration k  = 0
 Iteration k  = 1
 Iteration k  = 2
Iteration k  = 4 Iteration k  = 5

--- Page 692 ---

Value Iteration Algorithm (17)
Iteration k  = 6
 Iteration k  = 7
 Iteration k  = 8
Iteration k  = 9
 Iteration k  = 10
 Iteration k  = 11

--- Page 693 ---

Value Iteration Algorithm (18)
Iteration k  = 12
Iteration k  = 100

--- Page 694 ---

Value Iteration Algorithm (19)
Important Question: Will the values at each state keep changing indefinitely ?
Iteration k  = 0
<2, 3>Illustration: Let us consider the evolution of V*
k(<2, 3>) 
through the iterative updates of the value iteration algorithm
V*
k(<2, 3>) 
0 1  2   3  4   5 6  7  8   9  10 11‚Ä¶‚Ä¶..‚Ä¶‚Ä¶‚Ä¶‚Ä¶.‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶100 101 102 103‚Ä¶.k
Hint: It seems that the values at each state will definitely ‚Ä≥ converge ‚Ä≥ to specific 
‚Ä≥stationary ‚Ä≥ values!

--- Page 695 ---

Bellman Optimality Equation
Note: The optimal value function V‚àómust satisfy the following Bellman optimality 
equation :
As a classical dynamic programming method, the value iteration algorithm finds V‚àóby 
solving the above Bellman optimality equation using the simple backup operation :
Definition: A Bellman optimality operator Œ§ :‚Ñú|S|‚Üí ‚Ñú|S|is an operator that satisfies: for 
any V ‚àà‚Ñú|S|:
Note: The value iteration update can thus be represented as recursively applying the 
Bellman optimality operator:
Important Note: V*(s) is a fixed point of Œ§:

--- Page 696 ---

Bellman Optimality Equation (2)
Theorem: Œ§is a contraction mapping under sup -norm || ¬∑ ||‚àû, i.e., there exists Œ≥ ‚àà[0, 1) 
such that:
Note: Given two functions f(z)and g(z)such that:
Then, we can write:
‚â• 0 f(z*)
‚â• g(z*)

--- Page 697 ---

Bellman Optimality Equation (3)
For any state s, we have:
Optimal action = a*
Expectation over all future states s‚Ä≤
Since the above holds for any state s, it also holds for:
Contraction property!

--- Page 698 ---

Bellman Optimality Equation (4)
Theorem: The value iteration converges to V*:
Note: As stated earlier, Œ§ is a contraction mapping and V*is a fixed point of Œ§. Therefore, 
we can write:  
Let k ‚Üí ‚àû, and we have || Vk‚ÄìV*||‚àû‚Üí 0 since Œ≥ < 1. Therefore, we conclude that:

--- Page 699 ---

https://yuanz.web.illinois.edu/teaching/IE498fa19/lec_16.pdf

--- Page 700 ---

RL 
Agent
Left Right State s Action a State s‚Ä≤ T(s, a, s‚Ä≤) r(s, a, s‚Ä≤)
SL‚Üê SL 0.5 2.0
SL‚Üê SR 0.5 -1.0
SL‚Üí SL 0.5 1.0
SL‚Üí SR 0.5 2.0
SR‚Üê SL 0.0 -2.0
SR‚Üê SR 1.0 1.0
SR‚Üí SL 0.1 -3.0
SR‚Üí SR 0.9 -1.0
EnvironmentVL= +1 VR= -1
r‚Üê= 0.5 * (2.0 + Œ≥ * (+1))Discount factor Œ≥ = 0
+ 0.5 * ( -1.0 + Œ≥ * ( -1.0))
r‚Üê= 1.5 -1.0= 0.5r‚Üê:0.5
r‚Üí= 0.5 * (1.0 + Œ≥ * (+1)) + 0.5 * (2.0 + Œ≥ * ( -1.0))
r‚Üê= 1.0 + 0.5 = 1.5r‚Üí:1.51.5Value Iteration Algorithm

--- Page 701 ---

RL 
Agent
Left Right
Environment
Discount factor Œ≥ = 01.5State s Action a State s‚Ä≤ T(s, a, s‚Ä≤) r(s, a, s‚Ä≤)
SL‚Üê SL 0.5 2.0
SL‚Üê SR 0.5 -1.0
SL‚Üí SL 0.5 1.0
SL‚Üí SR 0.5 2.0
SR‚Üê SL 0.0 -2.0
SR‚Üê SR 1.0 1.0
SR‚Üí SL 0.1 -3.0
SR‚Üí SR 0.9 -1.0
r‚Üê= 0.0 * ( -2.0 + Œ≥ * (+1)) + 1.0 * (1.0 + Œ≥ * ( -1.0))
r‚Üê= 0.0 + 0.0 = 0.0r‚Üê:0.0
r‚Üí= 0.1 * ( -3.0 + Œ≥ * (+1)) + 0.9 * ( -1.0 + Œ≥ * ( -1.0))
r‚Üê= -0.2-1.8 = -2.0r‚Üí:-2.00.0Value Iteration Algorithm (2)

--- Page 702 ---

State s Action a State s‚Ä≤ T(s, a, s‚Ä≤) r(s, a, s‚Ä≤)
SL‚Üê SL 0.5 2.0
SL‚Üê SR 0.5 -1.0
SL‚Üí SL 0.5 1.0
SL‚Üí SR 0.5 2.0
SR‚Üê SL 0.0 -2.0
SR‚Üê SR 1.0 1.0
SR‚Üí SL 0.1 -3.0
SR‚Üí SR 0.9 -1.0
r‚Üê= 0.5 * (2.0 + Œ≥ * (1.5)) + 0.5 * ( -1.0 + Œ≥ * (0.0))
r‚Üê= 1.75 -0.5= 1.25r‚Üê:1.25
r‚Üí= 0.5 * (1.0 + Œ≥ * (1.5)) + 0.5 * (2.0 + Œ≥ * (0.0))
r‚Üê= 1.25 + 1.0 = 2.25r‚Üí:2.25
RL 
Agent
Left Right
Environment
Discount factor Œ≥ = 01.5 0.02.25Value Iteration Algorithm (3)

--- Page 703 ---

State s Action a State s‚Ä≤ T(s, a, s‚Ä≤) r(s, a, s‚Ä≤)
SL‚Üê SL 0.5 2.0
SL‚Üê SR 0.5 -1.0
SL‚Üí SL 0.5 1.0
SL‚Üí SR 0.5 2.0
SR‚Üê SL 0.0 -2.0
SR‚Üê SR 1.0 1.0
SR‚Üí SL 0.1 -3.0
SR‚Üí SR 0.9 -1.0
r‚Üê= 0.0 * ( -2.0 + Œ≥ * (1.5)) + 1.0 * (1.0 + Œ≥ * (0.0))
r‚Üê= 0.0 + 1.0 = 1.0r‚Üê:1.0
r‚Üí= 0.1 * ( -3.0 + Œ≥ * (1.5)) + 0.9 * ( -1.0 + Œ≥ * (0.0))
r‚Üê= -0.15 -0.9= -1.05r‚Üí:-1.05
RL 
Agent
Left Right
Environment
Discount factor Œ≥ = 01.5 0.02.25Value Iteration Algorithm (4)
1.0

--- Page 704 ---

State s Action a State s‚Ä≤ T(s, a, s‚Ä≤) r(s, a, s‚Ä≤)
SL‚Üê SL 0.5 2.0
SL‚Üê SR 0.5 -1.0
SL‚Üí SL 0.5 1.0
SL‚Üí SR 0.5 2.0
SR‚Üê SL 0.0 -2.0
SR‚Üê SR 1.0 1.0
SR‚Üí SL 0.1 -3.0
SR‚Üí SR 0.9 -1.0
r‚Üê= 0.5 * (2.0 + Œ≥ * (2.25)) + 0.5 * ( -1.0 + Œ≥ * (1.0))
r‚Üê= 2.125 + 0.0 = 2.125r‚Üê: 2.125
r‚Üí= 0.5 * (1.0 + Œ≥ * (2.25) + 0.5 * (2.0 + Œ≥ * (1.0))
r‚Üê= 1.725 + 1.5 = 3.225r‚Üí: 3.225Value Iteration Algorithm (5)
RL 
Agent
Left Right
Environment
Discount factor Œ≥ = 01.5 0.02.25 1.03.225

--- Page 705 ---

State s Action a State s‚Ä≤ T(s, a, s‚Ä≤) r(s, a, s‚Ä≤)
SL‚Üê SL 0.5 2.0
SL‚Üê SR 0.5 -1.0
SL‚Üí SL 0.5 1.0
SL‚Üí SR 0.5 2.0
SR‚Üê SL 0.0 -2.0
SR‚Üê SR 1.0 1.0
SR‚Üí SL 0.1 -3.0
SR‚Üí SR 0.9 -1.0
r‚Üê= 0.0 * ( -2.0 + Œ≥ * (2.25)) + 1.0 * (1.0 + Œ≥ * (1.0))
r‚Üê= 0.0 + 2.0 = 2.0r‚Üê: 2.0
r‚Üí= 0.1 * ( -3.0 + Œ≥ * (2.25) + 0.9 * ( -1.0 + Œ≥ * (1.0))
r‚Üê= -0.075 + 0.0 = -0.075r‚Üí: -0.075Value Iteration Algorithm (6)
RL 
Agent
Left Right
Environment
Discount factor Œ≥ = 01.5 0.02.25 1.03.225 2.0

--- Page 706 ---

State s Action a State s‚Ä≤ T(s, a, s‚Ä≤) r(s, a, s‚Ä≤)
SL‚Üê SL 0.5 2.0
SL‚Üê SR 0.5 -1.0
SL‚Üí SL 0.5 1.0
SL‚Üí SR 0.5 2.0
SR‚Üê SL 0.0 -2.0
SR‚Üê SR 1.0 1.0
SR‚Üí SL 0.1 -3.0
SR‚Üí SR 0.9 -1.0
r‚Üê= 0.5 * (2.0 + Œ≥ * (3.225)) + 0.5 * ( -1.0 + Œ≥ * (2.0))
r‚Üê= 2.612 + 0.5 = 
3.112r‚Üê: 3.112
r‚Üí= 0.5 * (1.0 + Œ≥ * (3.225) + 0.5 * (2.0 + Œ≥ * (2.0))
r‚Üê= 
2.112+ 2.0 = 
4.112r‚Üí: 
4.112Value Iteration Algorithm (7)
RL 
Agent
Left Right
Environment
Discount factor Œ≥ = 01.5 0.02.25 1.03.225 2.04.11
2

--- Page 707 ---

State s Action a State s‚Ä≤ T(s, a, s‚Ä≤) r(s, a, s‚Ä≤)
SL‚Üê SL 0.5 2.0
SL‚Üê SR 0.5 -1.0
SL‚Üí SL 0.5 1.0
SL‚Üí SR 0.5 2.0
SR‚Üê SL 0.0 -2.0
SR‚Üê SR 1.0 1.0
SR‚Üí SL 0.1 -3.0
SR‚Üí SR 0.9 -1.0
r‚Üê= 0.0 * ( -2.0 + Œ≥ * (3.225)) + 0.5 * (1.0 + Œ≥ * (2.0))
r‚Üê= 0.0 + 1.5 = 1.5r‚Üê: 1.5
r‚Üí= 0.1 * ( -3.0 + Œ≥ * (3.225) + 0.9 * ( -1.0 + Œ≥ * (2.0))
r‚Üê= 0.0225 + 0.9 = 0.9225r‚Üí: 0.9225Value Iteration Algorithm (8)
RL 
Agent
Left Right
Environment
Discount factor Œ≥ = 01.5 0.02.25 1.03.225 2.04.11
21.5

--- Page 709 ---

0.3 0.3 0.4 0 0 0 0 0 0
0.1 0.2 0.3 0.2 0.1 0 0 0 0
0 0.1 0.2 0.3 0.2 0.1 0 0 0
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶s1           s2          s3      s4           s5          s6          s7             s8          s9 
s1
s2
s3
s4
s5
s6
s7s8
s9

--- Page 710 ---

0 0.5 0 0 0 0.5 0
0 0 0.8 0 0 0 0
0 0 0 0.6 0.4 0 0
0 0 0 0 0 0 1
0.2 0.4 0.4 0 0 0 0
0.1 0 0 0 0 0.1 0
0 0 0 0 0 0 1C1           C2             C3      Suc. Sport S.M. Sleep
C1
C2
C3
Suc.
Sport
S.M.
Sleep

--- Page 711 ---

0 0.3 0 0 0 0 0.7 0
0 0 0.8 0 0 0 0 0
0 0 0 0.7 0.2 0.1 0 0
0 0 0 0.8 0 0 0 0.2
0 0 0.6 0 0.4 0 0 0
0.2 0.4 0.4 0 0 0 0 0
0.1 0 0 0 0 0 0.9 0
0 0 0 0 0 0 0 1C1         C2        C3          Suc. Fail  Sport S.M. Sleep
C1
C2
C3
Suc.
Sport
S.M.
SleepFail

--- Page 712 ---

Introduction to
Artificial Intelligence
with Python (Python 4 AI)
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 713 ---

Uncertainty

--- Page 717 ---

Probability

--- Page 718 ---

P(œâ)Possible Worlds

--- Page 719 ---

P(œâ)

--- Page 720 ---

0 ‚â§ P(œâ) ‚â§ 1

--- Page 721 ---

0 ‚â§P(œâ) = 1

--- Page 723 ---

P(    ) = 1/6

--- Page 726 ---

2
3
4
5
6
73
4
5
6
7
84
5
6
7
8
95
6
7
8
9
106
7
8
9
101
17
8
9
101
112

--- Page 727 ---

2
3
4
5
6
73
4
5
6
7
84
5
6
7
8
95
6
7
8
9
106
7
8
9
101
17
8
9
101
112

--- Page 728 ---

2
3
4
5
6
73
4
5
6
7
84
5
6
7
8
95
6
7
8
9
106
7
8
9
101
17
8
9
101
112

--- Page 729 ---

P(sum to 7) =P(sum to 12) =

--- Page 730 ---

unconditional probability
degree of belief in a proposition
in the absence of any other evidence

--- Page 731 ---

conditional probability
degree of belief in a proposition
given some evidence that has already
been revealed

--- Page 732 ---

conditional probability
P(a| b)

--- Page 733 ---

P(rain today | rain yesterday )

--- Page 734 ---

P(route change | traffic conditions )

--- Page 735 ---

P(disease | test results )

--- Page 737 ---

P(sum 12 |    )

--- Page 739 ---

P(    )

--- Page 740 ---

P(    )
P(sum 12 )
P(sum 12 |    )

--- Page 742 ---

random variable
a variable in probability theory with a domain of 
possible values it can take on

--- Page 743 ---

random variable
Roll
{1, 2, 3, 4, 5, 6}

--- Page 744 ---

random variable
Weather
{sun, cloud, rain, wind, snow }

--- Page 745 ---

random variable
Traffic
{none, light, heavy }

--- Page 746 ---

random variable
Flight
{on time, delayed, cancelled }

--- Page 747 ---

probability distribution
P(Flight = on time ) = 0.6
P(Flight = delayed ) = 0.3
P(Flight = cancelled ) = 0.1

--- Page 748 ---

probability distribution
P(Flight ) = ‚ü®0.6, 0.3, 0.1 ‚ü©

--- Page 749 ---

independence
the knowledge that one event occurs does not 
affect the probability of the other event

--- Page 750 ---

independence

--- Page 751 ---

independence

--- Page 752 ---

independence
P(        ) = P(    )P(    )

--- Page 753 ---

independence
P(        ) ‚â† P(    )P(    )

--- Page 754 ---

independence
P(        ) = P(    )P(     |    )

--- Page 755 ---

Bayes' Rule

--- Page 758 ---

Bayes' Rule

--- Page 759 ---

Bayes' Rule

--- Page 760 ---

PM AM
Given clouds in the morning,
what's the probability of rain in the 
afternoon?
‚Ä¢80% of rainy afternoons start with 
cloudy
mornings.
‚Ä¢40% of days have cloudy mornings.
‚Ä¢10% of days have rainy afternoons.

--- Page 762 ---

P(cloudy morning | rainy 
afternoon )Knowin
g
we can 
calculate
P(rainy afternoon | cloudy morning )

--- Page 763 ---

P(visible effect | unknown 
cause )Knowin
g
we can 
calculate
P(unknown cause | visible effect )

--- Page 764 ---

P(medical test result | 
disease )Knowin
g
we can 
calculate
P(disease | medical test result )

--- Page 765 ---

P(blurry text | counterfeit bill )Knowin
g
we can 
calculate
P(counterfeit bill | blurry text )

--- Page 766 ---

Joint Probability

--- Page 767 ---

C = cloud C = ¬¨cloud
0.4 0.6AM
R = rain R = ¬¨rain
C = cloud 0.08 0.32
C = ¬¨cloud 0.02 0.58PM
R = rain R = ¬¨rain
0.1 0.9
AMPM

--- Page 768 ---

P(C | rain)
R = rain R = ¬¨rain
C = cloud 0.08 0.32
C = ¬¨cloud 0.02 0.58P(C | rain) = P(C, rain)
P(rain)= Œ±P(C, rain)
=Œ±‚ü®0.08, 0.02 ‚ü©=‚ü®0.8, 0.2‚ü©

--- Page 769 ---

Probability Rules

--- Page 770 ---

Negation

--- Page 771 ---

Inclusion -Exclusion

--- Page 772 ---

Marginalization

--- Page 773 ---

Marginalization

--- Page 774 ---

Marginalization
R = rain R = ¬¨rain
C = cloud 0.08 0.32
C = ¬¨cloud 0.02 0.58
P(C = cloud )
= P(C = cloud , R = rain) + P(C = cloud , R = ¬¨rain)
= 0.08 + 0.32
= 0.40

--- Page 775 ---

Conditioning

--- Page 776 ---

Conditioning

--- Page 777 ---

Bayesian Networks

--- Page 778 ---

Bayesian network
data structure that represents the dependencies 
among random variables

--- Page 779 ---

Bayesian network
‚Ä¢directed graph
‚Ä¢each node represents a random variable
‚Ä¢arrow from Xto Ymeans Xis a parent of Y
‚Ä¢each node Xhas probability distribution
P(X | Parents (X))

--- Page 780 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }

--- Page 781 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
none light heavy
0.7 0.2 0.1

--- Page 782 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
R yes no
none 0.4 0.6
light 0.2 0.8
heavy 0.1 0.9

--- Page 783 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
R M on time delayed
none yes 0.8 0.2
none no 0.9 0.1
light yes 0.6 0.4
light no 0.7 0.3
heavy yes 0.4 0.6
heavy no 0.5 0.5

--- Page 784 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
T attend miss
on time 0.9 0.1
delayed 0.6 0.4

--- Page 785 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }

--- Page 786 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
P(light)
P(light)Computing Joint 
Probabilities

--- Page 787 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
P(light, no )
P(light) P(no| light)Computing Joint 
Probabilities

--- Page 788 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
P(light, no , delayed )
P(light) P(no| light) P(delayed | light, no )Computing Joint 
Probabilities

--- Page 789 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
P(light, no, delayed , miss)
P(light) P(no| light) P(delayed | light, no ) P(miss | delayed )Computing Joint 
Probabilities

--- Page 790 ---

Inference

--- Page 791 ---

Inference
‚Ä¢Query X: variable for which to compute distribution
‚Ä¢Evidence variables E: observed variables for event e
‚Ä¢Hidden variables Y: non -evidence, non -query variable.
‚Ä¢Goal: Calculate P(X | e)

--- Page 792 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
P(Appointment | light, no )
= Œ± P(Appointment, light, no )
= Œ± [P(Appointment, light, no, on time )
+ P(Appointment, light, no, delayed )]

--- Page 793 ---

Inference by Enumeration
P(X | e) = Œ± P(X, e) = Œ±  
yP(X, e, y)
X is the query variable.
eis the evidence.
yranges over values of hidden variables.
Œ± normalizes the result.

--- Page 794 ---

Approximate Inference

--- Page 795 ---

Sampling

--- Page 796 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }

--- Page 797 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
none light heavy
0.7 0.2 0.1R = none

--- Page 798 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
R yes no
none 0.4 0.6
light 0.2 0.8
heavy 0.1 0.9R = none
M = yes

--- Page 799 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
R M on time delayed
none yes 0.8 0.2
none no 0.9 0.1
light yes 0.6 0.4
light no 0.7 0.3
heavy yes 0.4 0.6
heavy no 0.5 0.5R = none
M = yes
T = on time

--- Page 800 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
T attend miss
on time 0.9 0.1
delayed 0.6 0.4R = none
M = yes
T = on time
A = attend

--- Page 801 ---

R = none
M = yes
T = on time
A = attend

--- Page 802 ---

R = none
M = yes
T = on time
A = attendR = none
M = no
T = on time
A = attendR = light
M = yes
T = delayed
A = attendR = light
M = no
T = on time
A = miss
R = none
M = yes
T = on time
A = attendR = none
M = yes
T = on time
A = attendR = heavy
M = no
T = delayed
A = missR = light
M = no
T = on time
A = attend

--- Page 803 ---

P(Train = on time ) 
?

--- Page 804 ---

R = none
M = yes
T = on time
A = attendR = none
M = no
T = on time
A = attendR = light
M = yes
T = delayed
A = attendR = light
M = no
T = on time
A = miss
R = none
M = yes
T = on time
A = attendR = none
M = yes
T = on time
A = attendR = heavy
M = no
T = delayed
A = missR = light
M = no
T = on time
A = attend

--- Page 805 ---

R = none
M = yes
T = on time
A = attendR = none
M = no
T = on time
A = attendR = light
M = yes
T = delayed
A = attendR = light
M = no
T = on time
A = miss
R = none
M = yes
T = on time
A = attendR = none
M = yes
T = on time
A = attendR = heavy
M = no
T = delayed
A = missR = light
M = no
T = on time
A = attend

--- Page 806 ---

P(Rain = light | Train = on time ) 
?

--- Page 807 ---

R = none
M = yes
T = on time
A = attendR = none
M = no
T = on time
A = attendR = light
M = yes
T = delayed
A = attendR = light
M = no
T = on time
A = miss
R = none
M = yes
T = on time
A = attendR = none
M = yes
T = on time
A = attendR = heavy
M = no
T = delayed
A = missR = light
M = no
T = on time
A = attend

--- Page 808 ---

R = none
M = yes
T = on time
A = attendR = none
M = no
T = on time
A = attendR = light
M = yes
T = delayed
A = attendR = light
M = no
T = on time
A = miss
R = none
M = yes
T = on time
A = attendR = none
M = yes
T = on time
A = attendR = heavy
M = no
T = delayed
A = missR = light
M = no
T = on time
A = attend

--- Page 809 ---

R = none
M = yes
T = on time
A = attendR = none
M = no
T = on time
A = attendR = light
M = yes
T = delayed
A = attendR = light
M = no
T = on time
A = miss
R = none
M = yes
T = on time
A = attendR = none
M = yes
T = on time
A = attendR = heavy
M = no
T = delayed
A = missR = light
M = no
T = on time
A = attend

--- Page 810 ---

Rejection Sampling

--- Page 811 ---

Likelihood Weighting

--- Page 812 ---

Likelihood Weighting
‚Ä¢Start by fixing the values for evidence variables.
‚Ä¢Sample the non -evidence variables using conditional 
probabilities in the Bayesian Network.
‚Ä¢Weight each sample by its likelihood : the probability of all 
of the evidence.

--- Page 813 ---

P(Rain = light | Train = on time ) 
?

--- Page 814 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }

--- Page 815 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
none light heavy
0.7 0.2 0.1T = on timeR = light

--- Page 816 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
R yes no
none 0.4 0.6
light 0.2 0.8
heavy 0.1 0.9T = on timeR = lightR = light
M = yes

--- Page 817 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
R M on time delayed
none yes 0.8 0.2
none no 0.9 0.1
light yes 0.6 0.4
light no 0.7 0.3
heavy yes 0.4 0.6
heavy no 0.5 0.5T = on timeR = lightR = light
M = yes

--- Page 818 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
T attend miss
on time 0.9 0.1
delayed 0.6 0.4T = on timeR = lightR = light
M = yesR = light
M = yes
A = attend

--- Page 819 ---

Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
R M on time delayed
none yes 0.8 0.2
none no 0.9 0.1
light yes 0.6 0.4
light no 0.7 0.3
heavy yes 0.4 0.6
heavy no 0.5 0.5T = on timeR = lightR = light
M = yesR = light
M = yes
A = attend

--- Page 820 ---

Appointment
{attend, miss }Train
{on time, delayed }Maintenance
{yes, no }Rain
{none, light, heavy }
R M on time delayed
none yes 0.8 0.2
none no 0.9 0.1
light yes 0.6 0.4
light no 0.7 0.3
heavy yes 0.4 0.6
heavy no 0.5 0.5T = on timeR = lightR = light
M = yesR = light
M = yes
A = attend

--- Page 821 ---

Uncertainty over Time

--- Page 822 ---

Xt: Weather at time 
t

--- Page 823 ---

Markov assumption
the assumption that the current state depends 
on only a finite fixed number of previous states

--- Page 824 ---

Markov Chain

--- Page 825 ---

Markov chain
a sequence of random variables where the 
distribution of each variable follows the Markov 
assumption

--- Page 826 ---

0.8 0.2
0.3 0.7Today ( Xt)Tomorrow 
(Xt+1)Transition Model

--- Page 827 ---

X0 X1X2 X3X4

--- Page 828 ---

Sensor Models

--- Page 829 ---

Hidden State Observation
robot's position robot's sensor data
words spoken audio waveforms
user engagement website or app analytics
weather umbrella

--- Page 830 ---

Hidden Markov Models

--- Page 831 ---

Hidden Markov Model
a Markov model for a system with hidden states 
that generate some observed event

--- Page 832 ---

0.2 0.8
0.9 0.1State ( Xt)Observation ( Et)Sensor Model

--- Page 833 ---

sensor Markov assumption
the assumption that the evidence variable 
depends only the corresponding state

--- Page 834 ---

X0 X1 X2 X3 X4
E0 E1 E2 E3 E4

--- Page 835 ---

Task Definition
filteringgiven observations from start until now, calculate 
distribution for current state
predictiongiven observations from start until now,
calculate distribution for a future state
smoothinggiven observations from start until now, calculate 
distribution for past state
most likely 
explanationgiven observations from start until now,
calculate most likely sequence of states

--- Page 836 ---

Introduction to
Artificial Intelligence
with Python (Python 4 AI)
Dr. Lahouari Ghouti, PhD
Abjad AI Company
Email: lghouti2019@gmail.com
LinkedIn: https://www.linkedin.com/in/dr -lahouari -ghouti/

--- Page 837 ---

Supervised Learning

--- Page 838 ---

Supervised Learning
Given a data set of input -output pairs, learn a 
function to map inputs to outputs

--- Page 839 ---

Classification
supervised learning task of learning a function 
mapping an input point to a discrete category

--- Page 841 ---

DateHumidity
(relative humidity)Pressure
(sea level, mb)Rain

--- Page 842 ---

DateHumidity
(relative humidity)Pressure
(sea level, mb)Rain
January 1 93% 999.7 Rain
January 2 49% 1015.5 No Rain
January 3 79% 1031.1 No Rain
January 4 65% 984.9 Rain
January 5 90% 975.2 Rain

--- Page 843 ---

‚Ñé(‚Ñéùë¢ùëöùëñùëëùëñùë°ùë¶ ,ùëùùëüùëíùë†ùë†ùë¢ùëüùëí )f(93, 999.7) = Rain
f(49, 1015.5) = No Rainf(humidity , pressure )
f(79, 1031.1) = No Rain

--- Page 844 ---

humiditypressure

--- Page 845 ---

humiditypressure

--- Page 846 ---

humiditypressure

--- Page 847 ---

humiditypressure

--- Page 848 ---

humiditypressure

--- Page 849 ---

Nearest -Neighbor Classification
Algorithm that, given an input, chooses the class 
of the nearest data point to that input

--- Page 850 ---

humiditypressure

--- Page 851 ---

humiditypressure

--- Page 852 ---

humiditypressure

--- Page 853 ---

humiditypressure

--- Page 854 ---

humiditypressure

--- Page 855 ---

humiditypressure

--- Page 856 ---

k-Nearest -Neighbor (KNN) C lassification
Algorithm that, given an input, chooses the most 
common class out of the knearest data points to 
that input

--- Page 857 ---

humiditypressure

--- Page 858 ---

humiditypressure

--- Page 859 ---

x1= Humidity
x2= Pressure
h(x1, x2) =No Rain otherwiseRain ifw0+ w1x1 + w2x2  ‚â• 0

--- Page 860 ---

h(x1, x2) =0 otherwise1 ifw0+ w1x1 + w2x2  ‚â• 0Weight Vector w: (w0, w1, w2)
Input Vector x: (1, x1, x2)
w ¬∑ x: w0+ w1x1 + w2x2

--- Page 861 ---

hw(x) =
0 otherwise1 ifw¬∑ x ‚â• 0Weight Vector w: (w0, w1, w2)
Input Vector x: (1, x1, x2)
w ¬∑ x: w0+ w1x1 + w2x2

--- Page 862 ---

Perceptron Learning Rule
Given data point  (x, y), update each weight 
according to:
wi = wi+ Œ±(y -hw(x)) √óxi

--- Page 863 ---

perceptron learning rule
Given data point  (x, y), update each weight 
according to:
wi = wi+ Œ±(actual value -estimate) √óxi

--- Page 864 ---

output
w¬∑ x01

--- Page 865 ---

humiditypressure

--- Page 866 ---

humiditypressure

--- Page 867 ---

output
w¬∑ x01hard threshold

--- Page 868 ---

output
w¬∑ x01
soft threshold

--- Page 869 ---

Support Vector Machines

--- Page 873 ---

Maximum Margin Separator
Boundary that maximizes the distance between 
any of the data points

--- Page 876 ---

Regression
Supervised learning task of learning a function 
mapping an input point to a continuous value

--- Page 877 ---

‚Ñé(ùëéùëëùë£ùëíùëüùë°ùëñùë†ùëñùëõùëî )f(1200) = 5800
f(2800) = 13400f(advertising )
f(1800) = 8400

--- Page 878 ---

advertisingsales

--- Page 879 ---

Evaluating Hypotheses

--- Page 880 ---

Loss Function
Function that expresses how poorly our 
hypothesis performs

--- Page 881 ---

0-1 Loss Function
L(actual, predicted) =
0 if actual = predicted,
1 otherwise

--- Page 882 ---

humiditypressure

--- Page 883 ---

humiditypressure0
000
00
0
00 000
00 0000
0 0 00 00
0
1
1110

--- Page 884 ---

L1Loss Function
L(actual, predicted) = | actual -predicted |

--- Page 885 ---

advertisingsales

--- Page 886 ---

advertisingsales

--- Page 887 ---

L2Loss Function
L(actual, predicted) = (actual -predicted)2

--- Page 888 ---

Overfitting
Amodel that fits too closely to a particular data 
set and therefore may fail to generalize to future 
data

--- Page 889 ---

humiditypressure

--- Page 890 ---

humiditypressure

--- Page 891 ---

humiditypressure

--- Page 892 ---

advertisingsales

--- Page 893 ---

advertisingsales

--- Page 894 ---

penalizing hypotheses that are more complex to 
favor simpler, more general hypotheses
cost(h) = loss(h)

--- Page 895 ---

penalizing hypotheses that are more complex to 
favor simpler, more general hypotheses
cost(h) = loss(h) +   complexity (h)

--- Page 896 ---

penalizing hypotheses that are more complex to 
favor simpler, more general hypotheses
cost(h) = loss(h) + Œª complexity (h)

--- Page 897 ---

Regularization
Penalizing hypotheses that are more complex to 
favor simpler, more general hypotheses
cost(h) = loss(h) + Œª complexity (h)

--- Page 898 ---

Holdout Cross-Validation
Splitting data into a training set and a
test set , such that learning happens on the 
training set and is evaluated on the test set

--- Page 899 ---

k-Fold Cross-Validation
Splitting data into ksets, and experimenting k
times, using each set as a test set once, and 
using remaining data as training set

--- Page 900 ---

scikit -learn

--- Page 901 ---

Reinforcement Learning

--- Page 902 ---

Reinforcement Learning
Given a set of rewards or punishments, learn 
what actions to take in the future

--- Page 903 ---

AgentEnvironment
stateactionreward

--- Page 904 ---

Markov Decision Process (MDP)
Model for decision -making , representing 
states , actions , and their rewards

--- Page 905 ---

Markov Decision Process (MDP)
Model for decision -making , representing 
states , actions , and their rewards

--- Page 906 ---

X0 X1 X2 X3 X4Markov Chain

--- Page 909 ---

r
r
rr r r
r r r
r r r

--- Page 910 ---

Markov Decision Process
‚Ä¢Set of states S
‚Ä¢Set of actions ACTIONS (s)
‚Ä¢Transition model P(s'| s,a)
‚Ä¢Reward function R(s, a, s')

--- Page 919 ---

Q-learning
Method for learning a function Q(s, a),
estimate of the value of performing action ain 
state s

--- Page 920 ---

Q-learning Overview
‚Ä¢Start with Q(s, a) = 0 for all s, a
‚Ä¢When we taken an action and receive a reward:
‚Ä¢Estimate the value of Q(s, a)based on current reward and 
expected future rewards
‚Ä¢Update Q(s, a) to take into account old estimate as well as 
our new estimate

--- Page 921 ---

Q-learning
‚Ä¢Start with Q(s, a) = 0 for all s, a
‚Ä¢Every time we take an action ain state sand observe a 
reward r, we update:
Q(s, a) ‚Üê Q( s, a) + Œ±(new value estimate -old value estimate)

--- Page 922 ---

Q-learning
‚Ä¢Start with Q(s, a) = 0 for all s, a
‚Ä¢Every time we take an action ain state sand observe a 
reward r, we update:
Q(s, a) ‚Üê Q( s, a) + Œ±(new value estimate -Q(s, a))

--- Page 923 ---

Q-learning
‚Ä¢Start with Q(s, a) = 0 for all s, a
‚Ä¢Every time we take an action ain state sand observe a 
reward r, we update:
Q(s, a) ‚Üê Q( s, a) + Œ±((r + future reward estimate) -Q(s, a))

--- Page 924 ---

Q-learning
‚Ä¢Start with Q(s, a) = 0 for all s, a
‚Ä¢Every time we take an action ain state sand observe a 
reward r, we update:
Q(s, a) ‚Üê Q( s, a) + Œ±((r + max a'Q(s', a')) -Q(s, a))

--- Page 925 ---

Q-learning
‚Ä¢Start with Q(s, a) = 0 for all s, a
‚Ä¢Every time we take an action ain state sand observe a 
reward r, we update:
Q(s, a) ‚Üê Q( s, a) + Œ±((r + Œ≥ max a'Q(s', a')) -Q(s, a))

--- Page 926 ---

Greedy Decision -Making
‚Ä¢When in state s, choose action a with highest Q(s, a)

--- Page 929 ---

Explore vs. Exploit

--- Page 930 ---

Œµ-greedy
‚Ä¢Set Œµequal to how often we want to move randomly.
‚Ä¢With probability 1 -Œµ, choose estimated best move.
‚Ä¢With probability Œµ, choose a random move.

--- Page 931 ---

Nim

--- Page 940 ---

Function Approximation
Approximating Q(s, a), often by a function 
combining various features, rather than storing 
one value for every state -action pair

--- Page 941 ---

Unsupervised Learning

--- Page 942 ---

Unsupervised Learning
Given input data without any additional 
feedback, learn patterns

--- Page 943 ---

Clustering

--- Page 944 ---

Clustering
Organizing a set of objects into groups in such a 
way that similar objects tend to be in the same 
group

--- Page 945 ---

Some Clustering Applications
‚Ä¢Genetic research
‚Ä¢Image segmentation
‚Ä¢Market research
‚Ä¢Medical imaging
‚Ä¢Social network analysis.

--- Page 946 ---

k-Means Clustering
Algorithm for clustering data based on 
repeatedly assigning points to clusters and 
updating those clusters' centers

--- Page 958 ---

Learning Paradigms
‚Ä¢Supervised Learning
‚Ä¢Reinforcement Learning
‚Ä¢Unsupervised Learning
